{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13417652,"sourceType":"datasetVersion","datasetId":8516021}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os, sys, random\n\nrepo_dir = \"gpt-2-based-llm\"\nrepo_url = f\"https://github.com/mathensley/{repo_dir}.git\"\n\n!rm -rf {repo_dir}\n!git clone {repo_url} {repo_dir}\nproject_path = os.path.join(os.getcwd(), repo_dir, 'src')\n\nif project_path not in sys.path:\n    sys.path.append(project_path)\n\nprint(f\"Repositório público clonado e caminho adicionado ao sys.path: {project_path}\")\n\nSEED = 40\nMAX_MB = 200\nMIN_PARAGRAPH_LENGTH = 0\nrandom.seed(SEED)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-18T00:57:47.736401Z","iopub.execute_input":"2025-10-18T00:57:47.736724Z","iopub.status.idle":"2025-10-18T00:57:48.515883Z","shell.execute_reply.started":"2025-10-18T00:57:47.736701Z","shell.execute_reply":"2025-10-18T00:57:48.515053Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'gpt-2-based-llm'...\nremote: Enumerating objects: 50, done.\u001b[K\nremote: Counting objects: 100% (50/50), done.\u001b[K\nremote: Compressing objects: 100% (34/34), done.\u001b[K\nremote: Total 50 (delta 14), reused 45 (delta 13), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (50/50), 20.22 KiB | 5.05 MiB/s, done.\nResolving deltas: 100% (14/14), done.\nRepositório público clonado e caminho adicionado ao sys.path: /kaggle/working/gpt-2-based-llm/src\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install --upgrade pip\n!pip install -r /kaggle/working/gpt-2-based-llm/requirements.txt\n!apt-get update -qq\n!apt install chromium-chromedriver -y\n!pip install selenium","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T00:57:52.616077Z","iopub.execute_input":"2025-10-18T00:57:52.616369Z","iopub.status.idle":"2025-10-18T00:58:34.781232Z","shell.execute_reply.started":"2025-10-18T00:57:52.616346Z","shell.execute_reply":"2025-10-18T00:58:34.780063Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\nCollecting pip\n  Downloading pip-25.2-py3-none-any.whl.metadata (4.7 kB)\nDownloading pip-25.2-py3-none-any.whl (1.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pip\n  Attempting uninstall: pip\n    Found existing installation: pip 24.1.2\n    Uninstalling pip-24.1.2:\n      Successfully uninstalled pip-24.1.2\nSuccessfully installed pip-25.2\nLooking in indexes: https://download.pytorch.org/whl/cu126\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/gpt-2-based-llm/requirements.txt (line 3)) (2.6.0+cu124)\nRequirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/gpt-2-based-llm/requirements.txt (line 6)) (0.9.0)\nRequirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/gpt-2-based-llm/requirements.txt (line 9)) (0.21.0)\n\u001b[31mERROR: Could not find a version that satisfies the requirement dotenv (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for dotenv\u001b[0m\u001b[31m\n\u001b[0mW: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following additional packages will be installed:\n  apparmor chromium-browser libfuse3-3 libpam-systemd libsystemd0 libudev1\n  snapd squashfs-tools systemd systemd-hwe-hwdb systemd-sysv udev\nSuggested packages:\n  apparmor-profiles-extra apparmor-utils fuse3 zenity | kdialog\n  systemd-container libtss2-esys-3.0.2-0 libtss2-mu0 libtss2-rc0\nRecommended packages:\n  networkd-dispatcher systemd-timesyncd | time-daemon libnss-systemd\nThe following NEW packages will be installed:\n  apparmor chromium-browser chromium-chromedriver libfuse3-3 snapd\n  squashfs-tools systemd-hwe-hwdb udev\nThe following packages will be upgraded:\n  libpam-systemd libsystemd0 libudev1 systemd systemd-sysv\n5 upgraded, 8 newly installed, 0 to remove and 154 not upgraded.\nNeed to get 39.3 MB of archives.\nAfter this operation, 134 MB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 systemd-sysv amd64 249.11-0ubuntu3.17 [10.5 kB]\nGet:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpam-systemd amd64 249.11-0ubuntu3.17 [203 kB]\nGet:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 systemd amd64 249.11-0ubuntu3.17 [4,583 kB]\nGet:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libsystemd0 amd64 249.11-0ubuntu3.17 [317 kB]\nGet:5 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 apparmor amd64 3.0.4-2ubuntu2.4 [598 kB]\nGet:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 squashfs-tools amd64 1:4.5-3build1 [159 kB]\nGet:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libudev1 amd64 249.11-0ubuntu3.17 [76.7 kB]\nGet:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 udev amd64 249.11-0ubuntu3.17 [1,557 kB]\nGet:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfuse3-3 amd64 3.10.5-1build1 [81.2 kB]\nGet:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 snapd amd64 2.71+ubuntu22.04 [31.6 MB]\nGet:11 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-browser amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [49.2 kB]\nGet:12 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-chromedriver amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [2,308 B]\nGet:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 systemd-hwe-hwdb all 249.11.6 [3,668 B]\nFetched 39.3 MB in 2s (21.8 MB/s)\nPreconfiguring packages ...\n\n(Reading database ... 128639 files and directories currently installed.)\nPreparing to unpack .../systemd-sysv_249.11-0ubuntu3.17_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  2%]\u001b[49m\u001b[39m [#.........................................................] \u001b8Unpacking systemd-sysv (249.11-0ubuntu3.17) over (249.11-0ubuntu3.16) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  4%]\u001b[49m\u001b[39m [##........................................................] \u001b8Preparing to unpack .../libpam-systemd_249.11-0ubuntu3.17_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  6%]\u001b[49m\u001b[39m [###.......................................................] \u001b8Unpacking libpam-systemd:amd64 (249.11-0ubuntu3.17) over (249.11-0ubuntu3.16) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  8%]\u001b[49m\u001b[39m [####......................................................] \u001b8Preparing to unpack .../systemd_249.11-0ubuntu3.17_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  9%]\u001b[49m\u001b[39m [#####.....................................................] \u001b8Unpacking systemd (249.11-0ubuntu3.17) over (249.11-0ubuntu3.16) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 11%]\u001b[49m\u001b[39m [######....................................................] \u001b8Preparing to unpack .../libsystemd0_249.11-0ubuntu3.17_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 13%]\u001b[49m\u001b[39m [#######...................................................] \u001b8Unpacking libsystemd0:amd64 (249.11-0ubuntu3.17) over (249.11-0ubuntu3.16) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 15%]\u001b[49m\u001b[39m [########..................................................] \u001b8Setting up libsystemd0:amd64 (249.11-0ubuntu3.17) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 17%]\u001b[49m\u001b[39m [#########.................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 19%]\u001b[49m\u001b[39m [##########................................................] \u001b8Selecting previously unselected package apparmor.\n(Reading database ... 128639 files and directories currently installed.)\nPreparing to unpack .../apparmor_3.0.4-2ubuntu2.4_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 21%]\u001b[49m\u001b[39m [############..............................................] \u001b8Unpacking apparmor (3.0.4-2ubuntu2.4) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 23%]\u001b[49m\u001b[39m [#############.............................................] \u001b8Selecting previously unselected package squashfs-tools.\nPreparing to unpack .../squashfs-tools_1%3a4.5-3build1_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 25%]\u001b[49m\u001b[39m [##############............................................] \u001b8Unpacking squashfs-tools (1:4.5-3build1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 26%]\u001b[49m\u001b[39m [###############...........................................] \u001b8Preparing to unpack .../libudev1_249.11-0ubuntu3.17_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 28%]\u001b[49m\u001b[39m [################..........................................] \u001b8Unpacking libudev1:amd64 (249.11-0ubuntu3.17) over (249.11-0ubuntu3.12) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 30%]\u001b[49m\u001b[39m [#################.........................................] \u001b8Setting up libudev1:amd64 (249.11-0ubuntu3.17) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 32%]\u001b[49m\u001b[39m [##################........................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 34%]\u001b[49m\u001b[39m [###################.......................................] \u001b8Selecting previously unselected package udev.\n(Reading database ... 128839 files and directories currently installed.)\nPreparing to unpack .../udev_249.11-0ubuntu3.17_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 36%]\u001b[49m\u001b[39m [####################......................................] \u001b8Unpacking udev (249.11-0ubuntu3.17) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 38%]\u001b[49m\u001b[39m [#####################.....................................] \u001b8Selecting previously unselected package libfuse3-3:amd64.\nPreparing to unpack .../libfuse3-3_3.10.5-1build1_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 40%]\u001b[49m\u001b[39m [######################....................................] \u001b8Unpacking libfuse3-3:amd64 (3.10.5-1build1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 42%]\u001b[49m\u001b[39m [########################..................................] \u001b8Selecting previously unselected package snapd.\nPreparing to unpack .../snapd_2.71+ubuntu22.04_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 43%]\u001b[49m\u001b[39m [#########################.................................] \u001b8Unpacking snapd (2.71+ubuntu22.04) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 45%]\u001b[49m\u001b[39m [##########################................................] \u001b8Setting up apparmor (3.0.4-2ubuntu2.4) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 47%]\u001b[49m\u001b[39m [###########################...............................] \u001b8Created symlink /etc/systemd/system/sysinit.target.wants/apparmor.service → /lib/systemd/system/apparmor.service.\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 49%]\u001b[49m\u001b[39m [############################..............................] \u001b8Setting up squashfs-tools (1:4.5-3build1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 51%]\u001b[49m\u001b[39m [#############################.............................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 53%]\u001b[49m\u001b[39m [##############################............................] \u001b8Setting up systemd (249.11-0ubuntu3.17) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 55%]\u001b[49m\u001b[39m [###############################...........................] \u001b8\u001b[0;1;31m/usr/lib/tmpfiles.d/static-nodes-permissions.conf:17: Failed to resolve group 'kvm'.\u001b[0m\n\u001b[0;1;31m/usr/lib/tmpfiles.d/static-nodes-permissions.conf:18: Failed to resolve group 'kvm'.\u001b[0m\n\u001b[0;1;31m/usr/lib/tmpfiles.d/static-nodes-permissions.conf:19: Failed to resolve group 'kvm'.\u001b[0m\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 57%]\u001b[49m\u001b[39m [################################..........................] \u001b8Setting up udev (249.11-0ubuntu3.17) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 58%]\u001b[49m\u001b[39m [#################################.........................] \u001b8invoke-rc.d: could not determine current runlevel\ninvoke-rc.d: policy-rc.d denied execution of start.\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 60%]\u001b[49m\u001b[39m [###################################.......................] \u001b8Setting up libfuse3-3:amd64 (3.10.5-1build1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 62%]\u001b[49m\u001b[39m [####################################......................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 64%]\u001b[49m\u001b[39m [#####################################.....................] \u001b8Setting up snapd (2.71+ubuntu22.04) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 66%]\u001b[49m\u001b[39m [######################################....................] \u001b8Created symlink /etc/systemd/system/multi-user.target.wants/snapd.apparmor.service → /lib/systemd/system/snapd.apparmor.service.\nCreated symlink /etc/systemd/system/multi-user.target.wants/snapd.autoimport.service → /lib/systemd/system/snapd.autoimport.service.\nCreated symlink /etc/systemd/system/multi-user.target.wants/snapd.core-fixup.service → /lib/systemd/system/snapd.core-fixup.service.\nCreated symlink /etc/systemd/system/multi-user.target.wants/snapd.recovery-chooser-trigger.service → /lib/systemd/system/snapd.recovery-chooser-trigger.service.\nCreated symlink /etc/systemd/system/multi-user.target.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\nCreated symlink /etc/systemd/system/cloud-final.service.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\nUnit /lib/systemd/system/snapd.seeded.service is added as a dependency to a non-existent unit cloud-final.service.\nCreated symlink /etc/systemd/system/multi-user.target.wants/snapd.service → /lib/systemd/system/snapd.service.\nCreated symlink /etc/systemd/system/timers.target.wants/snapd.snap-repair.timer → /lib/systemd/system/snapd.snap-repair.timer.\nCreated symlink /etc/systemd/system/sockets.target.wants/snapd.socket → /lib/systemd/system/snapd.socket.\nCreated symlink /etc/systemd/system/final.target.wants/snapd.system-shutdown.service → /lib/systemd/system/snapd.system-shutdown.service.\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 68%]\u001b[49m\u001b[39m [#######################################...................] \u001b8Selecting previously unselected package chromium-browser.\n(Reading database ... 129070 files and directories currently installed.)\nPreparing to unpack .../chromium-browser_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 70%]\u001b[49m\u001b[39m [########################################..................] \u001b8=> Installing the chromium snap\n==> Checking connectivity with the snap store\n===> System doesn't have a working snapd, skipping\nUnpacking chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 72%]\u001b[49m\u001b[39m [#########################################.................] \u001b8Selecting previously unselected package chromium-chromedriver.\nPreparing to unpack .../chromium-chromedriver_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 74%]\u001b[49m\u001b[39m [##########################################................] \u001b8Unpacking chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 75%]\u001b[49m\u001b[39m [###########################################...............] \u001b8Selecting previously unselected package systemd-hwe-hwdb.\nPreparing to unpack .../systemd-hwe-hwdb_249.11.6_all.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 77%]\u001b[49m\u001b[39m [############################################..............] \u001b8Unpacking systemd-hwe-hwdb (249.11.6) ...\nSetting up systemd-sysv (249.11-0ubuntu3.17) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 78%]\u001b[49m\u001b[39m [#############################################.............] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 80%]\u001b[49m\u001b[39m [##############################################............] \u001b8Setting up systemd-hwe-hwdb (249.11.6) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 82%]\u001b[49m\u001b[39m [###############################################...........] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 84%]\u001b[49m\u001b[39m [################################################..........] \u001b8Setting up chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 85%]\u001b[49m\u001b[39m [#################################################.........] \u001b8update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\nupdate-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 87%]\u001b[49m\u001b[39m [##################################################........] \u001b8Setting up libpam-systemd:amd64 (249.11-0ubuntu3.17) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 89%]\u001b[49m\u001b[39m [###################################################.......] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 91%]\u001b[49m\u001b[39m [####################################################......] \u001b8Setting up chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 93%]\u001b[49m\u001b[39m [#####################################################.....] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 95%]\u001b[49m\u001b[39m [######################################################....] \u001b8Processing triggers for udev (249.11-0ubuntu3.17) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 96%]\u001b[49m\u001b[39m [#######################################################...] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 98%]\u001b[49m\u001b[39m [########################################################..] \u001b8Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\nProcessing triggers for hicolor-icon-theme (0.17-2) ...\nProcessing triggers for libc-bin (2.35-0ubuntu3.8) ...\n/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n\nProcessing triggers for man-db (2.10.2-1) ...\nProcessing triggers for dbus (1.12.20-2ubuntu4.1) ...\n\n\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[JCollecting selenium\n  Downloading selenium-4.37.0-py3-none-any.whl.metadata (7.5 kB)\nRequirement already satisfied: urllib3<3.0,>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (2.5.0)\nCollecting trio<1.0,>=0.31.0 (from selenium)\n  Downloading trio-0.31.0-py3-none-any.whl.metadata (8.5 kB)\nCollecting trio-websocket<1.0,>=0.12.2 (from selenium)\n  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\nCollecting certifi>=2025.10.5 (from selenium)\n  Downloading certifi-2025.10.5-py3-none-any.whl.metadata (2.5 kB)\nRequirement already satisfied: typing_extensions<5.0,>=4.15.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (4.15.0)\nRequirement already satisfied: websocket-client<2.0,>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\nRequirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio<1.0,>=0.31.0->selenium) (25.3.0)\nRequirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio<1.0,>=0.31.0->selenium) (2.4.0)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio<1.0,>=0.31.0->selenium) (3.10)\nCollecting outcome (from trio<1.0,>=0.31.0->selenium)\n  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio<1.0,>=0.31.0->selenium) (1.3.1)\nCollecting wsproto>=0.14 (from trio-websocket<1.0,>=0.12.2->selenium)\n  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\nRequirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\nRequirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket<1.0,>=0.12.2->selenium) (0.16.0)\nDownloading selenium-4.37.0-py3-none-any.whl (9.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hDownloading trio-0.31.0-py3-none-any.whl (512 kB)\nDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\nDownloading certifi-2025.10.5-py3-none-any.whl (163 kB)\nDownloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\nDownloading wsproto-1.2.0-py3-none-any.whl (24 kB)\nInstalling collected packages: wsproto, outcome, certifi, trio, trio-websocket, selenium\n\u001b[2K  Attempting uninstall: certifi\n\u001b[2K    Found existing installation: certifi 2025.8.3\n\u001b[2K    Uninstalling certifi-2025.8.3:\n\u001b[2K      Successfully uninstalled certifi-2025.8.3\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [selenium]5/6\u001b[0m [selenium]\n\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\ngoogle-cloud-bigtable 2.32.0 requires google-api-core[grpc]<3.0.0,>=2.17.0, but you have google-api-core 1.34.1 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.1 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ngoogle-cloud-storage 2.19.0 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\ntransformers 4.53.3 requires huggingface-hub<1.0,>=0.30.0, but you have huggingface-hub 1.0.0rc2 which is incompatible.\njupyter-kernel-gateway 2.5.2 requires jupyter-client<8.0,>=5.2.0, but you have jupyter-client 8.6.3 which is incompatible.\ndataproc-spark-connect 0.8.3 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.9.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed certifi-2025.10.5 outcome-1.3.0.post0 selenium-4.37.0 trio-0.31.0 trio-websocket-0.12.2 wsproto-1.2.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!python /kaggle/working/{repo_dir}/scripts/download_data.py {MAX_MB} {SEED}\n!python /kaggle/working/{repo_dir}/scripts/prepare_data.py {MIN_PARAGRAPH_LENGTH}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch, tiktoken\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nimport time\nimport csv\nimport os\nimport sys\nimport wandb\nfrom itertools import cycle\nfrom pathlib import Path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T01:01:26.176491Z","iopub.execute_input":"2025-10-18T01:01:26.176823Z","iopub.status.idle":"2025-10-18T01:01:29.020248Z","shell.execute_reply.started":"2025-10-18T01:01:26.176801Z","shell.execute_reply":"2025-10-18T01:01:29.019351Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from utils import get_loaders\nfrom model.gpt2_original import GPT2ModelOriginal","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T01:01:34.842411Z","iopub.execute_input":"2025-10-18T01:01:34.843246Z","iopub.status.idle":"2025-10-18T01:01:34.847447Z","shell.execute_reply.started":"2025-10-18T01:01:34.843203Z","shell.execute_reply":"2025-10-18T01:01:34.846395Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"torch.manual_seed(40)\n\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nprint(f\"Usando dispositivo: {device.type}\")\n\nTOKENIZER = tiktoken.get_encoding(\"gpt2\")\nprint(TOKENIZER.n_vocab)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T01:01:35.961270Z","iopub.execute_input":"2025-10-18T01:01:35.961553Z","iopub.status.idle":"2025-10-18T01:01:36.034169Z","shell.execute_reply.started":"2025-10-18T01:01:35.961523Z","shell.execute_reply":"2025-10-18T01:01:36.033319Z"}},"outputs":[{"name":"stdout","text":"Usando dispositivo: cuda\n50257\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"CONFIG = {\n    \"vocab_size\": TOKENIZER.n_vocab,\n    \"embedding_dim\": 512,\n    \"context_length\": 256,\n    \"num_layers\": 8,\n    \"num_heads\": 8,\n    \"bias\": False,\n    \n    \"batch_size\": 8,\n    \"max_epochs\": 2,\n    \"num_workers\": 0,\n    \"stride\": 256 // 2,\n    \"dtype\": torch.float32,\n    \"device\": device,\n\n    \"eval_freq\": 100,\n    \"eval_iter\": 16,\n\n    \"save_wdb\": True,\n    \"save_freq_wdb\": 5000,\n    \"user\": \"matheus-figueiredo-silva-ufcg\",\n    \"project\": \"gpt2-original\",\n    \"name\": \"test1\",\n    \"run_id\": \"gpt2-original-run1\",\n    \"version\": \"v2\",\n    \"file_name\": \"mini_mlp.pth\"\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T22:49:43.601843Z","iopub.execute_input":"2025-10-17T22:49:43.602554Z","iopub.status.idle":"2025-10-17T22:49:43.606965Z","shell.execute_reply.started":"2025-10-17T22:49:43.602529Z","shell.execute_reply":"2025-10-17T22:49:43.606235Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"os.environ[\"WANDB_API_KEY\"] = \"8dc112bdcb6f80c3fb20da526d4a52923f2074f2\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T01:01:54.281731Z","iopub.execute_input":"2025-10-18T01:01:54.282015Z","iopub.status.idle":"2025-10-18T01:01:54.286508Z","shell.execute_reply.started":"2025-10-18T01:01:54.281995Z","shell.execute_reply":"2025-10-18T01:01:54.285609Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"train_loader, test_loader, val_loader = get_loaders(\n    data_path=\"/kaggle/input/processed1\",\n    tokenizer=TOKENIZER,\n    max_length=CONFIG[\"context_length\"],\n    batch_sz=CONFIG[\"batch_size\"],\n    num_workers=CONFIG[\"num_workers\"],\n    stride=CONFIG[\"stride\"]\n)\n\nprint(f\"Tamanho do conjunto de treinamento: {len(train_loader)}\\nTamanho do conjunto de teste: {len(test_loader)}\\nTamanho do conjunto de validação: {len(val_loader)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T01:01:56.039260Z","iopub.execute_input":"2025-10-18T01:01:56.039927Z","iopub.status.idle":"2025-10-18T01:02:20.601241Z","shell.execute_reply.started":"2025-10-18T01:01:56.039903Z","shell.execute_reply":"2025-10-18T01:02:20.600126Z"}},"outputs":[{"name":"stdout","text":"Arquivos carregados!\n\nDataset de TRAIN está pronto!\nDataloader de TRAIN está pronto!\nDataset de TEST está pronto!\nDataloader de TEST está pronto!\nDataset de VALIDATION está pronto!\nDataloader de VALIDATION está pronto!\n\n\n- Train\n\tTotal de amostras: 101482\n\tTokens em cada amostra: 256\n\tNúmero de batches: 12685\n\tNúmero de amostras por batch: 8\n- Test\n\tTotal de amostras: 13471\n\tTokens em cada amostra: 256\n\tNúmero de batches: 1683\n\tNúmero de amostras por batch: 8\n- Validation\n\tTotal de amostras: 12843\n\tTokens em cada amostra: 256\n\tNúmero de batches: 1605\n\tNúmero de amostras por batch: 8\n\n\nTamanho do conjunto de treinamento: 12685\nTamanho do conjunto de teste: 1683\nTamanho do conjunto de validação: 1605\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## Funções de Treino","metadata":{}},{"cell_type":"code","source":"from train.eval import calc_loss_batch_by_cross_entropy, evaluate_model\nfrom utils.plots import plot_graph\nfrom utils.generate import generate_text, generate_and_print_sample","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T01:20:29.898512Z","iopub.execute_input":"2025-10-18T01:20:29.898975Z","iopub.status.idle":"2025-10-18T01:20:29.904698Z","shell.execute_reply.started":"2025-10-18T01:20:29.898950Z","shell.execute_reply":"2025-10-18T01:20:29.903737Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"from utils import text_to_token_ids, token_ids_to_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T01:20:32.030636Z","iopub.execute_input":"2025-10-18T01:20:32.031518Z","iopub.status.idle":"2025-10-18T01:20:32.036183Z","shell.execute_reply.started":"2025-10-18T01:20:32.031481Z","shell.execute_reply":"2025-10-18T01:20:32.035522Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"def fetch_weights_and_bias(user, project, name, version, file_name):\n    \"\"\"\n    Busca um artifact de pesos salvo no W&B.\n    Exemplo de version: 'v1', 'v2', etc (não o run id).\n    \"\"\"\n    try:\n        api = wandb.Api()\n        artifact = api.artifact(f\"{user}/{project}/{name}:{version}\", type=\"model\")\n        artifact_dir = artifact.download()\n        file_path = os.path.join(artifact_dir, file_name)\n        print(f\"Fetch success -> {file_path}\")\n        return True\n\n    except Exception as e:\n        print(f\"Fetch Error: {e}\")\n        return False\n\n\ndef load_weights_and_bias(file_name):\n    try:\n        checkpoint = torch.load(file_name, map_location=\"cpu\")\n        return True, checkpoint\n    except Exception as e:\n        print(f\"Load error: {e}\")\n        return False, {}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T01:20:38.917289Z","iopub.execute_input":"2025-10-18T01:20:38.917996Z","iopub.status.idle":"2025-10-18T01:20:38.925208Z","shell.execute_reply.started":"2025-10-18T01:20:38.917970Z","shell.execute_reply":"2025-10-18T01:20:38.924389Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"def generate_and_print_sample_for_qwen3(model, tokenizer, device, start_context):\n    model.eval()\n    context_size = model.cfg[\"context_length\"]\n\n    encoded = text_to_token_ids(start_context, tokenizer, device).to(device)\n\n    with torch.no_grad():\n        token_ids = generate_text(\n            model=model,\n            idx=encoded,\n            max_new_tokens=50,\n            context_size=context_size\n        )\n\n    decoded_text = token_ids_to_text(token_ids, tokenizer)\n    print(f\"Amostra Gerada: '{decoded_text.replace(os.linesep, ' ')}'\")\n    model.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T01:20:44.110000Z","iopub.execute_input":"2025-10-18T01:20:44.110730Z","iopub.status.idle":"2025-10-18T01:20:44.116990Z","shell.execute_reply.started":"2025-10-18T01:20:44.110701Z","shell.execute_reply":"2025-10-18T01:20:44.116103Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"def train_aux(\n        wandb_run, model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter, \n        start_context, tokenizer, save_freq_wdb, file_name, save_wdb, state_dict, name, start_time\n):\n    train_losses, val_losses, track_tokens_seen = [], [], []\n    tokens_seen, global_step = 0, -1\n\n    epochs_complete = state_dict.get(\"epoch\", 0)\n    batchs_complete = state_dict.get(\"batch\", 0)\n    accumulated_time = state_dict.get(\"train_time\", 0)\n\n    for epoch in range(epochs_complete, num_epochs):\n        model.train()\n\n        for batch_idx, (input_batch, target_batch) in enumerate(train_loader):\n            if epoch == epochs_complete and batch_idx < batchs_complete:\n                continue\n\n            optimizer.zero_grad()\n            loss = calc_loss_batch_by_cross_entropy(\n                model,\n                input_batch,\n                target_batch,\n                device\n            )\n            loss.backward()\n            optimizer.step()\n            tokens_seen += input_batch.numel()\n            global_step += 1\n\n            if global_step % eval_freq == 0:\n                train_loss, val_loss = evaluate_model(\n                    model,\n                    train_loader,\n                    val_loader,\n                    device,\n                    eval_iter\n                )\n                train_losses.append(train_loss)\n                val_losses.append(val_loss)\n                track_tokens_seen.append(tokens_seen)\n\n                wandb_run.log({\n                    \"train_loss\": train_loss,\n                    \"val_loss\": val_loss,\n                    \"tokens_seen\": tokens_seen,\n                    \"epoch\": epoch + 1,\n                    \"global_step\": global_step,\n                })\n\n                print(\n                    f\"Ep {epoch+1} (Step {global_step:06d}): \"\n                    f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\"\n                )\n\n            # salva peso só após o primeiro batch\n            if save_wdb and global_step > 0 and global_step % save_freq_wdb == 0:\n                elapsed_time = time.time() - start_time + accumulated_time\n                torch.save({\n                    \"epoch\": epoch,\n                    \"model_state\": model.state_dict(),\n                    \"optimizer_state\": optimizer.state_dict(),\n                    \"batch\": batch_idx,\n                    \"train_time\": elapsed_time\n                },  file_name)\n                artifact = wandb.Artifact(name, type=\"model\")\n                artifact.add_file(file_name)\n                wandb_run.log_artifact(artifact)\n        print(\"\\nEXEMPLO DE GERAÇÃO:\")\n        #generate_and_print_sample(model, tokenizer, device, start_context)\n        generate_and_print_sample_for_qwen3(model, tokenizer, device, start_context)\n\n        # salva após finalizar o treino\n        elapsed_time = time.time() - start_time + accumulated_time\n        if save_wdb:\n          torch.save({\n              \"epoch\": epoch,\n              \"model_state\": model.state_dict(),\n              \"optimizer_state\": optimizer.state_dict(),\n              \"batch\": batch_idx,\n              \"train_time\": elapsed_time\n          },  file_name)\n          artifact = wandb.Artifact(file_name.split(\".\")[0] + \"_final\", type=\"model\")\n          artifact.add_file(file_name)\n          wandb_run.log_artifact(artifact)\n\n    return train_losses, val_losses, track_tokens_seen, elapsed_time\n\n\ndef run_train(model, optimizer, config, tokenizer, start_context=\"Bom dia!\"):\n    run = wandb.init(project=config[\"project\"], name=config[\"name\"], id=config[\"run_id\"], resume=\"allow\")\n    res_fetch = fetch_weights_and_bias(\n        user=config[\"user\"],\n        project=config[\"project\"],\n        name=config[\"name\"],\n        version=config[\"version\"],\n        file_name=config[\"file_name\"]\n    )\n\n    state_dict = {}\n    if res_fetch:\n        loaded, checkpoint = load_weights_and_bias(\n            file_name=f\"/kaggle/working/artifacts/{config['name']}:{config['version']}/{config['file_name']}\"\n        )\n        if loaded:\n            model.load_state_dict(checkpoint[\"model_state\"])\n            optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n            state_dict[\"epoch\"] = checkpoint.get(\"epoch\", 0)\n            state_dict[\"batch\"] = checkpoint.get(\"batch\", 0)\n            state_dict[\"train_time\"] = checkpoint.get(\"train_time\", 0.0)\n            print(\"Pesos carregados com sucesso!\")\n\n    print(\"\\nEPOCHS/BATCHS RECUPERADOS: \", state_dict)\n    num_epochs = config[\"max_epochs\"]\n\n    start_time = time.time()\n    train_losses, val_losses, tokens_seen, total_train_time = train_aux(\n        run, model, train_loader, val_loader, optimizer, device,\n        num_epochs=num_epochs, eval_freq=100, eval_iter=16,\n        start_context=start_context, tokenizer=tokenizer,\n        save_freq_wdb=config[\"save_freq_wdb\"], file_name=config[\"file_name\"],\n        save_wdb=config[\"save_wdb\"], state_dict=state_dict, \n        name=config[\"name\"], start_time=start_time\n    )\n    print(\"\\nGRÁFICO DE PERDA DURANTE O TREINO:\")\n    epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n    plot_graph(epochs_tensor, tokens_seen, train_losses, val_losses)\n    return tokens_seen[-1], total_train_time","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T01:20:47.772269Z","iopub.execute_input":"2025-10-18T01:20:47.772904Z","iopub.status.idle":"2025-10-18T01:20:47.789252Z","shell.execute_reply.started":"2025-10-18T01:20:47.772878Z","shell.execute_reply":"2025-10-18T01:20:47.788381Z"}},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"## Treino","metadata":{}},{"cell_type":"code","source":"model = GPT2ModelOriginal(CONFIG, device).to(device=device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n\nparams = sum(p.numel() for p in model.parameters())\nparams_model = params - sum(p.numel() for p in model.out_head.parameters())\nprint(f\"Número de parâmetros (sem head): {params_model:,}\")\n\nif device.type == \"cuda\": torch.cuda.reset_peak_memory_stats(device)\n\nstart_time = time.time()\ntokens_processed, total_train_time = run_train(model=model, optimizer=optimizer, config=CONFIG, tokenizer=TOKENIZER)\nend_time = time.time()\n\nelapsed = end_time - start_time\ntokens_per_sec = tokens_processed / elapsed\nmax_memory = torch.cuda.max_memory_allocated(device) / (1024**2)  # MB\n\nprint(\"\\nDESEMPENHO:\")\nprint(f\"Tempo total: {elapsed:.2f} s\")\nprint(f\"Tokens/s: {tokens_per_sec:.2f}\")\nprint(f\"Memória máxima: {max_memory:.2f} MB\")\n\nprint(f\"\\nTempo total de treino: {total_train_time:.2f} s ({total_train_time/60:.2f} min)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T19:30:29.312185Z","iopub.execute_input":"2025-10-17T19:30:29.312819Z","iopub.status.idle":"2025-10-17T20:35:14.983406Z","shell.execute_reply.started":"2025-10-17T19:30:29.312793Z","shell.execute_reply":"2025-10-17T20:35:14.982385Z"}},"outputs":[{"name":"stdout","text":"Número de parâmetros (sem head): 51,045,888\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmatheus-figueiredo-silva\u001b[0m (\u001b[33mmatheus-figueiredo-silva-ufcg\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.21.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20251017_193040-gpt2-original-run1</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/matheus-figueiredo-silva-ufcg/gpt2-original/runs/gpt2-original-run1' target=\"_blank\">test1</a></strong> to <a href='https://wandb.ai/matheus-figueiredo-silva-ufcg/gpt2-original' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/matheus-figueiredo-silva-ufcg/gpt2-original' target=\"_blank\">https://wandb.ai/matheus-figueiredo-silva-ufcg/gpt2-original</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/matheus-figueiredo-silva-ufcg/gpt2-original/runs/gpt2-original-run1' target=\"_blank\">https://wandb.ai/matheus-figueiredo-silva-ufcg/gpt2-original/runs/gpt2-original-run1</a>"},"metadata":{}},{"name":"stdout","text":"Fetch Error: artifact membership 'test1:v2' not found in 'matheus-figueiredo-silva-ufcg/gpt2-original'\n\nEPOCHS/BATCHS RECUPERADOS:  {}\nEp 1 (Step 000000): Train loss 10.432, Val loss 10.564\nEp 1 (Step 000100): Train loss 5.446, Val loss 5.396\nEp 1 (Step 000200): Train loss 5.100, Val loss 4.911\nEp 1 (Step 000300): Train loss 4.848, Val loss 4.672\nEp 1 (Step 000400): Train loss 4.588, Val loss 4.561\nEp 1 (Step 000500): Train loss 4.465, Val loss 4.504\nEp 1 (Step 000600): Train loss 4.326, Val loss 4.428\nEp 1 (Step 000700): Train loss 4.377, Val loss 4.380\nEp 1 (Step 000800): Train loss 4.328, Val loss 4.377\nEp 1 (Step 000900): Train loss 4.231, Val loss 4.287\nEp 1 (Step 001000): Train loss 4.097, Val loss 4.177\nEp 1 (Step 001100): Train loss 4.199, Val loss 4.175\nEp 1 (Step 001200): Train loss 4.067, Val loss 4.075\nEp 1 (Step 001300): Train loss 4.073, Val loss 4.029\nEp 1 (Step 001400): Train loss 3.965, Val loss 4.098\nEp 1 (Step 001500): Train loss 3.960, Val loss 3.962\nEp 1 (Step 001600): Train loss 3.899, Val loss 3.937\nEp 1 (Step 001700): Train loss 3.898, Val loss 3.865\nEp 1 (Step 001800): Train loss 3.827, Val loss 3.864\nEp 1 (Step 001900): Train loss 3.819, Val loss 3.894\nEp 1 (Step 002000): Train loss 3.787, Val loss 3.868\nEp 1 (Step 002100): Train loss 3.727, Val loss 3.854\nEp 1 (Step 002200): Train loss 3.740, Val loss 3.767\nEp 1 (Step 002300): Train loss 3.730, Val loss 3.770\nEp 1 (Step 002400): Train loss 3.695, Val loss 3.752\nEp 1 (Step 002500): Train loss 3.726, Val loss 3.780\nEp 1 (Step 002600): Train loss 3.630, Val loss 3.698\nEp 1 (Step 002700): Train loss 3.685, Val loss 3.672\nEp 1 (Step 002800): Train loss 3.684, Val loss 3.652\nEp 1 (Step 002900): Train loss 3.612, Val loss 3.657\nEp 1 (Step 003000): Train loss 3.600, Val loss 3.674\nEp 1 (Step 003100): Train loss 3.611, Val loss 3.705\nEp 1 (Step 003200): Train loss 3.507, Val loss 3.607\nEp 1 (Step 003300): Train loss 3.499, Val loss 3.626\nEp 1 (Step 003400): Train loss 3.592, Val loss 3.605\nEp 1 (Step 003500): Train loss 3.520, Val loss 3.607\nEp 1 (Step 003600): Train loss 3.549, Val loss 3.559\nEp 1 (Step 003700): Train loss 3.485, Val loss 3.595\nEp 1 (Step 003800): Train loss 3.515, Val loss 3.639\nEp 1 (Step 003900): Train loss 3.549, Val loss 3.549\nEp 1 (Step 004000): Train loss 3.473, Val loss 3.528\nEp 1 (Step 004100): Train loss 3.483, Val loss 3.634\nEp 1 (Step 004200): Train loss 3.434, Val loss 3.592\nEp 1 (Step 004300): Train loss 3.478, Val loss 3.527\nEp 1 (Step 004400): Train loss 3.458, Val loss 3.524\nEp 1 (Step 004500): Train loss 3.411, Val loss 3.533\nEp 1 (Step 004600): Train loss 3.387, Val loss 3.488\nEp 1 (Step 004700): Train loss 3.375, Val loss 3.513\nEp 1 (Step 004800): Train loss 3.462, Val loss 3.487\nEp 1 (Step 004900): Train loss 3.347, Val loss 3.475\nEp 1 (Step 005000): Train loss 3.357, Val loss 3.507\nEp 1 (Step 005100): Train loss 3.329, Val loss 3.461\nEp 1 (Step 005200): Train loss 3.368, Val loss 3.470\nEp 1 (Step 005300): Train loss 3.421, Val loss 3.455\nEp 1 (Step 005400): Train loss 3.413, Val loss 3.454\nEp 1 (Step 005500): Train loss 3.374, Val loss 3.525\nEp 1 (Step 005600): Train loss 3.328, Val loss 3.418\nEp 1 (Step 005700): Train loss 3.312, Val loss 3.454\nEp 1 (Step 005800): Train loss 3.242, Val loss 3.407\nEp 1 (Step 005900): Train loss 3.371, Val loss 3.435\nEp 1 (Step 006000): Train loss 3.308, Val loss 3.458\nEp 1 (Step 006100): Train loss 3.341, Val loss 3.429\nEp 1 (Step 006200): Train loss 3.264, Val loss 3.405\nEp 1 (Step 006300): Train loss 3.330, Val loss 3.423\nEp 1 (Step 006400): Train loss 3.300, Val loss 3.418\nEp 1 (Step 006500): Train loss 3.217, Val loss 3.396\nEp 1 (Step 006600): Train loss 3.205, Val loss 3.418\nEp 1 (Step 006700): Train loss 3.192, Val loss 3.414\nEp 1 (Step 006800): Train loss 3.226, Val loss 3.373\nEp 1 (Step 006900): Train loss 3.210, Val loss 3.408\nEp 1 (Step 007000): Train loss 3.316, Val loss 3.414\nEp 1 (Step 007100): Train loss 3.216, Val loss 3.385\nEp 1 (Step 007200): Train loss 3.278, Val loss 3.411\nEp 1 (Step 007300): Train loss 3.181, Val loss 3.415\nEp 1 (Step 007400): Train loss 3.243, Val loss 3.351\nEp 1 (Step 007500): Train loss 3.147, Val loss 3.348\nEp 1 (Step 007600): Train loss 3.164, Val loss 3.429\nEp 1 (Step 007700): Train loss 3.183, Val loss 3.351\nEp 1 (Step 007800): Train loss 3.242, Val loss 3.361\nEp 1 (Step 007900): Train loss 3.163, Val loss 3.386\nEp 1 (Step 008000): Train loss 3.143, Val loss 3.309\nEp 1 (Step 008100): Train loss 3.195, Val loss 3.347\nEp 1 (Step 008200): Train loss 3.166, Val loss 3.372\nEp 1 (Step 008300): Train loss 3.167, Val loss 3.347\nEp 1 (Step 008400): Train loss 3.191, Val loss 3.378\nEp 1 (Step 008500): Train loss 3.251, Val loss 3.359\nEp 1 (Step 008600): Train loss 3.230, Val loss 3.362\nEp 1 (Step 008700): Train loss 3.175, Val loss 3.396\nEp 1 (Step 008800): Train loss 3.182, Val loss 3.355\nEp 1 (Step 008900): Train loss 3.112, Val loss 3.345\nEp 1 (Step 009000): Train loss 3.134, Val loss 3.319\nEp 1 (Step 009100): Train loss 3.205, Val loss 3.378\nEp 1 (Step 009200): Train loss 3.225, Val loss 3.358\nEp 1 (Step 009300): Train loss 3.061, Val loss 3.334\nEp 1 (Step 009400): Train loss 3.130, Val loss 3.302\nEp 1 (Step 009500): Train loss 3.149, Val loss 3.247\nEp 1 (Step 009600): Train loss 3.096, Val loss 3.314\nEp 1 (Step 009700): Train loss 3.083, Val loss 3.287\nEp 1 (Step 009800): Train loss 3.096, Val loss 3.267\nEp 1 (Step 009900): Train loss 3.116, Val loss 3.296\nEp 1 (Step 010000): Train loss 3.161, Val loss 3.274\nEp 1 (Step 010100): Train loss 3.121, Val loss 3.267\nEp 1 (Step 010200): Train loss 3.102, Val loss 3.309\nEp 1 (Step 010300): Train loss 3.107, Val loss 3.286\nEp 1 (Step 010400): Train loss 3.122, Val loss 3.328\nEp 1 (Step 010500): Train loss 3.100, Val loss 3.304\nEp 1 (Step 010600): Train loss 3.069, Val loss 3.283\nEp 1 (Step 010700): Train loss 3.115, Val loss 3.279\nEp 1 (Step 010800): Train loss 3.055, Val loss 3.292\nEp 1 (Step 010900): Train loss 3.165, Val loss 3.310\nEp 1 (Step 011000): Train loss 3.089, Val loss 3.226\nEp 1 (Step 011100): Train loss 3.071, Val loss 3.277\nEp 1 (Step 011200): Train loss 3.077, Val loss 3.200\nEp 1 (Step 011300): Train loss 3.064, Val loss 3.239\nEp 1 (Step 011400): Train loss 3.054, Val loss 3.261\nEp 1 (Step 011500): Train loss 3.054, Val loss 3.256\nEp 1 (Step 011600): Train loss 3.060, Val loss 3.283\nEp 1 (Step 011700): Train loss 3.073, Val loss 3.269\nEp 1 (Step 011800): Train loss 3.022, Val loss 3.269\nEp 1 (Step 011900): Train loss 3.075, Val loss 3.251\nEp 1 (Step 012000): Train loss 3.060, Val loss 3.234\nEp 1 (Step 012100): Train loss 2.963, Val loss 3.262\nEp 1 (Step 012200): Train loss 3.013, Val loss 3.255\nEp 1 (Step 012300): Train loss 2.990, Val loss 3.280\nEp 1 (Step 012400): Train loss 3.033, Val loss 3.239\nEp 1 (Step 012500): Train loss 3.040, Val loss 3.259\nEp 1 (Step 012600): Train loss 2.973, Val loss 3.253\n\nEXEMPLO DE GERAÇÃO:\nAmostra Gerada: 'Bom dia!  Era o que ella era a sua vida, e a sua vida era a sua vida, e a sua vida, e a sua vida, e a sua vida, e'\nEp 2 (Step 012700): Train loss 3.014, Val loss 3.226\nEp 2 (Step 012800): Train loss 2.978, Val loss 3.227\nEp 2 (Step 012900): Train loss 2.954, Val loss 3.261\nEp 2 (Step 013000): Train loss 2.996, Val loss 3.153\nEp 2 (Step 013100): Train loss 2.996, Val loss 3.193\nEp 2 (Step 013200): Train loss 3.046, Val loss 3.195\nEp 2 (Step 013300): Train loss 3.010, Val loss 3.280\nEp 2 (Step 013400): Train loss 2.975, Val loss 3.199\nEp 2 (Step 013500): Train loss 2.996, Val loss 3.260\nEp 2 (Step 013600): Train loss 3.035, Val loss 3.277\nEp 2 (Step 013700): Train loss 2.967, Val loss 3.220\nEp 2 (Step 013800): Train loss 2.950, Val loss 3.257\nEp 2 (Step 013900): Train loss 2.981, Val loss 3.221\nEp 2 (Step 014000): Train loss 3.007, Val loss 3.240\nEp 2 (Step 014100): Train loss 2.974, Val loss 3.249\nEp 2 (Step 014200): Train loss 2.956, Val loss 3.239\nEp 2 (Step 014300): Train loss 2.990, Val loss 3.198\nEp 2 (Step 014400): Train loss 3.059, Val loss 3.266\nEp 2 (Step 014500): Train loss 3.078, Val loss 3.232\nEp 2 (Step 014600): Train loss 2.897, Val loss 3.246\nEp 2 (Step 014700): Train loss 3.039, Val loss 3.227\nEp 2 (Step 014800): Train loss 3.017, Val loss 3.181\nEp 2 (Step 014900): Train loss 2.973, Val loss 3.183\nEp 2 (Step 015000): Train loss 2.974, Val loss 3.206\nEp 2 (Step 015100): Train loss 2.989, Val loss 3.197\nEp 2 (Step 015200): Train loss 3.041, Val loss 3.263\nEp 2 (Step 015300): Train loss 2.969, Val loss 3.240\nEp 2 (Step 015400): Train loss 2.913, Val loss 3.247\nEp 2 (Step 015500): Train loss 2.958, Val loss 3.239\nEp 2 (Step 015600): Train loss 2.908, Val loss 3.180\nEp 2 (Step 015700): Train loss 2.883, Val loss 3.251\nEp 2 (Step 015800): Train loss 2.934, Val loss 3.204\nEp 2 (Step 015900): Train loss 2.966, Val loss 3.231\nEp 2 (Step 016000): Train loss 2.932, Val loss 3.239\nEp 2 (Step 016100): Train loss 2.985, Val loss 3.275\nEp 2 (Step 016200): Train loss 2.913, Val loss 3.224\nEp 2 (Step 016300): Train loss 2.977, Val loss 3.241\nEp 2 (Step 016400): Train loss 2.933, Val loss 3.202\nEp 2 (Step 016500): Train loss 2.977, Val loss 3.259\nEp 2 (Step 016600): Train loss 2.976, Val loss 3.172\nEp 2 (Step 016700): Train loss 2.886, Val loss 3.144\nEp 2 (Step 016800): Train loss 2.914, Val loss 3.237\nEp 2 (Step 016900): Train loss 2.923, Val loss 3.194\nEp 2 (Step 017000): Train loss 2.888, Val loss 3.200\nEp 2 (Step 017100): Train loss 2.888, Val loss 3.166\nEp 2 (Step 017200): Train loss 2.912, Val loss 3.189\nEp 2 (Step 017300): Train loss 2.963, Val loss 3.216\nEp 2 (Step 017400): Train loss 3.022, Val loss 3.140\nEp 2 (Step 017500): Train loss 2.941, Val loss 3.281\nEp 2 (Step 017600): Train loss 2.920, Val loss 3.195\nEp 2 (Step 017700): Train loss 2.860, Val loss 3.173\nEp 2 (Step 017800): Train loss 2.928, Val loss 3.225\nEp 2 (Step 017900): Train loss 2.937, Val loss 3.213\nEp 2 (Step 018000): Train loss 2.961, Val loss 3.190\nEp 2 (Step 018100): Train loss 2.923, Val loss 3.204\nEp 2 (Step 018200): Train loss 2.895, Val loss 3.195\nEp 2 (Step 018300): Train loss 2.945, Val loss 3.203\nEp 2 (Step 018400): Train loss 2.954, Val loss 3.181\nEp 2 (Step 018500): Train loss 2.898, Val loss 3.193\nEp 2 (Step 018600): Train loss 2.891, Val loss 3.251\nEp 2 (Step 018700): Train loss 2.916, Val loss 3.150\nEp 2 (Step 018800): Train loss 2.908, Val loss 3.136\nEp 2 (Step 018900): Train loss 2.912, Val loss 3.183\nEp 2 (Step 019000): Train loss 2.946, Val loss 3.119\nEp 2 (Step 019100): Train loss 2.935, Val loss 3.149\nEp 2 (Step 019200): Train loss 2.917, Val loss 3.210\nEp 2 (Step 019300): Train loss 2.880, Val loss 3.143\nEp 2 (Step 019400): Train loss 3.016, Val loss 3.135\nEp 2 (Step 019500): Train loss 2.904, Val loss 3.186\nEp 2 (Step 019600): Train loss 2.876, Val loss 3.173\nEp 2 (Step 019700): Train loss 2.919, Val loss 3.216\nEp 2 (Step 019800): Train loss 2.839, Val loss 3.208\nEp 2 (Step 019900): Train loss 2.855, Val loss 3.149\nEp 2 (Step 020000): Train loss 2.822, Val loss 3.182\nEp 2 (Step 020100): Train loss 2.884, Val loss 3.170\nEp 2 (Step 020200): Train loss 2.891, Val loss 3.193\nEp 2 (Step 020300): Train loss 2.872, Val loss 3.168\nEp 2 (Step 020400): Train loss 2.863, Val loss 3.156\nEp 2 (Step 020500): Train loss 2.896, Val loss 3.160\nEp 2 (Step 020600): Train loss 2.872, Val loss 3.147\nEp 2 (Step 020700): Train loss 2.871, Val loss 3.188\nEp 2 (Step 020800): Train loss 2.879, Val loss 3.184\nEp 2 (Step 020900): Train loss 2.887, Val loss 3.178\nEp 2 (Step 021000): Train loss 2.835, Val loss 3.194\nEp 2 (Step 021100): Train loss 2.810, Val loss 3.180\nEp 2 (Step 021200): Train loss 2.764, Val loss 3.151\nEp 2 (Step 021300): Train loss 2.899, Val loss 3.194\nEp 2 (Step 021400): Train loss 2.844, Val loss 3.207\nEp 2 (Step 021500): Train loss 2.881, Val loss 3.164\nEp 2 (Step 021600): Train loss 2.899, Val loss 3.125\nEp 2 (Step 021700): Train loss 2.808, Val loss 3.161\nEp 2 (Step 021800): Train loss 2.881, Val loss 3.186\nEp 2 (Step 021900): Train loss 2.805, Val loss 3.168\nEp 2 (Step 022000): Train loss 2.870, Val loss 3.115\nEp 2 (Step 022100): Train loss 2.793, Val loss 3.141\nEp 2 (Step 022200): Train loss 2.799, Val loss 3.162\nEp 2 (Step 022300): Train loss 2.815, Val loss 3.115\nEp 2 (Step 022400): Train loss 2.853, Val loss 3.162\nEp 2 (Step 022500): Train loss 2.807, Val loss 3.159\nEp 2 (Step 022600): Train loss 2.797, Val loss 3.115\nEp 2 (Step 022700): Train loss 2.908, Val loss 3.186\nEp 2 (Step 022800): Train loss 2.817, Val loss 3.181\nEp 2 (Step 022900): Train loss 2.860, Val loss 3.185\nEp 2 (Step 023000): Train loss 2.844, Val loss 3.142\nEp 2 (Step 023100): Train loss 2.810, Val loss 3.148\nEp 2 (Step 023200): Train loss 2.831, Val loss 3.135\nEp 2 (Step 023300): Train loss 2.826, Val loss 3.181\nEp 2 (Step 023400): Train loss 2.831, Val loss 3.162\nEp 2 (Step 023500): Train loss 2.821, Val loss 3.164\nEp 2 (Step 023600): Train loss 2.866, Val loss 3.190\nEp 2 (Step 023700): Train loss 2.792, Val loss 3.228\nEp 2 (Step 023800): Train loss 2.767, Val loss 3.069\nEp 2 (Step 023900): Train loss 2.840, Val loss 3.172\nEp 2 (Step 024000): Train loss 2.843, Val loss 3.179\nEp 2 (Step 024100): Train loss 2.869, Val loss 3.119\nEp 2 (Step 024200): Train loss 2.792, Val loss 3.106\nEp 2 (Step 024300): Train loss 2.797, Val loss 3.116\nEp 2 (Step 024400): Train loss 2.782, Val loss 3.154\nEp 2 (Step 024500): Train loss 2.804, Val loss 3.167\nEp 2 (Step 024600): Train loss 2.820, Val loss 3.170\nEp 2 (Step 024700): Train loss 2.811, Val loss 3.178\nEp 2 (Step 024800): Train loss 2.854, Val loss 3.126\nEp 2 (Step 024900): Train loss 2.835, Val loss 3.127\nEp 2 (Step 025000): Train loss 2.807, Val loss 3.156\nEp 2 (Step 025100): Train loss 2.787, Val loss 3.151\nEp 2 (Step 025200): Train loss 2.748, Val loss 3.146\nEp 2 (Step 025300): Train loss 2.776, Val loss 3.072\n\nEXEMPLO DE GERAÇÃO:\nAmostra Gerada: 'Bom dia!  --Eu no queria que eu lhe diga, que no queria que eu lhe fizesse o que quizesse.  --Eu no queria que eu lhe diga que'\n\nGRÁFICO DE PERDA DURANTE O TREINO:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 500x300 with 2 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":"\nDESEMPENHO:\nTempo total: 3882.26 s\nTokens/s: 13346.97\nMemória máxima: 3243.45 MB\n\nTempo total de treino: 3858.61 s (64.31 min)\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"# Carregar modelo","metadata":{}},{"cell_type":"code","source":"def load_latest_model(config, model_class, device):\n    api = wandb.Api()\n    artifact = api.artifact(\n        f\"{config['user']}/{config['project']}/{config['name']}:latest\", type=\"model\"\n    )\n    artifact_dir = artifact.download()\n\n    checkpoint_path = os.path.join(artifact_dir, config[\"file_name\"])\n    print(f\"Carregando pesos do checkpoint: {checkpoint_path}\")\n\n    # Carregar o checkpoint completo\n    checkpoint = torch.load(checkpoint_path, map_location=device)\n\n    # Inicializa modelo com a mesma config\n    model = model_class(config, device).to(device)\n\n    model.load_state_dict(checkpoint[\"model_state\"])\n    print(\"Pesos carregaos com sucesso!\")\n\n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"config = {\n    \"vocab_size\": TOKENIZER.n_vocab,\n    \"embedding_dim\": 512,\n    \"context_length\": 384,\n    \"num_layers\": 8,\n    \"num_heads\": 8,\n    \"bias\": False,\n    \"max_epochs\": 2,\n    \"dtype\": torch.float32,\n    \"user\": \"matheus-figueiredo-silva-ufcg\",\n    \"project\": \"gpt2-original\",\n    \"name\": \"mini_mlp_final\",\n    \"run_id\": \"gpt2-original-run1\",\n    \"version\": \"v0\",\n    \"file_name\": \"mini_mlp.pth\"\n}\n\nmodel_loaded = load_latest_model(config, GPT2ModelOriginal, device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"generate_and_print_sample(model, TOKENIZER, device, start_context=\"lisboa\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T21:25:49.564210Z","iopub.execute_input":"2025-10-17T21:25:49.564917Z","iopub.status.idle":"2025-10-17T21:25:49.942898Z","shell.execute_reply.started":"2025-10-17T21:25:49.564892Z","shell.execute_reply":"2025-10-17T21:25:49.942089Z"}},"outputs":[{"name":"stdout","text":"Amostra Gerada: 'lisboa, e ao outro dia, que era o mais importante dos seus amigos, e que o mais velho de todos os seus amigos, e que o mais velho de todos'\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"def generate_text_temp(model, prompt_text, tokenizer, device, max_new_tokens=50, temperature=0.8, top_k=50):\n    model.eval()\n\n    idx = text_to_token_ids(prompt_text, tokenizer, device)\n\n    #context_size = model.pos_embeddings.weight.shape[0]\n    context_size = model.cfg[\"context_length\"]\n\n    for _ in range(max_new_tokens):\n        idx_cond = idx[:, -context_size:]\n\n        logits = model(idx_cond)\n        logits = logits[:, -1, :] / temperature\n        if top_k is not None:\n            v, ix = torch.topk(logits, top_k)\n            logits[logits < v[:, [-1]]] = -float('inf')\n\n        probs = torch.nn.functional.softmax(logits, dim=-1)\n        next_id = torch.multinomial(probs, num_samples=1)\n        idx = torch.cat((idx, next_id), dim=1)\n\n    decoded = token_ids_to_text(idx, tokenizer)\n    #print(f\"Amostra Gerada: '{decoded.replace(os.linesep, ' ')}'\")\n\n    model.train()\n    return decoded","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T02:28:42.031746Z","iopub.execute_input":"2025-10-18T02:28:42.032355Z","iopub.status.idle":"2025-10-18T02:28:42.038152Z","shell.execute_reply.started":"2025-10-18T02:28:42.032332Z","shell.execute_reply":"2025-10-18T02:28:42.037242Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"generate_text_temp(model, \"bom dia\", tokenizer=TOKENIZER, device=device, max_new_tokens=60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T21:41:09.872899Z","iopub.execute_input":"2025-10-17T21:41:09.873518Z","iopub.status.idle":"2025-10-17T21:41:10.456723Z","shell.execute_reply.started":"2025-10-17T21:41:09.873495Z","shell.execute_reply":"2025-10-17T21:41:10.456077Z"}},"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"'bom dia de noite ao seu hospede. Apenas achou a esperana, porque as pessoas que iam, como estavam muito bruscas e muito linda saborosas, podiam fazer-se'"},"metadata":{}}],"execution_count":32},{"cell_type":"markdown","source":"# Perplexidade","metadata":{}},{"cell_type":"code","source":"def compute_perplexity(model, data_loader, device):\n    model.eval()\n    total_loss = 0\n    n_batches = 0\n\n    with torch.no_grad():\n        for batch_idx, (x, y) in enumerate(data_loader):\n            x, y = x.to(device), y.to(device)\n            loss = calc_loss_batch_by_cross_entropy(model, x, y, device)\n\n            print(f\"Batch {batch_idx+1}/{len(data_loader)}, Loss: {loss.item():.4f}\")\n\n            total_loss += loss.item()\n            n_batches += 1\n\n            if (batch_idx + 1) % 10 == 0:\n                avg_loss_partial = total_loss / n_batches\n                print(f\"  Média parcial até aqui: {avg_loss_partial:.4f}\")\n\n    avg_loss = total_loss / n_batches\n    print(f\"Loss média final no dataset de teste: {avg_loss:.4f}\")\n\n    perplexity = torch.exp(torch.tensor(avg_loss))\n    print(f\"Perplexidade calculada: {perplexity.item():.2f}\")\n\n    return perplexity.item()\n\nperplexity_test = compute_perplexity(model, test_loader, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T20:52:31.906364Z","iopub.execute_input":"2025-10-17T20:52:31.907071Z","iopub.status.idle":"2025-10-17T20:53:45.622313Z","shell.execute_reply.started":"2025-10-17T20:52:31.907047Z","shell.execute_reply":"2025-10-17T20:53:45.621465Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Batch 1/1683, Loss: 2.9609\nBatch 2/1683, Loss: 2.7716\nBatch 3/1683, Loss: 3.1394\nBatch 4/1683, Loss: 3.1359\nBatch 5/1683, Loss: 3.2410\nBatch 6/1683, Loss: 3.6303\nBatch 7/1683, Loss: 3.3782\nBatch 8/1683, Loss: 2.8299\nBatch 9/1683, Loss: 2.9114\nBatch 10/1683, Loss: 2.9334\n  Média parcial até aqui: 3.0932\nBatch 11/1683, Loss: 3.1096\nBatch 12/1683, Loss: 3.0884\nBatch 13/1683, Loss: 3.0380\nBatch 14/1683, Loss: 3.3291\nBatch 15/1683, Loss: 3.4281\nBatch 16/1683, Loss: 3.0453\nBatch 17/1683, Loss: 3.5430\nBatch 18/1683, Loss: 3.1123\nBatch 19/1683, Loss: 3.2825\nBatch 20/1683, Loss: 3.9358\n  Média parcial até aqui: 3.1922\nBatch 21/1683, Loss: 3.6103\nBatch 22/1683, Loss: 3.2874\nBatch 23/1683, Loss: 3.4338\nBatch 24/1683, Loss: 3.1249\nBatch 25/1683, Loss: 3.8929\nBatch 26/1683, Loss: 2.9227\nBatch 27/1683, Loss: 3.0308\nBatch 28/1683, Loss: 3.2628\nBatch 29/1683, Loss: 3.1549\nBatch 30/1683, Loss: 3.0843\n  Média parcial até aqui: 3.2216\nBatch 31/1683, Loss: 3.1147\nBatch 32/1683, Loss: 3.6929\nBatch 33/1683, Loss: 3.1512\nBatch 34/1683, Loss: 3.2192\nBatch 35/1683, Loss: 2.8203\nBatch 36/1683, Loss: 3.3921\nBatch 37/1683, Loss: 3.1303\nBatch 38/1683, Loss: 3.2304\nBatch 39/1683, Loss: 2.8936\nBatch 40/1683, Loss: 3.1148\n  Média parcial até aqui: 3.2102\nBatch 41/1683, Loss: 3.2764\nBatch 42/1683, Loss: 3.2247\nBatch 43/1683, Loss: 2.5801\nBatch 44/1683, Loss: 3.1206\nBatch 45/1683, Loss: 3.4087\nBatch 46/1683, Loss: 3.0406\nBatch 47/1683, Loss: 3.4616\nBatch 48/1683, Loss: 3.4319\nBatch 49/1683, Loss: 3.6717\nBatch 50/1683, Loss: 3.1878\n  Média parcial até aqui: 3.2162\nBatch 51/1683, Loss: 2.7356\nBatch 52/1683, Loss: 3.2283\nBatch 53/1683, Loss: 3.2540\nBatch 54/1683, Loss: 3.1673\nBatch 55/1683, Loss: 3.7278\nBatch 56/1683, Loss: 3.4730\nBatch 57/1683, Loss: 3.6792\nBatch 58/1683, Loss: 3.3771\nBatch 59/1683, Loss: 3.1481\nBatch 60/1683, Loss: 3.1845\n  Média parcial até aqui: 3.2298\nBatch 61/1683, Loss: 3.1698\nBatch 62/1683, Loss: 3.3670\nBatch 63/1683, Loss: 3.4083\nBatch 64/1683, Loss: 2.9554\nBatch 65/1683, Loss: 3.1834\nBatch 66/1683, Loss: 3.1655\nBatch 67/1683, Loss: 3.2705\nBatch 68/1683, Loss: 3.2622\nBatch 69/1683, Loss: 3.7718\nBatch 70/1683, Loss: 2.8476\n  Média parcial até aqui: 3.2313\nBatch 71/1683, Loss: 3.1703\nBatch 72/1683, Loss: 3.2667\nBatch 73/1683, Loss: 3.2718\nBatch 74/1683, Loss: 3.5839\nBatch 75/1683, Loss: 3.2590\nBatch 76/1683, Loss: 3.6838\nBatch 77/1683, Loss: 3.1617\nBatch 78/1683, Loss: 2.8400\nBatch 79/1683, Loss: 3.3567\nBatch 80/1683, Loss: 3.0431\n  Média parcial até aqui: 3.2353\nBatch 81/1683, Loss: 3.2600\nBatch 82/1683, Loss: 3.3496\nBatch 83/1683, Loss: 3.4120\nBatch 84/1683, Loss: 3.3498\nBatch 85/1683, Loss: 3.1517\nBatch 86/1683, Loss: 3.5479\nBatch 87/1683, Loss: 2.9871\nBatch 88/1683, Loss: 3.0017\nBatch 89/1683, Loss: 3.0580\nBatch 90/1683, Loss: 3.2321\n  Média parcial até aqui: 3.2353\nBatch 91/1683, Loss: 3.3495\nBatch 92/1683, Loss: 3.1302\nBatch 93/1683, Loss: 3.4748\nBatch 94/1683, Loss: 3.2679\nBatch 95/1683, Loss: 3.4340\nBatch 96/1683, Loss: 3.4051\nBatch 97/1683, Loss: 3.5164\nBatch 98/1683, Loss: 3.3288\nBatch 99/1683, Loss: 3.1783\nBatch 100/1683, Loss: 3.2004\n  Média parcial até aqui: 3.2446\nBatch 101/1683, Loss: 3.2388\nBatch 102/1683, Loss: 3.3696\nBatch 103/1683, Loss: 3.5259\nBatch 104/1683, Loss: 3.3485\nBatch 105/1683, Loss: 3.1617\nBatch 106/1683, Loss: 3.4959\nBatch 107/1683, Loss: 3.6870\nBatch 108/1683, Loss: 3.7198\nBatch 109/1683, Loss: 3.5778\nBatch 110/1683, Loss: 3.1715\n  Média parcial até aqui: 3.2614\nBatch 111/1683, Loss: 3.0994\nBatch 112/1683, Loss: 3.3430\nBatch 113/1683, Loss: 3.1183\nBatch 114/1683, Loss: 3.8370\nBatch 115/1683, Loss: 3.3614\nBatch 116/1683, Loss: 3.3831\nBatch 117/1683, Loss: 3.1221\nBatch 118/1683, Loss: 3.0278\nBatch 119/1683, Loss: 3.2356\nBatch 120/1683, Loss: 3.6903\n  Média parcial até aqui: 3.2665\nBatch 121/1683, Loss: 3.2899\nBatch 122/1683, Loss: 3.1790\nBatch 123/1683, Loss: 3.2326\nBatch 124/1683, Loss: 3.3117\nBatch 125/1683, Loss: 2.9214\nBatch 126/1683, Loss: 3.2619\nBatch 127/1683, Loss: 3.0339\nBatch 128/1683, Loss: 3.5726\nBatch 129/1683, Loss: 3.1973\nBatch 130/1683, Loss: 3.2150\n  Média parcial até aqui: 3.2630\nBatch 131/1683, Loss: 3.7308\nBatch 132/1683, Loss: 3.2005\nBatch 133/1683, Loss: 3.5971\nBatch 134/1683, Loss: 3.1258\nBatch 135/1683, Loss: 3.0347\nBatch 136/1683, Loss: 3.4263\nBatch 137/1683, Loss: 3.5352\nBatch 138/1683, Loss: 2.9052\nBatch 139/1683, Loss: 3.4008\nBatch 140/1683, Loss: 3.4215\n  Média parcial até aqui: 3.2683\nBatch 141/1683, Loss: 3.4912\nBatch 142/1683, Loss: 3.2014\nBatch 143/1683, Loss: 3.6605\nBatch 144/1683, Loss: 3.0924\nBatch 145/1683, Loss: 3.5958\nBatch 146/1683, Loss: 2.9878\nBatch 147/1683, Loss: 3.2046\nBatch 148/1683, Loss: 3.5630\nBatch 149/1683, Loss: 3.2846\nBatch 150/1683, Loss: 3.2278\n  Média parcial até aqui: 3.2725\nBatch 151/1683, Loss: 3.1130\nBatch 152/1683, Loss: 3.2299\nBatch 153/1683, Loss: 3.7090\nBatch 154/1683, Loss: 3.2934\nBatch 155/1683, Loss: 3.5973\nBatch 156/1683, Loss: 3.5139\nBatch 157/1683, Loss: 3.2512\nBatch 158/1683, Loss: 3.5418\nBatch 159/1683, Loss: 3.1190\nBatch 160/1683, Loss: 3.3322\n  Média parcial até aqui: 3.2786\nBatch 161/1683, Loss: 3.0459\nBatch 162/1683, Loss: 3.2697\nBatch 163/1683, Loss: 2.9248\nBatch 164/1683, Loss: 2.9418\nBatch 165/1683, Loss: 3.4810\nBatch 166/1683, Loss: 3.4635\nBatch 167/1683, Loss: 2.9231\nBatch 168/1683, Loss: 3.5572\nBatch 169/1683, Loss: 2.8666\nBatch 170/1683, Loss: 3.7669\n  Média parcial até aqui: 3.2754\nBatch 171/1683, Loss: 2.7181\nBatch 172/1683, Loss: 3.2531\nBatch 173/1683, Loss: 2.9698\nBatch 174/1683, Loss: 3.2783\nBatch 175/1683, Loss: 3.6648\nBatch 176/1683, Loss: 3.1291\nBatch 177/1683, Loss: 3.0592\nBatch 178/1683, Loss: 3.4200\nBatch 179/1683, Loss: 3.6102\nBatch 180/1683, Loss: 2.9372\n  Média parcial até aqui: 3.2714\nBatch 181/1683, Loss: 3.1411\nBatch 182/1683, Loss: 3.4031\nBatch 183/1683, Loss: 3.3191\nBatch 184/1683, Loss: 3.5907\nBatch 185/1683, Loss: 3.3141\nBatch 186/1683, Loss: 3.2647\nBatch 187/1683, Loss: 2.7751\nBatch 188/1683, Loss: 3.4018\nBatch 189/1683, Loss: 3.2493\nBatch 190/1683, Loss: 3.1038\n  Média parcial até aqui: 3.2706\nBatch 191/1683, Loss: 3.1142\nBatch 192/1683, Loss: 3.1696\nBatch 193/1683, Loss: 3.7776\nBatch 194/1683, Loss: 3.5902\nBatch 195/1683, Loss: 3.2988\nBatch 196/1683, Loss: 2.9125\nBatch 197/1683, Loss: 4.0177\nBatch 198/1683, Loss: 3.2592\nBatch 199/1683, Loss: 3.4149\nBatch 200/1683, Loss: 3.1598\n  Média parcial até aqui: 3.2757\nBatch 201/1683, Loss: 3.0354\nBatch 202/1683, Loss: 3.2912\nBatch 203/1683, Loss: 2.9127\nBatch 204/1683, Loss: 2.8652\nBatch 205/1683, Loss: 3.1617\nBatch 206/1683, Loss: 3.5457\nBatch 207/1683, Loss: 3.3364\nBatch 208/1683, Loss: 3.6118\nBatch 209/1683, Loss: 3.6521\nBatch 210/1683, Loss: 3.3263\n  Média parcial até aqui: 3.2756\nBatch 211/1683, Loss: 3.5712\nBatch 212/1683, Loss: 3.3802\nBatch 213/1683, Loss: 3.2524\nBatch 214/1683, Loss: 3.2493\nBatch 215/1683, Loss: 3.2777\nBatch 216/1683, Loss: 3.2315\nBatch 217/1683, Loss: 3.3619\nBatch 218/1683, Loss: 3.1366\nBatch 219/1683, Loss: 3.6007\nBatch 220/1683, Loss: 3.4487\n  Média parcial até aqui: 3.2790\nBatch 221/1683, Loss: 3.3238\nBatch 222/1683, Loss: 3.2032\nBatch 223/1683, Loss: 3.4910\nBatch 224/1683, Loss: 3.0567\nBatch 225/1683, Loss: 3.1464\nBatch 226/1683, Loss: 3.0984\nBatch 227/1683, Loss: 3.4207\nBatch 228/1683, Loss: 3.0753\nBatch 229/1683, Loss: 3.6353\nBatch 230/1683, Loss: 2.9899\n  Média parcial até aqui: 3.2775\nBatch 231/1683, Loss: 3.2211\nBatch 232/1683, Loss: 2.8305\nBatch 233/1683, Loss: 3.1682\nBatch 234/1683, Loss: 3.3816\nBatch 235/1683, Loss: 2.8724\nBatch 236/1683, Loss: 3.3195\nBatch 237/1683, Loss: 3.2007\nBatch 238/1683, Loss: 3.0962\nBatch 239/1683, Loss: 3.0486\nBatch 240/1683, Loss: 3.2045\n  Média parcial até aqui: 3.2715\nBatch 241/1683, Loss: 3.2499\nBatch 242/1683, Loss: 3.2284\nBatch 243/1683, Loss: 3.3685\nBatch 244/1683, Loss: 3.5851\nBatch 245/1683, Loss: 2.8882\nBatch 246/1683, Loss: 3.3535\nBatch 247/1683, Loss: 3.1909\nBatch 248/1683, Loss: 3.3346\nBatch 249/1683, Loss: 2.7724\nBatch 250/1683, Loss: 3.2109\n  Média parcial até aqui: 3.2694\nBatch 251/1683, Loss: 3.4488\nBatch 252/1683, Loss: 3.1907\nBatch 253/1683, Loss: 3.3058\nBatch 254/1683, Loss: 3.3569\nBatch 255/1683, Loss: 2.9090\nBatch 256/1683, Loss: 3.7093\nBatch 257/1683, Loss: 3.2425\nBatch 258/1683, Loss: 3.0669\nBatch 259/1683, Loss: 3.4122\nBatch 260/1683, Loss: 3.4014\n  Média parcial até aqui: 3.2707\nBatch 261/1683, Loss: 3.5494\nBatch 262/1683, Loss: 3.2223\nBatch 263/1683, Loss: 3.0831\nBatch 264/1683, Loss: 3.0194\nBatch 265/1683, Loss: 3.1145\nBatch 266/1683, Loss: 3.1841\nBatch 267/1683, Loss: 3.4821\nBatch 268/1683, Loss: 3.1683\nBatch 269/1683, Loss: 2.9920\nBatch 270/1683, Loss: 3.4896\n  Média parcial até aqui: 3.2693\nBatch 271/1683, Loss: 3.7770\nBatch 272/1683, Loss: 3.8981\nBatch 273/1683, Loss: 2.7607\nBatch 274/1683, Loss: 3.4028\nBatch 275/1683, Loss: 3.0131\nBatch 276/1683, Loss: 3.1383\nBatch 277/1683, Loss: 3.2213\nBatch 278/1683, Loss: 3.1885\nBatch 279/1683, Loss: 3.4553\nBatch 280/1683, Loss: 3.1400\n  Média parcial até aqui: 3.2703\nBatch 281/1683, Loss: 2.8776\nBatch 282/1683, Loss: 3.2837\nBatch 283/1683, Loss: 3.3614\nBatch 284/1683, Loss: 3.0995\nBatch 285/1683, Loss: 3.5320\nBatch 286/1683, Loss: 3.2288\nBatch 287/1683, Loss: 3.0116\nBatch 288/1683, Loss: 3.6114\nBatch 289/1683, Loss: 3.1585\nBatch 290/1683, Loss: 3.0681\n  Média parcial até aqui: 3.2687\nBatch 291/1683, Loss: 3.5765\nBatch 292/1683, Loss: 2.9708\nBatch 293/1683, Loss: 3.3855\nBatch 294/1683, Loss: 3.4760\nBatch 295/1683, Loss: 2.7565\nBatch 296/1683, Loss: 3.3131\nBatch 297/1683, Loss: 2.9664\nBatch 298/1683, Loss: 3.3966\nBatch 299/1683, Loss: 3.7127\nBatch 300/1683, Loss: 3.2561\n  Média parcial até aqui: 3.2691\nBatch 301/1683, Loss: 2.9803\nBatch 302/1683, Loss: 3.4176\nBatch 303/1683, Loss: 3.6319\nBatch 304/1683, Loss: 3.0060\nBatch 305/1683, Loss: 2.6779\nBatch 306/1683, Loss: 3.3038\nBatch 307/1683, Loss: 3.1557\nBatch 308/1683, Loss: 2.9796\nBatch 309/1683, Loss: 3.1286\nBatch 310/1683, Loss: 3.4294\n  Média parcial até aqui: 3.2660\nBatch 311/1683, Loss: 3.4377\nBatch 312/1683, Loss: 3.1819\nBatch 313/1683, Loss: 3.0569\nBatch 314/1683, Loss: 3.5163\nBatch 315/1683, Loss: 3.2838\nBatch 316/1683, Loss: 3.4013\nBatch 317/1683, Loss: 3.2087\nBatch 318/1683, Loss: 3.4063\nBatch 319/1683, Loss: 3.2902\nBatch 320/1683, Loss: 3.1035\n  Média parcial até aqui: 3.2667\nBatch 321/1683, Loss: 2.9552\nBatch 322/1683, Loss: 3.4977\nBatch 323/1683, Loss: 3.6493\nBatch 324/1683, Loss: 3.0248\nBatch 325/1683, Loss: 3.6354\nBatch 326/1683, Loss: 3.2915\nBatch 327/1683, Loss: 3.0942\nBatch 328/1683, Loss: 3.5955\nBatch 329/1683, Loss: 3.4274\nBatch 330/1683, Loss: 3.4604\n  Média parcial até aqui: 3.2696\nBatch 331/1683, Loss: 3.5016\nBatch 332/1683, Loss: 3.1405\nBatch 333/1683, Loss: 3.3546\nBatch 334/1683, Loss: 3.4028\nBatch 335/1683, Loss: 3.2588\nBatch 336/1683, Loss: 3.4750\nBatch 337/1683, Loss: 3.4857\nBatch 338/1683, Loss: 3.1586\nBatch 339/1683, Loss: 3.3337\nBatch 340/1683, Loss: 3.2516\n  Média parcial até aqui: 3.2716\nBatch 341/1683, Loss: 3.4780\nBatch 342/1683, Loss: 3.3977\nBatch 343/1683, Loss: 3.5355\nBatch 344/1683, Loss: 3.6059\nBatch 345/1683, Loss: 3.3150\nBatch 346/1683, Loss: 3.0993\nBatch 347/1683, Loss: 3.0350\nBatch 348/1683, Loss: 3.3636\nBatch 349/1683, Loss: 3.4985\nBatch 350/1683, Loss: 3.2666\n  Média parcial até aqui: 3.2741\nBatch 351/1683, Loss: 3.6283\nBatch 352/1683, Loss: 3.0774\nBatch 353/1683, Loss: 3.4488\nBatch 354/1683, Loss: 3.6043\nBatch 355/1683, Loss: 3.1037\nBatch 356/1683, Loss: 2.9608\nBatch 357/1683, Loss: 2.9994\nBatch 358/1683, Loss: 3.1332\nBatch 359/1683, Loss: 4.0656\nBatch 360/1683, Loss: 3.0678\n  Média parcial até aqui: 3.2750\nBatch 361/1683, Loss: 3.5269\nBatch 362/1683, Loss: 3.3528\nBatch 363/1683, Loss: 3.2852\nBatch 364/1683, Loss: 3.1271\nBatch 365/1683, Loss: 3.3271\nBatch 366/1683, Loss: 2.6702\nBatch 367/1683, Loss: 3.1235\nBatch 368/1683, Loss: 3.2107\nBatch 369/1683, Loss: 2.6818\nBatch 370/1683, Loss: 3.4630\n  Média parcial até aqui: 3.2724\nBatch 371/1683, Loss: 3.8084\nBatch 372/1683, Loss: 3.1520\nBatch 373/1683, Loss: 3.2422\nBatch 374/1683, Loss: 3.2276\nBatch 375/1683, Loss: 3.2252\nBatch 376/1683, Loss: 3.1202\nBatch 377/1683, Loss: 3.3081\nBatch 378/1683, Loss: 3.3786\nBatch 379/1683, Loss: 3.1835\nBatch 380/1683, Loss: 2.9316\n  Média parcial até aqui: 3.2720\nBatch 381/1683, Loss: 3.6393\nBatch 382/1683, Loss: 3.6911\nBatch 383/1683, Loss: 3.2719\nBatch 384/1683, Loss: 3.3544\nBatch 385/1683, Loss: 3.4344\nBatch 386/1683, Loss: 3.2986\nBatch 387/1683, Loss: 3.1525\nBatch 388/1683, Loss: 3.6758\nBatch 389/1683, Loss: 3.4792\nBatch 390/1683, Loss: 3.1527\n  Média parcial até aqui: 3.2757\nBatch 391/1683, Loss: 3.5972\nBatch 392/1683, Loss: 3.2544\nBatch 393/1683, Loss: 3.1242\nBatch 394/1683, Loss: 3.7362\nBatch 395/1683, Loss: 3.0147\nBatch 396/1683, Loss: 3.0570\nBatch 397/1683, Loss: 2.9942\nBatch 398/1683, Loss: 3.4678\nBatch 399/1683, Loss: 3.3868\nBatch 400/1683, Loss: 2.9810\n  Média parcial até aqui: 3.2753\nBatch 401/1683, Loss: 3.1735\nBatch 402/1683, Loss: 3.2719\nBatch 403/1683, Loss: 3.4529\nBatch 404/1683, Loss: 3.2858\nBatch 405/1683, Loss: 3.1833\nBatch 406/1683, Loss: 3.0775\nBatch 407/1683, Loss: 3.2951\nBatch 408/1683, Loss: 3.2907\nBatch 409/1683, Loss: 3.2023\nBatch 410/1683, Loss: 3.2604\n  Média parcial até aqui: 3.2747\nBatch 411/1683, Loss: 3.0247\nBatch 412/1683, Loss: 3.1445\nBatch 413/1683, Loss: 3.1527\nBatch 414/1683, Loss: 3.0731\nBatch 415/1683, Loss: 3.2942\nBatch 416/1683, Loss: 3.3201\nBatch 417/1683, Loss: 3.7903\nBatch 418/1683, Loss: 3.5165\nBatch 419/1683, Loss: 3.2687\nBatch 420/1683, Loss: 3.1225\n  Média parcial até aqui: 3.2746\nBatch 421/1683, Loss: 3.4580\nBatch 422/1683, Loss: 3.8201\nBatch 423/1683, Loss: 2.9315\nBatch 424/1683, Loss: 3.1581\nBatch 425/1683, Loss: 3.3277\nBatch 426/1683, Loss: 3.6136\nBatch 427/1683, Loss: 4.1121\nBatch 428/1683, Loss: 3.3402\nBatch 429/1683, Loss: 3.1161\nBatch 430/1683, Loss: 3.3861\n  Média parcial até aqui: 3.2781\nBatch 431/1683, Loss: 3.2940\nBatch 432/1683, Loss: 3.0205\nBatch 433/1683, Loss: 3.0716\nBatch 434/1683, Loss: 3.1995\nBatch 435/1683, Loss: 3.3759\nBatch 436/1683, Loss: 3.4670\nBatch 437/1683, Loss: 3.6798\nBatch 438/1683, Loss: 3.0860\nBatch 439/1683, Loss: 3.1423\nBatch 440/1683, Loss: 3.2158\n  Média parcial até aqui: 3.2776\nBatch 441/1683, Loss: 3.0335\nBatch 442/1683, Loss: 3.0387\nBatch 443/1683, Loss: 3.1220\nBatch 444/1683, Loss: 3.4225\nBatch 445/1683, Loss: 3.4235\nBatch 446/1683, Loss: 3.5126\nBatch 447/1683, Loss: 3.5166\nBatch 448/1683, Loss: 3.4513\nBatch 449/1683, Loss: 3.1674\nBatch 450/1683, Loss: 3.0194\n  Média parcial até aqui: 3.2774\nBatch 451/1683, Loss: 3.1797\nBatch 452/1683, Loss: 3.1423\nBatch 453/1683, Loss: 2.5865\nBatch 454/1683, Loss: 3.2785\nBatch 455/1683, Loss: 3.5766\nBatch 456/1683, Loss: 3.3700\nBatch 457/1683, Loss: 3.2045\nBatch 458/1683, Loss: 3.7573\nBatch 459/1683, Loss: 3.3212\nBatch 460/1683, Loss: 3.3189\n  Média parcial até aqui: 3.2774\nBatch 461/1683, Loss: 3.4551\nBatch 462/1683, Loss: 3.4089\nBatch 463/1683, Loss: 2.9370\nBatch 464/1683, Loss: 3.0238\nBatch 465/1683, Loss: 3.4647\nBatch 466/1683, Loss: 3.3977\nBatch 467/1683, Loss: 3.5814\nBatch 468/1683, Loss: 3.3582\nBatch 469/1683, Loss: 3.0690\nBatch 470/1683, Loss: 3.2067\n  Média parcial até aqui: 3.2776\nBatch 471/1683, Loss: 3.4525\nBatch 472/1683, Loss: 3.4112\nBatch 473/1683, Loss: 3.4771\nBatch 474/1683, Loss: 3.0751\nBatch 475/1683, Loss: 3.2625\nBatch 476/1683, Loss: 2.9546\nBatch 477/1683, Loss: 3.1590\nBatch 478/1683, Loss: 3.2855\nBatch 479/1683, Loss: 2.8217\nBatch 480/1683, Loss: 3.5689\n  Média parcial até aqui: 3.2770\nBatch 481/1683, Loss: 3.2612\nBatch 482/1683, Loss: 3.5662\nBatch 483/1683, Loss: 3.3456\nBatch 484/1683, Loss: 3.4096\nBatch 485/1683, Loss: 3.3093\nBatch 486/1683, Loss: 3.8026\nBatch 487/1683, Loss: 3.0562\nBatch 488/1683, Loss: 3.1943\nBatch 489/1683, Loss: 2.9992\nBatch 490/1683, Loss: 3.6280\n  Média parcial até aqui: 3.2786\nBatch 491/1683, Loss: 3.6053\nBatch 492/1683, Loss: 3.0192\nBatch 493/1683, Loss: 3.1107\nBatch 494/1683, Loss: 3.3209\nBatch 495/1683, Loss: 3.2827\nBatch 496/1683, Loss: 2.9350\nBatch 497/1683, Loss: 3.2249\nBatch 498/1683, Loss: 3.4521\nBatch 499/1683, Loss: 3.6164\nBatch 500/1683, Loss: 3.0681\n  Média parcial até aqui: 3.2783\nBatch 501/1683, Loss: 3.2568\nBatch 502/1683, Loss: 2.9915\nBatch 503/1683, Loss: 3.2869\nBatch 504/1683, Loss: 3.1275\nBatch 505/1683, Loss: 3.3128\nBatch 506/1683, Loss: 3.3348\nBatch 507/1683, Loss: 3.2826\nBatch 508/1683, Loss: 3.4102\nBatch 509/1683, Loss: 3.3603\nBatch 510/1683, Loss: 3.3149\n  Média parcial até aqui: 3.2781\nBatch 511/1683, Loss: 3.2749\nBatch 512/1683, Loss: 3.1996\nBatch 513/1683, Loss: 3.3210\nBatch 514/1683, Loss: 3.0813\nBatch 515/1683, Loss: 3.1308\nBatch 516/1683, Loss: 3.1862\nBatch 517/1683, Loss: 3.3724\nBatch 518/1683, Loss: 3.3949\nBatch 519/1683, Loss: 3.3084\nBatch 520/1683, Loss: 3.5344\n  Média parcial até aqui: 3.2782\nBatch 521/1683, Loss: 3.4903\nBatch 522/1683, Loss: 3.3310\nBatch 523/1683, Loss: 3.0438\nBatch 524/1683, Loss: 2.8097\nBatch 525/1683, Loss: 3.4784\nBatch 526/1683, Loss: 2.8595\nBatch 527/1683, Loss: 3.3339\nBatch 528/1683, Loss: 3.0333\nBatch 529/1683, Loss: 2.7808\nBatch 530/1683, Loss: 2.8936\n  Média parcial até aqui: 3.2749\nBatch 531/1683, Loss: 2.9939\nBatch 532/1683, Loss: 3.2913\nBatch 533/1683, Loss: 3.4269\nBatch 534/1683, Loss: 3.2902\nBatch 535/1683, Loss: 3.0959\nBatch 536/1683, Loss: 3.2454\nBatch 537/1683, Loss: 3.1243\nBatch 538/1683, Loss: 3.3535\nBatch 539/1683, Loss: 3.1305\nBatch 540/1683, Loss: 3.3967\n  Média parcial até aqui: 3.2742\nBatch 541/1683, Loss: 3.4084\nBatch 542/1683, Loss: 3.1444\nBatch 543/1683, Loss: 2.8795\nBatch 544/1683, Loss: 3.1106\nBatch 545/1683, Loss: 3.4737\nBatch 546/1683, Loss: 3.1534\nBatch 547/1683, Loss: 2.9891\nBatch 548/1683, Loss: 3.7799\nBatch 549/1683, Loss: 3.3605\nBatch 550/1683, Loss: 3.1674\n  Média parcial até aqui: 3.2737\nBatch 551/1683, Loss: 3.6669\nBatch 552/1683, Loss: 3.3893\nBatch 553/1683, Loss: 3.3862\nBatch 554/1683, Loss: 3.1371\nBatch 555/1683, Loss: 3.6046\nBatch 556/1683, Loss: 3.3664\nBatch 557/1683, Loss: 3.1163\nBatch 558/1683, Loss: 3.7351\nBatch 559/1683, Loss: 2.9972\nBatch 560/1683, Loss: 3.5759\n  Média parcial até aqui: 3.2759\nBatch 561/1683, Loss: 3.1360\nBatch 562/1683, Loss: 2.8986\nBatch 563/1683, Loss: 3.3742\nBatch 564/1683, Loss: 3.3609\nBatch 565/1683, Loss: 3.1887\nBatch 566/1683, Loss: 3.6399\nBatch 567/1683, Loss: 3.2255\nBatch 568/1683, Loss: 2.9289\nBatch 569/1683, Loss: 3.3097\nBatch 570/1683, Loss: 3.4826\n  Média parcial até aqui: 3.2755\nBatch 571/1683, Loss: 3.1166\nBatch 572/1683, Loss: 3.3418\nBatch 573/1683, Loss: 3.2568\nBatch 574/1683, Loss: 3.3740\nBatch 575/1683, Loss: 3.1593\nBatch 576/1683, Loss: 3.6254\nBatch 577/1683, Loss: 3.0195\nBatch 578/1683, Loss: 3.0087\nBatch 579/1683, Loss: 3.3923\nBatch 580/1683, Loss: 3.0225\n  Média parcial até aqui: 3.2747\nBatch 581/1683, Loss: 3.1529\nBatch 582/1683, Loss: 3.2124\nBatch 583/1683, Loss: 3.0375\nBatch 584/1683, Loss: 3.3833\nBatch 585/1683, Loss: 3.1129\nBatch 586/1683, Loss: 3.2702\nBatch 587/1683, Loss: 3.4529\nBatch 588/1683, Loss: 2.9471\nBatch 589/1683, Loss: 3.4490\nBatch 590/1683, Loss: 2.8618\n  Média parcial até aqui: 3.2733\nBatch 591/1683, Loss: 3.5561\nBatch 592/1683, Loss: 3.0783\nBatch 593/1683, Loss: 2.8110\nBatch 594/1683, Loss: 3.3066\nBatch 595/1683, Loss: 3.2049\nBatch 596/1683, Loss: 3.5913\nBatch 597/1683, Loss: 3.1220\nBatch 598/1683, Loss: 3.7565\nBatch 599/1683, Loss: 3.6639\nBatch 600/1683, Loss: 3.0841\n  Média parcial até aqui: 3.2740\nBatch 601/1683, Loss: 3.0032\nBatch 602/1683, Loss: 3.2817\nBatch 603/1683, Loss: 3.2586\nBatch 604/1683, Loss: 3.1435\nBatch 605/1683, Loss: 3.0698\nBatch 606/1683, Loss: 3.0027\nBatch 607/1683, Loss: 2.8767\nBatch 608/1683, Loss: 3.1695\nBatch 609/1683, Loss: 3.3794\nBatch 610/1683, Loss: 3.2691\n  Média parcial até aqui: 3.2719\nBatch 611/1683, Loss: 3.3149\nBatch 612/1683, Loss: 3.5605\nBatch 613/1683, Loss: 3.1365\nBatch 614/1683, Loss: 3.4011\nBatch 615/1683, Loss: 3.6401\nBatch 616/1683, Loss: 3.1249\nBatch 617/1683, Loss: 3.1754\nBatch 618/1683, Loss: 3.1412\nBatch 619/1683, Loss: 3.2363\nBatch 620/1683, Loss: 3.6459\n  Média parcial até aqui: 3.2730\nBatch 621/1683, Loss: 3.0744\nBatch 622/1683, Loss: 3.2702\nBatch 623/1683, Loss: 3.3450\nBatch 624/1683, Loss: 3.3335\nBatch 625/1683, Loss: 3.0578\nBatch 626/1683, Loss: 2.4379\nBatch 627/1683, Loss: 3.1463\nBatch 628/1683, Loss: 3.3981\nBatch 629/1683, Loss: 2.7500\nBatch 630/1683, Loss: 3.1870\n  Média parcial até aqui: 3.2702\nBatch 631/1683, Loss: 3.3758\nBatch 632/1683, Loss: 3.3010\nBatch 633/1683, Loss: 3.1503\nBatch 634/1683, Loss: 2.8917\nBatch 635/1683, Loss: 3.0562\nBatch 636/1683, Loss: 3.2815\nBatch 637/1683, Loss: 2.9505\nBatch 638/1683, Loss: 3.1361\nBatch 639/1683, Loss: 3.3418\nBatch 640/1683, Loss: 3.6378\n  Média parcial até aqui: 3.2693\nBatch 641/1683, Loss: 3.4322\nBatch 642/1683, Loss: 3.6347\nBatch 643/1683, Loss: 2.7987\nBatch 644/1683, Loss: 3.4763\nBatch 645/1683, Loss: 3.2093\nBatch 646/1683, Loss: 3.3589\nBatch 647/1683, Loss: 3.3209\nBatch 648/1683, Loss: 3.1309\nBatch 649/1683, Loss: 2.9040\nBatch 650/1683, Loss: 3.3350\n  Média parcial até aqui: 3.2692\nBatch 651/1683, Loss: 3.3327\nBatch 652/1683, Loss: 3.2828\nBatch 653/1683, Loss: 3.0035\nBatch 654/1683, Loss: 2.9061\nBatch 655/1683, Loss: 3.2643\nBatch 656/1683, Loss: 3.2515\nBatch 657/1683, Loss: 3.0895\nBatch 658/1683, Loss: 3.3811\nBatch 659/1683, Loss: 4.0234\nBatch 660/1683, Loss: 3.1771\n  Média parcial até aqui: 3.2692\nBatch 661/1683, Loss: 3.1741\nBatch 662/1683, Loss: 3.4128\nBatch 663/1683, Loss: 3.0469\nBatch 664/1683, Loss: 3.6052\nBatch 665/1683, Loss: 3.6448\nBatch 666/1683, Loss: 3.1433\nBatch 667/1683, Loss: 3.3204\nBatch 668/1683, Loss: 3.2271\nBatch 669/1683, Loss: 3.7323\nBatch 670/1683, Loss: 3.0705\n  Média parcial até aqui: 3.2702\nBatch 671/1683, Loss: 2.9575\nBatch 672/1683, Loss: 2.9104\nBatch 673/1683, Loss: 3.8717\nBatch 674/1683, Loss: 2.9365\nBatch 675/1683, Loss: 3.0389\nBatch 676/1683, Loss: 3.5247\nBatch 677/1683, Loss: 3.8216\nBatch 678/1683, Loss: 2.9630\nBatch 679/1683, Loss: 3.0787\nBatch 680/1683, Loss: 3.0208\n  Média parcial até aqui: 3.2694\nBatch 681/1683, Loss: 3.1620\nBatch 682/1683, Loss: 3.0136\nBatch 683/1683, Loss: 3.3793\nBatch 684/1683, Loss: 3.4894\nBatch 685/1683, Loss: 3.3491\nBatch 686/1683, Loss: 3.0129\nBatch 687/1683, Loss: 3.3047\nBatch 688/1683, Loss: 3.4625\nBatch 689/1683, Loss: 3.2953\nBatch 690/1683, Loss: 3.0394\n  Média parcial até aqui: 3.2691\nBatch 691/1683, Loss: 3.1128\nBatch 692/1683, Loss: 3.3226\nBatch 693/1683, Loss: 3.0271\nBatch 694/1683, Loss: 3.5144\nBatch 695/1683, Loss: 3.7016\nBatch 696/1683, Loss: 3.6738\nBatch 697/1683, Loss: 3.5585\nBatch 698/1683, Loss: 3.1372\nBatch 699/1683, Loss: 3.2177\nBatch 700/1683, Loss: 3.4529\n  Média parcial até aqui: 3.2706\nBatch 701/1683, Loss: 3.3214\nBatch 702/1683, Loss: 3.1306\nBatch 703/1683, Loss: 3.2171\nBatch 704/1683, Loss: 3.3476\nBatch 705/1683, Loss: 3.6587\nBatch 706/1683, Loss: 3.2155\nBatch 707/1683, Loss: 2.9911\nBatch 708/1683, Loss: 2.9591\nBatch 709/1683, Loss: 3.2140\nBatch 710/1683, Loss: 2.9362\n  Média parcial até aqui: 3.2696\nBatch 711/1683, Loss: 4.0218\nBatch 712/1683, Loss: 3.7546\nBatch 713/1683, Loss: 3.3610\nBatch 714/1683, Loss: 3.3410\nBatch 715/1683, Loss: 3.0546\nBatch 716/1683, Loss: 3.1497\nBatch 717/1683, Loss: 3.3974\nBatch 718/1683, Loss: 3.3218\nBatch 719/1683, Loss: 3.3687\nBatch 720/1683, Loss: 3.1259\n  Média parcial até aqui: 3.2712\nBatch 721/1683, Loss: 2.9303\nBatch 722/1683, Loss: 3.3770\nBatch 723/1683, Loss: 3.3363\nBatch 724/1683, Loss: 3.3751\nBatch 725/1683, Loss: 3.3998\nBatch 726/1683, Loss: 3.1282\nBatch 727/1683, Loss: 3.2027\nBatch 728/1683, Loss: 3.5982\nBatch 729/1683, Loss: 3.2399\nBatch 730/1683, Loss: 3.4832\n  Média parcial até aqui: 3.2717\nBatch 731/1683, Loss: 3.7177\nBatch 732/1683, Loss: 3.1308\nBatch 733/1683, Loss: 3.0668\nBatch 734/1683, Loss: 3.4223\nBatch 735/1683, Loss: 3.0304\nBatch 736/1683, Loss: 3.5936\nBatch 737/1683, Loss: 3.4718\nBatch 738/1683, Loss: 2.8662\nBatch 739/1683, Loss: 3.6356\nBatch 740/1683, Loss: 3.0397\n  Média parcial até aqui: 3.2721\nBatch 741/1683, Loss: 3.3029\nBatch 742/1683, Loss: 2.9199\nBatch 743/1683, Loss: 3.4189\nBatch 744/1683, Loss: 3.0216\nBatch 745/1683, Loss: 3.1842\nBatch 746/1683, Loss: 3.2419\nBatch 747/1683, Loss: 3.0732\nBatch 748/1683, Loss: 3.3126\nBatch 749/1683, Loss: 3.5405\nBatch 750/1683, Loss: 3.2148\n  Média parcial até aqui: 3.2714\nBatch 751/1683, Loss: 3.0559\nBatch 752/1683, Loss: 3.6156\nBatch 753/1683, Loss: 3.1843\nBatch 754/1683, Loss: 3.0114\nBatch 755/1683, Loss: 3.1153\nBatch 756/1683, Loss: 3.1185\nBatch 757/1683, Loss: 3.0114\nBatch 758/1683, Loss: 3.4944\nBatch 759/1683, Loss: 3.4922\nBatch 760/1683, Loss: 3.6997\n  Média parcial até aqui: 3.2715\nBatch 761/1683, Loss: 3.5061\nBatch 762/1683, Loss: 3.4553\nBatch 763/1683, Loss: 3.1772\nBatch 764/1683, Loss: 3.7056\nBatch 765/1683, Loss: 3.2767\nBatch 766/1683, Loss: 3.2215\nBatch 767/1683, Loss: 3.1057\nBatch 768/1683, Loss: 3.2387\nBatch 769/1683, Loss: 3.1603\nBatch 770/1683, Loss: 3.5096\n  Média parcial até aqui: 3.2724\nBatch 771/1683, Loss: 3.0463\nBatch 772/1683, Loss: 3.5012\nBatch 773/1683, Loss: 3.1171\nBatch 774/1683, Loss: 3.5005\nBatch 775/1683, Loss: 3.6370\nBatch 776/1683, Loss: 3.0705\nBatch 777/1683, Loss: 2.9977\nBatch 778/1683, Loss: 3.5375\nBatch 779/1683, Loss: 3.0636\nBatch 780/1683, Loss: 2.8853\n  Média parcial até aqui: 3.2719\nBatch 781/1683, Loss: 3.0713\nBatch 782/1683, Loss: 3.3251\nBatch 783/1683, Loss: 3.0953\nBatch 784/1683, Loss: 2.9190\nBatch 785/1683, Loss: 3.1316\nBatch 786/1683, Loss: 3.6701\nBatch 787/1683, Loss: 3.7673\nBatch 788/1683, Loss: 3.4290\nBatch 789/1683, Loss: 3.3306\nBatch 790/1683, Loss: 3.4184\n  Média parcial até aqui: 3.2724\nBatch 791/1683, Loss: 3.3321\nBatch 792/1683, Loss: 3.4485\nBatch 793/1683, Loss: 2.9800\nBatch 794/1683, Loss: 3.1328\nBatch 795/1683, Loss: 2.7488\nBatch 796/1683, Loss: 3.3045\nBatch 797/1683, Loss: 3.3624\nBatch 798/1683, Loss: 3.5377\nBatch 799/1683, Loss: 3.6398\nBatch 800/1683, Loss: 3.2683\n  Média parcial até aqui: 3.2725\nBatch 801/1683, Loss: 3.1101\nBatch 802/1683, Loss: 3.0803\nBatch 803/1683, Loss: 3.1509\nBatch 804/1683, Loss: 3.2578\nBatch 805/1683, Loss: 3.4924\nBatch 806/1683, Loss: 3.1320\nBatch 807/1683, Loss: 3.6834\nBatch 808/1683, Loss: 3.3356\nBatch 809/1683, Loss: 3.1854\nBatch 810/1683, Loss: 3.1387\n  Média parcial até aqui: 3.2723\nBatch 811/1683, Loss: 2.7521\nBatch 812/1683, Loss: 3.0322\nBatch 813/1683, Loss: 3.0765\nBatch 814/1683, Loss: 3.9093\nBatch 815/1683, Loss: 3.4235\nBatch 816/1683, Loss: 3.0787\nBatch 817/1683, Loss: 3.3008\nBatch 818/1683, Loss: 3.1197\nBatch 819/1683, Loss: 3.1553\nBatch 820/1683, Loss: 3.4088\n  Média parcial até aqui: 3.2717\nBatch 821/1683, Loss: 3.5241\nBatch 822/1683, Loss: 3.4474\nBatch 823/1683, Loss: 3.7900\nBatch 824/1683, Loss: 3.1479\nBatch 825/1683, Loss: 3.5041\nBatch 826/1683, Loss: 3.2026\nBatch 827/1683, Loss: 3.1973\nBatch 828/1683, Loss: 3.3774\nBatch 829/1683, Loss: 3.1047\nBatch 830/1683, Loss: 3.3606\n  Média parcial até aqui: 3.2729\nBatch 831/1683, Loss: 3.8401\nBatch 832/1683, Loss: 3.4841\nBatch 833/1683, Loss: 3.2773\nBatch 834/1683, Loss: 3.3648\nBatch 835/1683, Loss: 3.5156\nBatch 836/1683, Loss: 3.0389\nBatch 837/1683, Loss: 3.1026\nBatch 838/1683, Loss: 3.0675\nBatch 839/1683, Loss: 3.7654\nBatch 840/1683, Loss: 3.7359\n  Média parcial até aqui: 3.2746\nBatch 841/1683, Loss: 3.3656\nBatch 842/1683, Loss: 3.5628\nBatch 843/1683, Loss: 2.9276\nBatch 844/1683, Loss: 3.2495\nBatch 845/1683, Loss: 2.9872\nBatch 846/1683, Loss: 3.2585\nBatch 847/1683, Loss: 3.2694\nBatch 848/1683, Loss: 2.7909\nBatch 849/1683, Loss: 3.5746\nBatch 850/1683, Loss: 3.5103\n  Média parcial até aqui: 3.2743\nBatch 851/1683, Loss: 3.0362\nBatch 852/1683, Loss: 3.1631\nBatch 853/1683, Loss: 3.1994\nBatch 854/1683, Loss: 3.5711\nBatch 855/1683, Loss: 3.1862\nBatch 856/1683, Loss: 3.3822\nBatch 857/1683, Loss: 3.6005\nBatch 858/1683, Loss: 3.3685\nBatch 859/1683, Loss: 2.8206\nBatch 860/1683, Loss: 3.4409\n  Média parcial até aqui: 3.2743\nBatch 861/1683, Loss: 2.9213\nBatch 862/1683, Loss: 3.3487\nBatch 863/1683, Loss: 3.8837\nBatch 864/1683, Loss: 3.1408\nBatch 865/1683, Loss: 3.5266\nBatch 866/1683, Loss: 3.4012\nBatch 867/1683, Loss: 3.8361\nBatch 868/1683, Loss: 3.1754\nBatch 869/1683, Loss: 3.1992\nBatch 870/1683, Loss: 3.5970\n  Média parcial até aqui: 3.2758\nBatch 871/1683, Loss: 3.6691\nBatch 872/1683, Loss: 3.3093\nBatch 873/1683, Loss: 3.0379\nBatch 874/1683, Loss: 3.3799\nBatch 875/1683, Loss: 2.9168\nBatch 876/1683, Loss: 3.0330\nBatch 877/1683, Loss: 3.2280\nBatch 878/1683, Loss: 3.4402\nBatch 879/1683, Loss: 3.5631\nBatch 880/1683, Loss: 3.2733\n  Média parcial até aqui: 3.2759\nBatch 881/1683, Loss: 3.4103\nBatch 882/1683, Loss: 3.3208\nBatch 883/1683, Loss: 3.4525\nBatch 884/1683, Loss: 3.2313\nBatch 885/1683, Loss: 3.4505\nBatch 886/1683, Loss: 3.2019\nBatch 887/1683, Loss: 3.3102\nBatch 888/1683, Loss: 3.1061\nBatch 889/1683, Loss: 3.9102\nBatch 890/1683, Loss: 3.0747\n  Média parcial até aqui: 3.2767\nBatch 891/1683, Loss: 3.7515\nBatch 892/1683, Loss: 3.9389\nBatch 893/1683, Loss: 2.9901\nBatch 894/1683, Loss: 3.0907\nBatch 895/1683, Loss: 3.3042\nBatch 896/1683, Loss: 3.3364\nBatch 897/1683, Loss: 3.1708\nBatch 898/1683, Loss: 3.6771\nBatch 899/1683, Loss: 3.1601\nBatch 900/1683, Loss: 3.0098\n  Média parcial até aqui: 3.2774\nBatch 901/1683, Loss: 3.6258\nBatch 902/1683, Loss: 3.2996\nBatch 903/1683, Loss: 3.5677\nBatch 904/1683, Loss: 3.3541\nBatch 905/1683, Loss: 3.3795\nBatch 906/1683, Loss: 3.0721\nBatch 907/1683, Loss: 2.8056\nBatch 908/1683, Loss: 3.0592\nBatch 909/1683, Loss: 3.3422\nBatch 910/1683, Loss: 3.1131\n  Média parcial até aqui: 3.2773\nBatch 911/1683, Loss: 3.3310\nBatch 912/1683, Loss: 3.2159\nBatch 913/1683, Loss: 2.9977\nBatch 914/1683, Loss: 3.2007\nBatch 915/1683, Loss: 3.1738\nBatch 916/1683, Loss: 3.3812\nBatch 917/1683, Loss: 3.1229\nBatch 918/1683, Loss: 3.4395\nBatch 919/1683, Loss: 3.4773\nBatch 920/1683, Loss: 3.1913\n  Média parcial até aqui: 3.2770\nBatch 921/1683, Loss: 3.2934\nBatch 922/1683, Loss: 3.1616\nBatch 923/1683, Loss: 3.7002\nBatch 924/1683, Loss: 3.3710\nBatch 925/1683, Loss: 3.5548\nBatch 926/1683, Loss: 3.2225\nBatch 927/1683, Loss: 3.0300\nBatch 928/1683, Loss: 2.6506\nBatch 929/1683, Loss: 3.0249\nBatch 930/1683, Loss: 3.2841\n  Média parcial até aqui: 3.2765\nBatch 931/1683, Loss: 3.5116\nBatch 932/1683, Loss: 3.2735\nBatch 933/1683, Loss: 3.4936\nBatch 934/1683, Loss: 3.1907\nBatch 935/1683, Loss: 3.3163\nBatch 936/1683, Loss: 3.2221\nBatch 937/1683, Loss: 3.1362\nBatch 938/1683, Loss: 3.4639\nBatch 939/1683, Loss: 3.1980\nBatch 940/1683, Loss: 3.5225\n  Média parcial até aqui: 3.2771\nBatch 941/1683, Loss: 3.3945\nBatch 942/1683, Loss: 3.4749\nBatch 943/1683, Loss: 3.6864\nBatch 944/1683, Loss: 2.9772\nBatch 945/1683, Loss: 3.3101\nBatch 946/1683, Loss: 3.2134\nBatch 947/1683, Loss: 3.1496\nBatch 948/1683, Loss: 3.4342\nBatch 949/1683, Loss: 3.1485\nBatch 950/1683, Loss: 3.5816\n  Média parcial até aqui: 3.2777\nBatch 951/1683, Loss: 2.8128\nBatch 952/1683, Loss: 3.3877\nBatch 953/1683, Loss: 2.9737\nBatch 954/1683, Loss: 3.2487\nBatch 955/1683, Loss: 3.1011\nBatch 956/1683, Loss: 3.6772\nBatch 957/1683, Loss: 3.4762\nBatch 958/1683, Loss: 3.4176\nBatch 959/1683, Loss: 3.2091\nBatch 960/1683, Loss: 3.1920\n  Média parcial até aqui: 3.2774\nBatch 961/1683, Loss: 3.3500\nBatch 962/1683, Loss: 3.2865\nBatch 963/1683, Loss: 3.3838\nBatch 964/1683, Loss: 3.5178\nBatch 965/1683, Loss: 3.4271\nBatch 966/1683, Loss: 3.4447\nBatch 967/1683, Loss: 3.1624\nBatch 968/1683, Loss: 3.2523\nBatch 969/1683, Loss: 2.9229\nBatch 970/1683, Loss: 2.8765\n  Média parcial até aqui: 3.2773\nBatch 971/1683, Loss: 2.7924\nBatch 972/1683, Loss: 3.2460\nBatch 973/1683, Loss: 3.3450\nBatch 974/1683, Loss: 3.1270\nBatch 975/1683, Loss: 3.2345\nBatch 976/1683, Loss: 3.0822\nBatch 977/1683, Loss: 3.4507\nBatch 978/1683, Loss: 2.9348\nBatch 979/1683, Loss: 3.2684\nBatch 980/1683, Loss: 3.0839\n  Média parcial até aqui: 3.2761\nBatch 981/1683, Loss: 3.6876\nBatch 982/1683, Loss: 3.6366\nBatch 983/1683, Loss: 3.8115\nBatch 984/1683, Loss: 3.2883\nBatch 985/1683, Loss: 3.1662\nBatch 986/1683, Loss: 3.6931\nBatch 987/1683, Loss: 3.0536\nBatch 988/1683, Loss: 3.7911\nBatch 989/1683, Loss: 3.1309\nBatch 990/1683, Loss: 2.8152\n  Média parcial até aqui: 3.2774\nBatch 991/1683, Loss: 2.9487\nBatch 992/1683, Loss: 4.0392\nBatch 993/1683, Loss: 3.3475\nBatch 994/1683, Loss: 3.2482\nBatch 995/1683, Loss: 3.6578\nBatch 996/1683, Loss: 3.2046\nBatch 997/1683, Loss: 2.8594\nBatch 998/1683, Loss: 3.6130\nBatch 999/1683, Loss: 2.8908\nBatch 1000/1683, Loss: 2.8157\n  Média parcial até aqui: 3.2772\nBatch 1001/1683, Loss: 3.2979\nBatch 1002/1683, Loss: 3.6874\nBatch 1003/1683, Loss: 2.6459\nBatch 1004/1683, Loss: 2.8088\nBatch 1005/1683, Loss: 2.8985\nBatch 1006/1683, Loss: 3.0069\nBatch 1007/1683, Loss: 3.2208\nBatch 1008/1683, Loss: 3.0574\nBatch 1009/1683, Loss: 2.8822\nBatch 1010/1683, Loss: 3.0215\n  Média parcial até aqui: 3.2750\nBatch 1011/1683, Loss: 3.1879\nBatch 1012/1683, Loss: 3.2378\nBatch 1013/1683, Loss: 3.2290\nBatch 1014/1683, Loss: 3.2276\nBatch 1015/1683, Loss: 3.0853\nBatch 1016/1683, Loss: 3.1085\nBatch 1017/1683, Loss: 3.0863\nBatch 1018/1683, Loss: 3.3198\nBatch 1019/1683, Loss: 3.1515\nBatch 1020/1683, Loss: 3.5256\n  Média parcial até aqui: 3.2744\nBatch 1021/1683, Loss: 3.1437\nBatch 1022/1683, Loss: 3.1292\nBatch 1023/1683, Loss: 3.0924\nBatch 1024/1683, Loss: 3.4649\nBatch 1025/1683, Loss: 2.9368\nBatch 1026/1683, Loss: 2.9423\nBatch 1027/1683, Loss: 3.4162\nBatch 1028/1683, Loss: 3.1289\nBatch 1029/1683, Loss: 3.0542\nBatch 1030/1683, Loss: 3.3138\n  Média parcial até aqui: 3.2733\nBatch 1031/1683, Loss: 3.0391\nBatch 1032/1683, Loss: 3.3879\nBatch 1033/1683, Loss: 3.2256\nBatch 1034/1683, Loss: 3.3432\nBatch 1035/1683, Loss: 3.1919\nBatch 1036/1683, Loss: 3.2931\nBatch 1037/1683, Loss: 3.4493\nBatch 1038/1683, Loss: 3.0970\nBatch 1039/1683, Loss: 3.3330\nBatch 1040/1683, Loss: 3.0651\n  Média parcial até aqui: 3.2730\nBatch 1041/1683, Loss: 3.3306\nBatch 1042/1683, Loss: 3.0787\nBatch 1043/1683, Loss: 3.4115\nBatch 1044/1683, Loss: 3.1563\nBatch 1045/1683, Loss: 2.9384\nBatch 1046/1683, Loss: 3.1147\nBatch 1047/1683, Loss: 3.0976\nBatch 1048/1683, Loss: 3.8221\nBatch 1049/1683, Loss: 3.0589\nBatch 1050/1683, Loss: 3.1711\n  Média parcial até aqui: 3.2725\nBatch 1051/1683, Loss: 3.2358\nBatch 1052/1683, Loss: 3.5751\nBatch 1053/1683, Loss: 3.0996\nBatch 1054/1683, Loss: 3.3499\nBatch 1055/1683, Loss: 3.0105\nBatch 1056/1683, Loss: 3.3151\nBatch 1057/1683, Loss: 2.9654\nBatch 1058/1683, Loss: 3.2701\nBatch 1059/1683, Loss: 3.3053\nBatch 1060/1683, Loss: 3.1681\n  Média parcial até aqui: 3.2721\nBatch 1061/1683, Loss: 3.2534\nBatch 1062/1683, Loss: 3.0775\nBatch 1063/1683, Loss: 3.5795\nBatch 1064/1683, Loss: 3.3665\nBatch 1065/1683, Loss: 3.3626\nBatch 1066/1683, Loss: 3.3493\nBatch 1067/1683, Loss: 3.1437\nBatch 1068/1683, Loss: 2.9162\nBatch 1069/1683, Loss: 3.1059\nBatch 1070/1683, Loss: 3.5371\n  Média parcial até aqui: 3.2721\nBatch 1071/1683, Loss: 3.1914\nBatch 1072/1683, Loss: 2.9998\nBatch 1073/1683, Loss: 3.1074\nBatch 1074/1683, Loss: 3.3947\nBatch 1075/1683, Loss: 3.1547\nBatch 1076/1683, Loss: 3.2796\nBatch 1077/1683, Loss: 3.1637\nBatch 1078/1683, Loss: 3.1500\nBatch 1079/1683, Loss: 3.2545\nBatch 1080/1683, Loss: 2.8527\n  Média parcial até aqui: 3.2710\nBatch 1081/1683, Loss: 3.6122\nBatch 1082/1683, Loss: 3.8390\nBatch 1083/1683, Loss: 3.1646\nBatch 1084/1683, Loss: 3.3403\nBatch 1085/1683, Loss: 3.3518\nBatch 1086/1683, Loss: 3.1934\nBatch 1087/1683, Loss: 3.0804\nBatch 1088/1683, Loss: 3.0447\nBatch 1089/1683, Loss: 3.0081\nBatch 1090/1683, Loss: 3.1905\n  Média parcial até aqui: 3.2711\nBatch 1091/1683, Loss: 3.5201\nBatch 1092/1683, Loss: 3.1603\nBatch 1093/1683, Loss: 3.5502\nBatch 1094/1683, Loss: 3.4663\nBatch 1095/1683, Loss: 3.0846\nBatch 1096/1683, Loss: 3.0562\nBatch 1097/1683, Loss: 3.0841\nBatch 1098/1683, Loss: 3.2995\nBatch 1099/1683, Loss: 3.0191\nBatch 1100/1683, Loss: 3.2478\n  Média parcial até aqui: 3.2709\nBatch 1101/1683, Loss: 3.2546\nBatch 1102/1683, Loss: 3.5236\nBatch 1103/1683, Loss: 3.6244\nBatch 1104/1683, Loss: 3.6485\nBatch 1105/1683, Loss: 3.3811\nBatch 1106/1683, Loss: 3.3901\nBatch 1107/1683, Loss: 3.1114\nBatch 1108/1683, Loss: 3.3654\nBatch 1109/1683, Loss: 3.3942\nBatch 1110/1683, Loss: 3.0558\n  Média parcial até aqui: 3.2718\nBatch 1111/1683, Loss: 3.3576\nBatch 1112/1683, Loss: 3.2166\nBatch 1113/1683, Loss: 3.4034\nBatch 1114/1683, Loss: 3.3544\nBatch 1115/1683, Loss: 3.5940\nBatch 1116/1683, Loss: 3.2442\nBatch 1117/1683, Loss: 3.3940\nBatch 1118/1683, Loss: 3.4661\nBatch 1119/1683, Loss: 3.3027\nBatch 1120/1683, Loss: 3.2313\n  Média parcial até aqui: 3.2726\nBatch 1121/1683, Loss: 3.1702\nBatch 1122/1683, Loss: 3.1022\nBatch 1123/1683, Loss: 3.3135\nBatch 1124/1683, Loss: 3.5002\nBatch 1125/1683, Loss: 3.3329\nBatch 1126/1683, Loss: 3.1979\nBatch 1127/1683, Loss: 4.0177\nBatch 1128/1683, Loss: 3.3884\nBatch 1129/1683, Loss: 3.1399\nBatch 1130/1683, Loss: 3.0478\n  Média parcial até aqui: 3.2730\nBatch 1131/1683, Loss: 3.5223\nBatch 1132/1683, Loss: 3.1476\nBatch 1133/1683, Loss: 3.6408\nBatch 1134/1683, Loss: 3.4977\nBatch 1135/1683, Loss: 3.1701\nBatch 1136/1683, Loss: 3.0177\nBatch 1137/1683, Loss: 3.4106\nBatch 1138/1683, Loss: 3.1849\nBatch 1139/1683, Loss: 3.3447\nBatch 1140/1683, Loss: 3.0310\n  Média parcial até aqui: 3.2732\nBatch 1141/1683, Loss: 3.1377\nBatch 1142/1683, Loss: 3.1734\nBatch 1143/1683, Loss: 2.8669\nBatch 1144/1683, Loss: 3.1733\nBatch 1145/1683, Loss: 2.9694\nBatch 1146/1683, Loss: 3.4858\nBatch 1147/1683, Loss: 2.8484\nBatch 1148/1683, Loss: 3.2568\nBatch 1149/1683, Loss: 3.0364\nBatch 1150/1683, Loss: 3.2696\n  Média parcial até aqui: 3.2719\nBatch 1151/1683, Loss: 3.4552\nBatch 1152/1683, Loss: 3.3691\nBatch 1153/1683, Loss: 3.0621\nBatch 1154/1683, Loss: 3.1215\nBatch 1155/1683, Loss: 3.0052\nBatch 1156/1683, Loss: 3.0921\nBatch 1157/1683, Loss: 3.1804\nBatch 1158/1683, Loss: 3.4214\nBatch 1159/1683, Loss: 3.2434\nBatch 1160/1683, Loss: 3.5070\n  Média parcial até aqui: 3.2717\nBatch 1161/1683, Loss: 3.1637\nBatch 1162/1683, Loss: 3.5604\nBatch 1163/1683, Loss: 3.4875\nBatch 1164/1683, Loss: 3.2323\nBatch 1165/1683, Loss: 2.9315\nBatch 1166/1683, Loss: 3.1309\nBatch 1167/1683, Loss: 3.2590\nBatch 1168/1683, Loss: 3.2464\nBatch 1169/1683, Loss: 3.3533\nBatch 1170/1683, Loss: 3.2583\n  Média parcial até aqui: 3.2716\nBatch 1171/1683, Loss: 3.1970\nBatch 1172/1683, Loss: 2.9967\nBatch 1173/1683, Loss: 3.0563\nBatch 1174/1683, Loss: 3.0533\nBatch 1175/1683, Loss: 3.1676\nBatch 1176/1683, Loss: 3.2505\nBatch 1177/1683, Loss: 3.1209\nBatch 1178/1683, Loss: 3.1445\nBatch 1179/1683, Loss: 3.3163\nBatch 1180/1683, Loss: 3.1219\n  Média parcial até aqui: 3.2705\nBatch 1181/1683, Loss: 3.1648\nBatch 1182/1683, Loss: 3.3171\nBatch 1183/1683, Loss: 3.0127\nBatch 1184/1683, Loss: 3.4860\nBatch 1185/1683, Loss: 3.1572\nBatch 1186/1683, Loss: 3.1490\nBatch 1187/1683, Loss: 3.0911\nBatch 1188/1683, Loss: 3.0654\nBatch 1189/1683, Loss: 3.0856\nBatch 1190/1683, Loss: 3.5110\n  Média parcial até aqui: 3.2700\nBatch 1191/1683, Loss: 3.1768\nBatch 1192/1683, Loss: 3.4977\nBatch 1193/1683, Loss: 3.0908\nBatch 1194/1683, Loss: 3.2630\nBatch 1195/1683, Loss: 3.2281\nBatch 1196/1683, Loss: 3.7242\nBatch 1197/1683, Loss: 3.5919\nBatch 1198/1683, Loss: 2.8842\nBatch 1199/1683, Loss: 3.3934\nBatch 1200/1683, Loss: 3.7900\n  Média parcial até aqui: 3.2707\nBatch 1201/1683, Loss: 3.0167\nBatch 1202/1683, Loss: 3.3386\nBatch 1203/1683, Loss: 3.1855\nBatch 1204/1683, Loss: 3.0984\nBatch 1205/1683, Loss: 3.4924\nBatch 1206/1683, Loss: 3.1051\nBatch 1207/1683, Loss: 3.3980\nBatch 1208/1683, Loss: 3.0710\nBatch 1209/1683, Loss: 2.8671\nBatch 1210/1683, Loss: 3.3532\n  Média parcial até aqui: 3.2701\nBatch 1211/1683, Loss: 3.2792\nBatch 1212/1683, Loss: 3.0100\nBatch 1213/1683, Loss: 3.7818\nBatch 1214/1683, Loss: 3.4675\nBatch 1215/1683, Loss: 3.3749\nBatch 1216/1683, Loss: 3.0993\nBatch 1217/1683, Loss: 2.9180\nBatch 1218/1683, Loss: 3.3902\nBatch 1219/1683, Loss: 3.1850\nBatch 1220/1683, Loss: 3.1603\n  Média parcial até aqui: 3.2701\nBatch 1221/1683, Loss: 3.2910\nBatch 1222/1683, Loss: 3.3193\nBatch 1223/1683, Loss: 3.3957\nBatch 1224/1683, Loss: 3.5513\nBatch 1225/1683, Loss: 2.8204\nBatch 1226/1683, Loss: 3.1238\nBatch 1227/1683, Loss: 3.5514\nBatch 1228/1683, Loss: 3.1012\nBatch 1229/1683, Loss: 3.9668\nBatch 1230/1683, Loss: 2.9972\n  Média parcial até aqui: 3.2704\nBatch 1231/1683, Loss: 3.2302\nBatch 1232/1683, Loss: 3.9189\nBatch 1233/1683, Loss: 2.8805\nBatch 1234/1683, Loss: 3.2472\nBatch 1235/1683, Loss: 3.2990\nBatch 1236/1683, Loss: 3.9157\nBatch 1237/1683, Loss: 3.1626\nBatch 1238/1683, Loss: 3.2344\nBatch 1239/1683, Loss: 3.1569\nBatch 1240/1683, Loss: 3.1437\n  Média parcial até aqui: 3.2708\nBatch 1241/1683, Loss: 3.1602\nBatch 1242/1683, Loss: 3.3811\nBatch 1243/1683, Loss: 3.3895\nBatch 1244/1683, Loss: 3.3025\nBatch 1245/1683, Loss: 2.9701\nBatch 1246/1683, Loss: 3.2821\nBatch 1247/1683, Loss: 3.0008\nBatch 1248/1683, Loss: 3.4495\nBatch 1249/1683, Loss: 3.4228\nBatch 1250/1683, Loss: 3.9219\n  Média parcial até aqui: 3.2713\nBatch 1251/1683, Loss: 3.5458\nBatch 1252/1683, Loss: 3.4410\nBatch 1253/1683, Loss: 2.9758\nBatch 1254/1683, Loss: 3.1964\nBatch 1255/1683, Loss: 4.1228\nBatch 1256/1683, Loss: 2.9935\nBatch 1257/1683, Loss: 3.4980\nBatch 1258/1683, Loss: 3.4659\nBatch 1259/1683, Loss: 3.2007\nBatch 1260/1683, Loss: 3.0596\n  Média parcial até aqui: 3.2719\nBatch 1261/1683, Loss: 2.9266\nBatch 1262/1683, Loss: 2.8809\nBatch 1263/1683, Loss: 3.7066\nBatch 1264/1683, Loss: 3.2750\nBatch 1265/1683, Loss: 3.2575\nBatch 1266/1683, Loss: 3.0927\nBatch 1267/1683, Loss: 3.3400\nBatch 1268/1683, Loss: 3.0544\nBatch 1269/1683, Loss: 3.1022\nBatch 1270/1683, Loss: 3.1549\n  Média parcial até aqui: 3.2711\nBatch 1271/1683, Loss: 3.0872\nBatch 1272/1683, Loss: 3.2945\nBatch 1273/1683, Loss: 3.2342\nBatch 1274/1683, Loss: 2.9934\nBatch 1275/1683, Loss: 3.3661\nBatch 1276/1683, Loss: 3.1686\nBatch 1277/1683, Loss: 2.9797\nBatch 1278/1683, Loss: 3.0573\nBatch 1279/1683, Loss: 2.9823\nBatch 1280/1683, Loss: 3.4592\n  Média parcial até aqui: 3.2703\nBatch 1281/1683, Loss: 3.0008\nBatch 1282/1683, Loss: 3.1614\nBatch 1283/1683, Loss: 2.8436\nBatch 1284/1683, Loss: 3.5916\nBatch 1285/1683, Loss: 3.3891\nBatch 1286/1683, Loss: 3.7064\nBatch 1287/1683, Loss: 3.0248\nBatch 1288/1683, Loss: 3.1655\nBatch 1289/1683, Loss: 3.5551\nBatch 1290/1683, Loss: 3.1123\n  Média parcial até aqui: 3.2702\nBatch 1291/1683, Loss: 3.4449\nBatch 1292/1683, Loss: 3.1655\nBatch 1293/1683, Loss: 2.9354\nBatch 1294/1683, Loss: 3.0897\nBatch 1295/1683, Loss: 3.4777\nBatch 1296/1683, Loss: 3.1188\nBatch 1297/1683, Loss: 3.0153\nBatch 1298/1683, Loss: 3.4903\nBatch 1299/1683, Loss: 3.2300\nBatch 1300/1683, Loss: 3.1691\n  Média parcial até aqui: 3.2697\nBatch 1301/1683, Loss: 3.5364\nBatch 1302/1683, Loss: 3.1349\nBatch 1303/1683, Loss: 3.8448\nBatch 1304/1683, Loss: 3.2433\nBatch 1305/1683, Loss: 3.4595\nBatch 1306/1683, Loss: 2.9420\nBatch 1307/1683, Loss: 3.1815\nBatch 1308/1683, Loss: 3.1247\nBatch 1309/1683, Loss: 3.1056\nBatch 1310/1683, Loss: 3.5167\n  Média parcial até aqui: 3.2700\nBatch 1311/1683, Loss: 3.3321\nBatch 1312/1683, Loss: 3.3082\nBatch 1313/1683, Loss: 3.0333\nBatch 1314/1683, Loss: 3.2762\nBatch 1315/1683, Loss: 3.6651\nBatch 1316/1683, Loss: 3.2893\nBatch 1317/1683, Loss: 3.3990\nBatch 1318/1683, Loss: 3.4584\nBatch 1319/1683, Loss: 3.0260\nBatch 1320/1683, Loss: 3.5309\n  Média parcial até aqui: 3.2705\nBatch 1321/1683, Loss: 3.5403\nBatch 1322/1683, Loss: 3.1351\nBatch 1323/1683, Loss: 2.7571\nBatch 1324/1683, Loss: 3.4034\nBatch 1325/1683, Loss: 3.1559\nBatch 1326/1683, Loss: 3.4364\nBatch 1327/1683, Loss: 3.1013\nBatch 1328/1683, Loss: 3.1959\nBatch 1329/1683, Loss: 3.5226\nBatch 1330/1683, Loss: 3.6771\n  Média parcial até aqui: 3.2707\nBatch 1331/1683, Loss: 3.0263\nBatch 1332/1683, Loss: 3.4336\nBatch 1333/1683, Loss: 3.2925\nBatch 1334/1683, Loss: 3.0030\nBatch 1335/1683, Loss: 3.3989\nBatch 1336/1683, Loss: 3.1426\nBatch 1337/1683, Loss: 3.2915\nBatch 1338/1683, Loss: 3.4092\nBatch 1339/1683, Loss: 3.0134\nBatch 1340/1683, Loss: 3.2336\n  Média parcial até aqui: 3.2703\nBatch 1341/1683, Loss: 3.0403\nBatch 1342/1683, Loss: 3.4225\nBatch 1343/1683, Loss: 3.0202\nBatch 1344/1683, Loss: 3.7432\nBatch 1345/1683, Loss: 3.7350\nBatch 1346/1683, Loss: 3.2129\nBatch 1347/1683, Loss: 3.6541\nBatch 1348/1683, Loss: 2.9541\nBatch 1349/1683, Loss: 3.2834\nBatch 1350/1683, Loss: 3.1133\n  Média parcial até aqui: 3.2707\nBatch 1351/1683, Loss: 3.5941\nBatch 1352/1683, Loss: 3.0870\nBatch 1353/1683, Loss: 3.2305\nBatch 1354/1683, Loss: 3.0801\nBatch 1355/1683, Loss: 3.2481\nBatch 1356/1683, Loss: 2.9943\nBatch 1357/1683, Loss: 3.2571\nBatch 1358/1683, Loss: 3.0383\nBatch 1359/1683, Loss: 3.1602\nBatch 1360/1683, Loss: 3.0373\n  Média parcial até aqui: 3.2700\nBatch 1361/1683, Loss: 2.5402\nBatch 1362/1683, Loss: 2.9816\nBatch 1363/1683, Loss: 3.5759\nBatch 1364/1683, Loss: 3.2320\nBatch 1365/1683, Loss: 3.0307\nBatch 1366/1683, Loss: 3.3416\nBatch 1367/1683, Loss: 3.0527\nBatch 1368/1683, Loss: 3.3346\nBatch 1369/1683, Loss: 3.2750\nBatch 1370/1683, Loss: 3.3882\n  Média parcial até aqui: 3.2693\nBatch 1371/1683, Loss: 3.4235\nBatch 1372/1683, Loss: 3.0246\nBatch 1373/1683, Loss: 3.3423\nBatch 1374/1683, Loss: 3.5909\nBatch 1375/1683, Loss: 3.0980\nBatch 1376/1683, Loss: 3.0232\nBatch 1377/1683, Loss: 2.9404\nBatch 1378/1683, Loss: 3.6413\nBatch 1379/1683, Loss: 3.3248\nBatch 1380/1683, Loss: 3.6163\n  Média parcial até aqui: 3.2695\nBatch 1381/1683, Loss: 3.1095\nBatch 1382/1683, Loss: 3.4066\nBatch 1383/1683, Loss: 3.2969\nBatch 1384/1683, Loss: 3.0426\nBatch 1385/1683, Loss: 3.1734\nBatch 1386/1683, Loss: 3.5392\nBatch 1387/1683, Loss: 3.2252\nBatch 1388/1683, Loss: 3.1058\nBatch 1389/1683, Loss: 2.9061\nBatch 1390/1683, Loss: 3.2510\n  Média parcial até aqui: 3.2691\nBatch 1391/1683, Loss: 2.8523\nBatch 1392/1683, Loss: 3.1436\nBatch 1393/1683, Loss: 3.0765\nBatch 1394/1683, Loss: 2.9854\nBatch 1395/1683, Loss: 3.5858\nBatch 1396/1683, Loss: 3.1193\nBatch 1397/1683, Loss: 3.5296\nBatch 1398/1683, Loss: 2.9064\nBatch 1399/1683, Loss: 3.3203\nBatch 1400/1683, Loss: 3.1819\n  Média parcial até aqui: 3.2683\nBatch 1401/1683, Loss: 3.2219\nBatch 1402/1683, Loss: 3.5179\nBatch 1403/1683, Loss: 2.8907\nBatch 1404/1683, Loss: 3.2383\nBatch 1405/1683, Loss: 3.2486\nBatch 1406/1683, Loss: 3.2797\nBatch 1407/1683, Loss: 3.4983\nBatch 1408/1683, Loss: 3.4263\nBatch 1409/1683, Loss: 3.5132\nBatch 1410/1683, Loss: 3.2749\n  Média parcial até aqui: 3.2686\nBatch 1411/1683, Loss: 3.1158\nBatch 1412/1683, Loss: 3.1646\nBatch 1413/1683, Loss: 3.2881\nBatch 1414/1683, Loss: 3.2151\nBatch 1415/1683, Loss: 2.9995\nBatch 1416/1683, Loss: 3.0960\nBatch 1417/1683, Loss: 3.4955\nBatch 1418/1683, Loss: 3.0227\nBatch 1419/1683, Loss: 3.1391\nBatch 1420/1683, Loss: 3.6551\n  Média parcial até aqui: 3.2683\nBatch 1421/1683, Loss: 3.2122\nBatch 1422/1683, Loss: 3.6850\nBatch 1423/1683, Loss: 3.3619\nBatch 1424/1683, Loss: 3.1009\nBatch 1425/1683, Loss: 3.2695\nBatch 1426/1683, Loss: 3.1436\nBatch 1427/1683, Loss: 3.1439\nBatch 1428/1683, Loss: 3.2357\nBatch 1429/1683, Loss: 3.2241\nBatch 1430/1683, Loss: 3.0542\n  Média parcial até aqui: 3.2681\nBatch 1431/1683, Loss: 3.5385\nBatch 1432/1683, Loss: 3.6769\nBatch 1433/1683, Loss: 3.2008\nBatch 1434/1683, Loss: 3.4092\nBatch 1435/1683, Loss: 3.1752\nBatch 1436/1683, Loss: 4.0261\nBatch 1437/1683, Loss: 3.1484\nBatch 1438/1683, Loss: 3.1520\nBatch 1439/1683, Loss: 3.0387\nBatch 1440/1683, Loss: 3.4773\n  Média parcial até aqui: 3.2689\nBatch 1441/1683, Loss: 3.5520\nBatch 1442/1683, Loss: 3.3511\nBatch 1443/1683, Loss: 3.3393\nBatch 1444/1683, Loss: 3.6454\nBatch 1445/1683, Loss: 2.8775\nBatch 1446/1683, Loss: 3.5825\nBatch 1447/1683, Loss: 3.3583\nBatch 1448/1683, Loss: 3.5196\nBatch 1449/1683, Loss: 3.6922\nBatch 1450/1683, Loss: 3.3298\n  Média parcial até aqui: 3.2700\nBatch 1451/1683, Loss: 3.1013\nBatch 1452/1683, Loss: 3.3986\nBatch 1453/1683, Loss: 3.1099\nBatch 1454/1683, Loss: 3.2853\nBatch 1455/1683, Loss: 3.5384\nBatch 1456/1683, Loss: 3.3221\nBatch 1457/1683, Loss: 3.0932\nBatch 1458/1683, Loss: 3.2688\nBatch 1459/1683, Loss: 3.0241\nBatch 1460/1683, Loss: 3.4642\n  Média parcial até aqui: 3.2699\nBatch 1461/1683, Loss: 3.2072\nBatch 1462/1683, Loss: 3.2625\nBatch 1463/1683, Loss: 3.1956\nBatch 1464/1683, Loss: 2.9979\nBatch 1465/1683, Loss: 2.9714\nBatch 1466/1683, Loss: 3.4483\nBatch 1467/1683, Loss: 3.2620\nBatch 1468/1683, Loss: 3.1527\nBatch 1469/1683, Loss: 3.0829\nBatch 1470/1683, Loss: 3.2028\n  Média parcial até aqui: 3.2693\nBatch 1471/1683, Loss: 3.2766\nBatch 1472/1683, Loss: 3.1920\nBatch 1473/1683, Loss: 3.5054\nBatch 1474/1683, Loss: 3.5466\nBatch 1475/1683, Loss: 3.0538\nBatch 1476/1683, Loss: 3.4035\nBatch 1477/1683, Loss: 3.3264\nBatch 1478/1683, Loss: 3.0118\nBatch 1479/1683, Loss: 4.0471\nBatch 1480/1683, Loss: 3.1933\n  Média parcial até aqui: 3.2699\nBatch 1481/1683, Loss: 3.2468\nBatch 1482/1683, Loss: 2.8269\nBatch 1483/1683, Loss: 3.1048\nBatch 1484/1683, Loss: 2.8259\nBatch 1485/1683, Loss: 2.9089\nBatch 1486/1683, Loss: 3.1407\nBatch 1487/1683, Loss: 3.3085\nBatch 1488/1683, Loss: 3.2404\nBatch 1489/1683, Loss: 3.1787\nBatch 1490/1683, Loss: 3.1475\n  Média parcial até aqui: 3.2687\nBatch 1491/1683, Loss: 3.1857\nBatch 1492/1683, Loss: 2.9266\nBatch 1493/1683, Loss: 2.8270\nBatch 1494/1683, Loss: 3.4514\nBatch 1495/1683, Loss: 3.4258\nBatch 1496/1683, Loss: 3.3020\nBatch 1497/1683, Loss: 3.1896\nBatch 1498/1683, Loss: 3.4901\nBatch 1499/1683, Loss: 3.3145\nBatch 1500/1683, Loss: 2.6555\n  Média parcial até aqui: 3.2681\nBatch 1501/1683, Loss: 2.7146\nBatch 1502/1683, Loss: 3.1587\nBatch 1503/1683, Loss: 3.2396\nBatch 1504/1683, Loss: 3.7567\nBatch 1505/1683, Loss: 3.6121\nBatch 1506/1683, Loss: 3.4340\nBatch 1507/1683, Loss: 3.6200\nBatch 1508/1683, Loss: 3.4858\nBatch 1509/1683, Loss: 3.4336\nBatch 1510/1683, Loss: 3.4191\n  Média parcial até aqui: 3.2689\nBatch 1511/1683, Loss: 3.4339\nBatch 1512/1683, Loss: 3.2991\nBatch 1513/1683, Loss: 3.8605\nBatch 1514/1683, Loss: 3.3435\nBatch 1515/1683, Loss: 3.0737\nBatch 1516/1683, Loss: 3.2444\nBatch 1517/1683, Loss: 3.4087\nBatch 1518/1683, Loss: 3.2699\nBatch 1519/1683, Loss: 3.2454\nBatch 1520/1683, Loss: 3.5465\n  Média parcial até aqui: 3.2696\nBatch 1521/1683, Loss: 3.0073\nBatch 1522/1683, Loss: 3.3071\nBatch 1523/1683, Loss: 3.3375\nBatch 1524/1683, Loss: 2.9506\nBatch 1525/1683, Loss: 3.2015\nBatch 1526/1683, Loss: 3.2844\nBatch 1527/1683, Loss: 3.4870\nBatch 1528/1683, Loss: 3.2024\nBatch 1529/1683, Loss: 3.1465\nBatch 1530/1683, Loss: 3.2532\n  Média parcial até aqui: 3.2692\nBatch 1531/1683, Loss: 3.1268\nBatch 1532/1683, Loss: 3.1452\nBatch 1533/1683, Loss: 3.0441\nBatch 1534/1683, Loss: 2.9057\nBatch 1535/1683, Loss: 3.0591\nBatch 1536/1683, Loss: 3.3613\nBatch 1537/1683, Loss: 3.5268\nBatch 1538/1683, Loss: 2.5531\nBatch 1539/1683, Loss: 3.1492\nBatch 1540/1683, Loss: 3.1000\n  Média parcial até aqui: 3.2681\nBatch 1541/1683, Loss: 2.9578\nBatch 1542/1683, Loss: 3.2611\nBatch 1543/1683, Loss: 3.4703\nBatch 1544/1683, Loss: 3.2294\nBatch 1545/1683, Loss: 3.4339\nBatch 1546/1683, Loss: 3.1706\nBatch 1547/1683, Loss: 3.8740\nBatch 1548/1683, Loss: 3.3460\nBatch 1549/1683, Loss: 3.0518\nBatch 1550/1683, Loss: 3.0451\n  Média parcial até aqui: 3.2682\nBatch 1551/1683, Loss: 3.6761\nBatch 1552/1683, Loss: 3.2658\nBatch 1553/1683, Loss: 3.1863\nBatch 1554/1683, Loss: 3.2167\nBatch 1555/1683, Loss: 3.0016\nBatch 1556/1683, Loss: 3.2389\nBatch 1557/1683, Loss: 3.3782\nBatch 1558/1683, Loss: 2.9329\nBatch 1559/1683, Loss: 3.0977\nBatch 1560/1683, Loss: 3.1728\n  Média parcial até aqui: 3.2679\nBatch 1561/1683, Loss: 3.3207\nBatch 1562/1683, Loss: 3.1508\nBatch 1563/1683, Loss: 3.3646\nBatch 1564/1683, Loss: 3.3487\nBatch 1565/1683, Loss: 3.0880\nBatch 1566/1683, Loss: 3.5105\nBatch 1567/1683, Loss: 3.0039\nBatch 1568/1683, Loss: 3.3147\nBatch 1569/1683, Loss: 3.0956\nBatch 1570/1683, Loss: 3.7879\n  Média parcial até aqui: 3.2681\nBatch 1571/1683, Loss: 2.9444\nBatch 1572/1683, Loss: 3.3656\nBatch 1573/1683, Loss: 3.8114\nBatch 1574/1683, Loss: 3.2284\nBatch 1575/1683, Loss: 3.6165\nBatch 1576/1683, Loss: 3.4703\nBatch 1577/1683, Loss: 3.1139\nBatch 1578/1683, Loss: 3.1161\nBatch 1579/1683, Loss: 3.1815\nBatch 1580/1683, Loss: 3.1039\n  Média parcial até aqui: 3.2683\nBatch 1581/1683, Loss: 3.5589\nBatch 1582/1683, Loss: 3.4859\nBatch 1583/1683, Loss: 3.3782\nBatch 1584/1683, Loss: 2.8587\nBatch 1585/1683, Loss: 3.3189\nBatch 1586/1683, Loss: 3.6217\nBatch 1587/1683, Loss: 3.2731\nBatch 1588/1683, Loss: 3.8094\nBatch 1589/1683, Loss: 3.1858\nBatch 1590/1683, Loss: 3.4622\n  Média parcial até aqui: 3.2691\nBatch 1591/1683, Loss: 2.9569\nBatch 1592/1683, Loss: 3.1945\nBatch 1593/1683, Loss: 3.0404\nBatch 1594/1683, Loss: 3.2966\nBatch 1595/1683, Loss: 3.0993\nBatch 1596/1683, Loss: 3.8345\nBatch 1597/1683, Loss: 3.0893\nBatch 1598/1683, Loss: 2.9913\nBatch 1599/1683, Loss: 3.2101\nBatch 1600/1683, Loss: 3.9557\n  Média parcial até aqui: 3.2690\nBatch 1601/1683, Loss: 4.2673\nBatch 1602/1683, Loss: 3.3355\nBatch 1603/1683, Loss: 3.6112\nBatch 1604/1683, Loss: 3.6056\nBatch 1605/1683, Loss: 3.0726\nBatch 1606/1683, Loss: 3.2002\nBatch 1607/1683, Loss: 3.2708\nBatch 1608/1683, Loss: 3.2383\nBatch 1609/1683, Loss: 3.2421\nBatch 1610/1683, Loss: 3.2137\n  Média parcial até aqui: 3.2699\nBatch 1611/1683, Loss: 3.3549\nBatch 1612/1683, Loss: 3.2201\nBatch 1613/1683, Loss: 3.5950\nBatch 1614/1683, Loss: 3.2144\nBatch 1615/1683, Loss: 3.2398\nBatch 1616/1683, Loss: 3.2902\nBatch 1617/1683, Loss: 3.0581\nBatch 1618/1683, Loss: 2.8919\nBatch 1619/1683, Loss: 3.2664\nBatch 1620/1683, Loss: 3.0061\n  Média parcial até aqui: 3.2695\nBatch 1621/1683, Loss: 3.3595\nBatch 1622/1683, Loss: 3.1348\nBatch 1623/1683, Loss: 3.1860\nBatch 1624/1683, Loss: 3.3314\nBatch 1625/1683, Loss: 4.0522\nBatch 1626/1683, Loss: 3.1477\nBatch 1627/1683, Loss: 3.0774\nBatch 1628/1683, Loss: 3.0275\nBatch 1629/1683, Loss: 3.4551\nBatch 1630/1683, Loss: 3.2950\n  Média parcial até aqui: 3.2698\nBatch 1631/1683, Loss: 3.3537\nBatch 1632/1683, Loss: 3.2410\nBatch 1633/1683, Loss: 3.0684\nBatch 1634/1683, Loss: 3.6148\nBatch 1635/1683, Loss: 3.1703\nBatch 1636/1683, Loss: 3.1140\nBatch 1637/1683, Loss: 3.2442\nBatch 1638/1683, Loss: 3.2696\nBatch 1639/1683, Loss: 2.7225\nBatch 1640/1683, Loss: 3.5256\n  Média parcial até aqui: 3.2695\nBatch 1641/1683, Loss: 3.1376\nBatch 1642/1683, Loss: 3.0416\nBatch 1643/1683, Loss: 3.5728\nBatch 1644/1683, Loss: 3.1131\nBatch 1645/1683, Loss: 3.1123\nBatch 1646/1683, Loss: 3.9045\nBatch 1647/1683, Loss: 3.1380\nBatch 1648/1683, Loss: 3.4646\nBatch 1649/1683, Loss: 3.6729\nBatch 1650/1683, Loss: 3.1123\n  Média parcial até aqui: 3.2699\nBatch 1651/1683, Loss: 3.1412\nBatch 1652/1683, Loss: 3.4588\nBatch 1653/1683, Loss: 3.0270\nBatch 1654/1683, Loss: 3.1985\nBatch 1655/1683, Loss: 3.1300\nBatch 1656/1683, Loss: 3.3141\nBatch 1657/1683, Loss: 3.8434\nBatch 1658/1683, Loss: 3.4683\nBatch 1659/1683, Loss: 3.5356\nBatch 1660/1683, Loss: 3.2303\n  Média parcial até aqui: 3.2703\nBatch 1661/1683, Loss: 3.4512\nBatch 1662/1683, Loss: 3.3687\nBatch 1663/1683, Loss: 3.0906\nBatch 1664/1683, Loss: 3.6578\nBatch 1665/1683, Loss: 3.4918\nBatch 1666/1683, Loss: 3.0397\nBatch 1667/1683, Loss: 3.2429\nBatch 1668/1683, Loss: 3.4102\nBatch 1669/1683, Loss: 3.6289\nBatch 1670/1683, Loss: 2.7671\n  Média parcial até aqui: 3.2705\nBatch 1671/1683, Loss: 3.2465\nBatch 1672/1683, Loss: 3.1710\nBatch 1673/1683, Loss: 3.4688\nBatch 1674/1683, Loss: 3.1787\nBatch 1675/1683, Loss: 2.7799\nBatch 1676/1683, Loss: 3.2702\nBatch 1677/1683, Loss: 3.1267\nBatch 1678/1683, Loss: 3.2220\nBatch 1679/1683, Loss: 3.1575\nBatch 1680/1683, Loss: 3.2694\n  Média parcial até aqui: 3.2701\nBatch 1681/1683, Loss: 3.2406\nBatch 1682/1683, Loss: 3.1190\nBatch 1683/1683, Loss: 3.2330\nLoss média final no dataset de teste: 3.2699\nPerplexidade calculada: 26.31\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"# Qwen3","metadata":{}},{"cell_type":"code","source":"from model.gpt2_to_qwen3 import GTP2ModelToQwen3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T01:21:03.995140Z","iopub.execute_input":"2025-10-18T01:21:03.995845Z","iopub.status.idle":"2025-10-18T01:21:04.000551Z","shell.execute_reply.started":"2025-10-18T01:21:03.995820Z","shell.execute_reply":"2025-10-18T01:21:03.999527Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"CONFIG = {\n    \"vocab_size\": TOKENIZER.n_vocab,\n    \"emb_dim\": 512,\n    \"context_length\": 256,\n    \"n_layers\": 8,\n    \"n_heads\": 8,\n    \"head_dim\": 64,\n    \"hidden_dim\": 2048,\n    \"n_kv_groups\": 4,\n    \"qk_norm\": True,\n    \"rope_base\": 10000.0,\n    \"num_experts\": 4,                  # MoE: quantidade de experts\n    \"num_experts_per_token\": 2,        # experts ativos por token\n    \"moe_intermediate_size\": 1024,\n    \"bias\": False,\n    \n    \"batch_size\": 8,\n    \"max_epochs\": 2,\n    \"num_workers\": 0,\n    \"stride\": 256 // 2,\n    \"dtype\": torch.bfloat16,\n    \"device\": device,\n\n    \"eval_freq\": 100,\n    \"eval_iter\": 16,\n\n    \"save_wdb\": True,\n    \"save_freq_wdb\": 5000,\n    \"user\": \"matheus-figueiredo-silva-ufcg\",\n    \"project\": \"gpt2-to-qwen3\",\n    \"name\": \"test1\",\n    \"run_id\": \"gpt2-to-qwen3-run1\",\n    \"version\": \"v1\",\n    \"file_name\": \"mini_mlp.pth\"\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T01:01:45.550447Z","iopub.execute_input":"2025-10-18T01:01:45.550966Z","iopub.status.idle":"2025-10-18T01:01:45.557413Z","shell.execute_reply.started":"2025-10-18T01:01:45.550931Z","shell.execute_reply":"2025-10-18T01:01:45.556621Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"model_qwen3 = GTP2ModelToQwen3(CONFIG).to(device=device, dtype=CONFIG[\"dtype\"])\n\ntorch.backends.cuda.matmul.allow_tf32 = True\ntorch.backends.cudnn.allow_tf32 = True\n\noptimizer = torch.optim.AdamW(model_qwen3.parameters(), lr=0.0004, weight_decay=0.1)\n\nparams = sum(p.numel() for p in model_qwen3.parameters())\nparams_model = params - sum(p.numel() for p in model_qwen3.out_head.parameters())\nprint(f\"Número de parâmetros (sem head): {params_model:,}\")\n\nif device.type == \"cuda\": torch.cuda.reset_peak_memory_stats(device)\n\nstart_time = time.time()\ntokens_processed, total_train_time = run_train(model=model_qwen3, optimizer=optimizer, config=CONFIG, tokenizer=TOKENIZER)\nend_time = time.time()\n\nelapsed = end_time - start_time\ntokens_per_sec = tokens_processed / elapsed\nmax_memory = torch.cuda.max_memory_allocated(device) / (1024**2)  # MB\n\nprint(\"\\nDESEMPENHO:\")\nprint(f\"Tempo total: {elapsed:.2f} s\")\nprint(f\"Tokens/s: {tokens_per_sec:.2f}\")\nprint(f\"Memória máxima: {max_memory:.2f} MB\")\n\nprint(f\"\\nTempo total de treino: {total_train_time:.2f} s ({total_train_time/60:.2f} min)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T01:21:08.582791Z","iopub.execute_input":"2025-10-18T01:21:08.583824Z","iopub.status.idle":"2025-10-18T02:21:34.018033Z","shell.execute_reply.started":"2025-10-18T01:21:08.583789Z","shell.execute_reply":"2025-10-18T02:21:34.017269Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Número de parâmetros (sem head): 82,380,800\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing previous runs because reinit is set to 'default'."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>global_step</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██</td></tr><tr><td>tokens_seen</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██</td></tr><tr><td>train_loss</td><td>▇▇███▅▅▇▇▆▆▅▇▇▇▆▄▁█▅▅▃▅▄▄▃▅</td></tr><tr><td>val_loss</td><td>▇▆▅▇▅▅█▂▄▅▃▆▅▃▁▃▂▂▄▅▅▄▄▃▁▁▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>global_step</td><td>2600</td></tr><tr><td>tokens_seen</td><td>5326848</td></tr><tr><td>train_loss</td><td>2.89453</td></tr><tr><td>val_loss</td><td>3.12012</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">test1</strong> at: <a href='https://wandb.ai/matheus-figueiredo-silva-ufcg/gpt2-to-qwen3/runs/gpt2-to-qwen3-run1' target=\"_blank\">https://wandb.ai/matheus-figueiredo-silva-ufcg/gpt2-to-qwen3/runs/gpt2-to-qwen3-run1</a><br> View project at: <a href='https://wandb.ai/matheus-figueiredo-silva-ufcg/gpt2-to-qwen3' target=\"_blank\">https://wandb.ai/matheus-figueiredo-silva-ufcg/gpt2-to-qwen3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20251018_010459-gpt2-to-qwen3-run1/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.21.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20251018_012110-gpt2-to-qwen3-run1</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Resuming run <strong><a href='https://wandb.ai/matheus-figueiredo-silva-ufcg/gpt2-to-qwen3/runs/gpt2-to-qwen3-run1' target=\"_blank\">test1</a></strong> to <a href='https://wandb.ai/matheus-figueiredo-silva-ufcg/gpt2-to-qwen3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/matheus-figueiredo-silva-ufcg/gpt2-to-qwen3' target=\"_blank\">https://wandb.ai/matheus-figueiredo-silva-ufcg/gpt2-to-qwen3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/matheus-figueiredo-silva-ufcg/gpt2-to-qwen3/runs/gpt2-to-qwen3-run1' target=\"_blank\">https://wandb.ai/matheus-figueiredo-silva-ufcg/gpt2-to-qwen3/runs/gpt2-to-qwen3-run1</a>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact test1:v1, 618.83MB. 1 files... \n\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \nDone. 0:0:1.6 (394.1MB/s)\n","output_type":"stream"},{"name":"stdout","text":"Fetch success -> /kaggle/working/artifacts/test1:v1/mini_mlp.pth\nPesos carregados com sucesso!\n\nEPOCHS/BATCHS RECUPERADOS:  {'epoch': 0, 'batch': 10000, 'train_time': 2311.222449541092}\nEp 1 (Step 000000): Train loss 2.983, Val loss 3.173\nEp 1 (Step 000100): Train loss 2.987, Val loss 3.177\nEp 1 (Step 000200): Train loss 2.917, Val loss 3.195\nEp 1 (Step 000300): Train loss 2.963, Val loss 3.185\nEp 1 (Step 000400): Train loss 2.982, Val loss 3.146\nEp 1 (Step 000500): Train loss 2.902, Val loss 3.140\nEp 1 (Step 000600): Train loss 2.894, Val loss 3.119\nEp 1 (Step 000700): Train loss 2.956, Val loss 3.168\nEp 1 (Step 000800): Train loss 2.963, Val loss 3.148\nEp 1 (Step 000900): Train loss 2.950, Val loss 3.162\nEp 1 (Step 001000): Train loss 2.909, Val loss 3.144\nEp 1 (Step 001100): Train loss 2.894, Val loss 3.183\nEp 1 (Step 001200): Train loss 2.939, Val loss 3.109\nEp 1 (Step 001300): Train loss 2.983, Val loss 3.173\nEp 1 (Step 001400): Train loss 2.890, Val loss 3.108\nEp 1 (Step 001500): Train loss 2.869, Val loss 3.180\nEp 1 (Step 001600): Train loss 2.907, Val loss 3.183\nEp 1 (Step 001700): Train loss 2.927, Val loss 3.122\nEp 1 (Step 001800): Train loss 2.843, Val loss 3.180\nEp 1 (Step 001900): Train loss 2.876, Val loss 3.114\nEp 1 (Step 002000): Train loss 2.894, Val loss 3.124\nEp 1 (Step 002100): Train loss 2.941, Val loss 3.170\nEp 1 (Step 002200): Train loss 2.880, Val loss 3.192\nEp 1 (Step 002300): Train loss 2.934, Val loss 3.157\nEp 1 (Step 002400): Train loss 2.938, Val loss 3.152\nEp 1 (Step 002500): Train loss 2.909, Val loss 3.186\nEp 1 (Step 002600): Train loss 2.910, Val loss 3.157\n\nEXEMPLO DE GERAÇÃO:\nAmostra Gerada: 'Bom dia!  E, como a sua voz, ao longe, ao longe, ao longe, ao longe, ao longe, ao longe, ao longe, ao l'\nEp 2 (Step 002700): Train loss 2.952, Val loss 3.203\nEp 2 (Step 002800): Train loss 2.908, Val loss 3.207\nEp 2 (Step 002900): Train loss 2.918, Val loss 3.140\nEp 2 (Step 003000): Train loss 2.911, Val loss 3.150\nEp 2 (Step 003100): Train loss 2.895, Val loss 3.135\nEp 2 (Step 003200): Train loss 2.829, Val loss 3.135\nEp 2 (Step 003300): Train loss 2.911, Val loss 3.167\nEp 2 (Step 003400): Train loss 2.888, Val loss 3.168\nEp 2 (Step 003500): Train loss 2.905, Val loss 3.179\nEp 2 (Step 003600): Train loss 2.861, Val loss 3.146\nEp 2 (Step 003700): Train loss 2.889, Val loss 3.144\nEp 2 (Step 003800): Train loss 2.877, Val loss 3.100\nEp 2 (Step 003900): Train loss 2.869, Val loss 3.123\nEp 2 (Step 004000): Train loss 2.899, Val loss 3.150\nEp 2 (Step 004100): Train loss 2.844, Val loss 3.104\nEp 2 (Step 004200): Train loss 2.855, Val loss 3.138\nEp 2 (Step 004300): Train loss 2.798, Val loss 3.122\nEp 2 (Step 004400): Train loss 2.877, Val loss 3.116\nEp 2 (Step 004500): Train loss 2.866, Val loss 3.150\nEp 2 (Step 004600): Train loss 2.834, Val loss 3.148\nEp 2 (Step 004700): Train loss 2.821, Val loss 3.146\nEp 2 (Step 004800): Train loss 2.728, Val loss 3.079\nEp 2 (Step 004900): Train loss 2.890, Val loss 3.151\nEp 2 (Step 005000): Train loss 2.861, Val loss 3.108\nEp 2 (Step 005100): Train loss 2.759, Val loss 3.082\nEp 2 (Step 005200): Train loss 2.830, Val loss 3.097\nEp 2 (Step 005300): Train loss 2.825, Val loss 3.140\nEp 2 (Step 005400): Train loss 2.882, Val loss 3.123\nEp 2 (Step 005500): Train loss 2.763, Val loss 3.112\nEp 2 (Step 005600): Train loss 2.841, Val loss 3.156\nEp 2 (Step 005700): Train loss 2.837, Val loss 3.098\nEp 2 (Step 005800): Train loss 2.901, Val loss 3.093\nEp 2 (Step 005900): Train loss 2.855, Val loss 3.081\nEp 2 (Step 006000): Train loss 2.809, Val loss 3.109\nEp 2 (Step 006100): Train loss 2.828, Val loss 3.132\nEp 2 (Step 006200): Train loss 2.820, Val loss 3.142\nEp 2 (Step 006300): Train loss 2.856, Val loss 3.104\nEp 2 (Step 006400): Train loss 2.750, Val loss 3.069\nEp 2 (Step 006500): Train loss 2.876, Val loss 3.097\nEp 2 (Step 006600): Train loss 2.828, Val loss 3.119\nEp 2 (Step 006700): Train loss 2.820, Val loss 3.136\nEp 2 (Step 006800): Train loss 2.786, Val loss 3.079\nEp 2 (Step 006900): Train loss 2.848, Val loss 3.102\nEp 2 (Step 007000): Train loss 2.835, Val loss 3.085\nEp 2 (Step 007100): Train loss 2.830, Val loss 3.098\nEp 2 (Step 007200): Train loss 2.803, Val loss 3.159\nEp 2 (Step 007300): Train loss 2.791, Val loss 3.097\nEp 2 (Step 007400): Train loss 2.794, Val loss 3.113\nEp 2 (Step 007500): Train loss 2.768, Val loss 3.146\nEp 2 (Step 007600): Train loss 2.818, Val loss 3.092\nEp 2 (Step 007700): Train loss 2.777, Val loss 3.097\nEp 2 (Step 007800): Train loss 2.754, Val loss 3.118\nEp 2 (Step 007900): Train loss 2.757, Val loss 3.136\nEp 2 (Step 008000): Train loss 2.678, Val loss 3.082\nEp 2 (Step 008100): Train loss 2.823, Val loss 3.106\nEp 2 (Step 008200): Train loss 2.751, Val loss 3.107\nEp 2 (Step 008300): Train loss 2.736, Val loss 3.068\nEp 2 (Step 008400): Train loss 2.774, Val loss 3.086\nEp 2 (Step 008500): Train loss 2.709, Val loss 3.072\nEp 2 (Step 008600): Train loss 2.793, Val loss 3.067\nEp 2 (Step 008700): Train loss 2.813, Val loss 3.181\nEp 2 (Step 008800): Train loss 2.771, Val loss 3.073\nEp 2 (Step 008900): Train loss 2.771, Val loss 3.083\nEp 2 (Step 009000): Train loss 2.756, Val loss 3.114\nEp 2 (Step 009100): Train loss 2.775, Val loss 3.141\nEp 2 (Step 009200): Train loss 2.766, Val loss 3.077\nEp 2 (Step 009300): Train loss 2.765, Val loss 3.104\nEp 2 (Step 009400): Train loss 2.773, Val loss 3.082\nEp 2 (Step 009500): Train loss 2.723, Val loss 3.077\nEp 2 (Step 009600): Train loss 2.747, Val loss 3.147\nEp 2 (Step 009700): Train loss 2.820, Val loss 3.120\nEp 2 (Step 009800): Train loss 2.771, Val loss 3.106\nEp 2 (Step 009900): Train loss 2.793, Val loss 3.135\nEp 2 (Step 010000): Train loss 2.771, Val loss 3.090\nEp 2 (Step 010100): Train loss 2.864, Val loss 3.075\nEp 2 (Step 010200): Train loss 2.750, Val loss 3.109\nEp 2 (Step 010300): Train loss 2.756, Val loss 3.105\nEp 2 (Step 010400): Train loss 2.782, Val loss 3.057\nEp 2 (Step 010500): Train loss 2.777, Val loss 3.077\nEp 2 (Step 010600): Train loss 2.735, Val loss 3.103\nEp 2 (Step 010700): Train loss 2.763, Val loss 3.090\nEp 2 (Step 010800): Train loss 2.749, Val loss 3.104\nEp 2 (Step 010900): Train loss 2.792, Val loss 3.099\nEp 2 (Step 011000): Train loss 2.780, Val loss 3.101\nEp 2 (Step 011100): Train loss 2.779, Val loss 3.109\nEp 2 (Step 011200): Train loss 2.727, Val loss 3.111\nEp 2 (Step 011300): Train loss 2.759, Val loss 3.107\nEp 2 (Step 011400): Train loss 2.727, Val loss 3.094\nEp 2 (Step 011500): Train loss 2.771, Val loss 3.089\nEp 2 (Step 011600): Train loss 2.660, Val loss 3.095\nEp 2 (Step 011700): Train loss 2.727, Val loss 3.117\nEp 2 (Step 011800): Train loss 2.772, Val loss 3.089\nEp 2 (Step 011900): Train loss 2.770, Val loss 3.050\nEp 2 (Step 012000): Train loss 2.768, Val loss 3.086\nEp 2 (Step 012100): Train loss 2.698, Val loss 3.101\nEp 2 (Step 012200): Train loss 2.725, Val loss 3.102\nEp 2 (Step 012300): Train loss 2.629, Val loss 3.105\nEp 2 (Step 012400): Train loss 2.773, Val loss 3.062\nEp 2 (Step 012500): Train loss 2.721, Val loss 3.140\nEp 2 (Step 012600): Train loss 2.708, Val loss 3.037\nEp 2 (Step 012700): Train loss 2.760, Val loss 3.073\nEp 2 (Step 012800): Train loss 2.712, Val loss 3.034\nEp 2 (Step 012900): Train loss 2.719, Val loss 3.061\nEp 2 (Step 013000): Train loss 2.710, Val loss 3.054\nEp 2 (Step 013100): Train loss 2.737, Val loss 3.087\nEp 2 (Step 013200): Train loss 2.750, Val loss 3.047\nEp 2 (Step 013300): Train loss 2.729, Val loss 3.124\nEp 2 (Step 013400): Train loss 2.707, Val loss 3.043\nEp 2 (Step 013500): Train loss 2.618, Val loss 3.098\nEp 2 (Step 013600): Train loss 2.728, Val loss 3.079\nEp 2 (Step 013700): Train loss 2.754, Val loss 3.011\nEp 2 (Step 013800): Train loss 2.718, Val loss 3.050\nEp 2 (Step 013900): Train loss 2.704, Val loss 3.017\nEp 2 (Step 014000): Train loss 2.702, Val loss 3.058\nEp 2 (Step 014100): Train loss 2.679, Val loss 3.059\nEp 2 (Step 014200): Train loss 2.737, Val loss 3.075\nEp 2 (Step 014300): Train loss 2.725, Val loss 3.072\nEp 2 (Step 014400): Train loss 2.737, Val loss 3.077\nEp 2 (Step 014500): Train loss 2.719, Val loss 3.040\nEp 2 (Step 014600): Train loss 2.681, Val loss 3.079\nEp 2 (Step 014700): Train loss 2.689, Val loss 3.037\nEp 2 (Step 014800): Train loss 2.705, Val loss 3.051\nEp 2 (Step 014900): Train loss 2.679, Val loss 3.139\nEp 2 (Step 015000): Train loss 2.670, Val loss 3.092\nEp 2 (Step 015100): Train loss 2.630, Val loss 3.081\nEp 2 (Step 015200): Train loss 2.720, Val loss 3.079\nEp 2 (Step 015300): Train loss 2.664, Val loss 3.081\n\nEXEMPLO DE GERAÇÃO:\nAmostra Gerada: 'Bom dia!  --Eu no sei se o senhor no estou em casa de minha filha.  --Eu no sei o que  que me fazem.  --Eu no sei o'\n\nGRÁFICO DE PERDA DURANTE O TREINO:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 500x300 with 2 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":"\nDESEMPENHO:\nTempo total: 3623.42 s\nTokens/s: 8648.30\nMemória máxima: 2823.18 MB\n\nTempo total de treino: 5918.50 s (98.64 min)\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"def compute_perplexity(model, data_loader, device):\n    model.eval()\n    total_loss = 0\n    n_batches = 0\n\n    with torch.no_grad():\n        for batch_idx, (x, y) in enumerate(data_loader):\n            x, y = x.to(device), y.to(device)\n            loss = calc_loss_batch_by_cross_entropy(model, x, y, device)\n\n            print(f\"Batch {batch_idx+1}/{len(data_loader)}, Loss: {loss.item():.4f}\")\n\n            total_loss += loss.item()\n            n_batches += 1\n\n            if (batch_idx + 1) % 10 == 0:\n                avg_loss_partial = total_loss / n_batches\n                print(f\"  Média parcial até aqui: {avg_loss_partial:.4f}\")\n\n    avg_loss = total_loss / n_batches\n    print(f\"Loss média final no dataset de teste: {avg_loss:.4f}\")\n\n    perplexity = torch.exp(torch.tensor(avg_loss))\n    print(f\"Perplexidade calculada: {perplexity.item():.2f}\")\n\n    return perplexity.item()\n\nperplexity_test = compute_perplexity(model_qwen3, test_loader, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T02:23:27.177017Z","iopub.execute_input":"2025-10-18T02:23:27.177928Z","iopub.status.idle":"2025-10-18T02:25:34.648925Z","shell.execute_reply.started":"2025-10-18T02:23:27.177903Z","shell.execute_reply":"2025-10-18T02:25:34.647913Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Batch 1/1683, Loss: 3.4375\nBatch 2/1683, Loss: 3.0781\nBatch 3/1683, Loss: 3.0156\nBatch 4/1683, Loss: 3.1094\nBatch 5/1683, Loss: 3.0469\nBatch 6/1683, Loss: 3.1875\nBatch 7/1683, Loss: 3.4375\nBatch 8/1683, Loss: 2.9531\nBatch 9/1683, Loss: 3.1094\nBatch 10/1683, Loss: 3.0938\n  Média parcial até aqui: 3.1469\nBatch 11/1683, Loss: 3.6094\nBatch 12/1683, Loss: 3.2188\nBatch 13/1683, Loss: 2.9688\nBatch 14/1683, Loss: 2.7812\nBatch 15/1683, Loss: 2.9844\nBatch 16/1683, Loss: 3.2188\nBatch 17/1683, Loss: 2.8906\nBatch 18/1683, Loss: 2.6719\nBatch 19/1683, Loss: 3.1719\nBatch 20/1683, Loss: 3.0625\n  Média parcial até aqui: 3.1023\nBatch 21/1683, Loss: 3.1250\nBatch 22/1683, Loss: 2.9844\nBatch 23/1683, Loss: 3.0469\nBatch 24/1683, Loss: 3.2031\nBatch 25/1683, Loss: 3.2344\nBatch 26/1683, Loss: 3.0781\nBatch 27/1683, Loss: 3.2344\nBatch 28/1683, Loss: 2.9844\nBatch 29/1683, Loss: 3.2969\nBatch 30/1683, Loss: 3.1875\n  Média parcial até aqui: 3.1141\nBatch 31/1683, Loss: 3.0000\nBatch 32/1683, Loss: 3.6406\nBatch 33/1683, Loss: 3.4844\nBatch 34/1683, Loss: 3.1250\nBatch 35/1683, Loss: 3.3906\nBatch 36/1683, Loss: 3.0000\nBatch 37/1683, Loss: 2.8906\nBatch 38/1683, Loss: 3.2969\nBatch 39/1683, Loss: 3.4688\nBatch 40/1683, Loss: 3.0938\n  Média parcial até aqui: 3.1453\nBatch 41/1683, Loss: 3.8594\nBatch 42/1683, Loss: 2.8750\nBatch 43/1683, Loss: 3.4688\nBatch 44/1683, Loss: 4.1875\nBatch 45/1683, Loss: 3.0938\nBatch 46/1683, Loss: 2.9375\nBatch 47/1683, Loss: 3.0312\nBatch 48/1683, Loss: 3.1562\nBatch 49/1683, Loss: 3.8906\nBatch 50/1683, Loss: 3.2188\n  Média parcial até aqui: 3.1906\nBatch 51/1683, Loss: 3.3281\nBatch 52/1683, Loss: 3.1562\nBatch 53/1683, Loss: 2.8594\nBatch 54/1683, Loss: 3.5938\nBatch 55/1683, Loss: 3.5469\nBatch 56/1683, Loss: 3.0781\nBatch 57/1683, Loss: 3.3594\nBatch 58/1683, Loss: 3.3594\nBatch 59/1683, Loss: 3.3594\nBatch 60/1683, Loss: 3.2969\n  Média parcial até aqui: 3.2078\nBatch 61/1683, Loss: 3.1719\nBatch 62/1683, Loss: 3.1875\nBatch 63/1683, Loss: 2.7344\nBatch 64/1683, Loss: 3.0938\nBatch 65/1683, Loss: 3.0156\nBatch 66/1683, Loss: 3.1094\nBatch 67/1683, Loss: 3.1250\nBatch 68/1683, Loss: 3.1875\nBatch 69/1683, Loss: 3.6562\nBatch 70/1683, Loss: 3.4688\n  Média parcial até aqui: 3.2031\nBatch 71/1683, Loss: 3.3438\nBatch 72/1683, Loss: 2.8750\nBatch 73/1683, Loss: 3.0938\nBatch 74/1683, Loss: 2.9219\nBatch 75/1683, Loss: 3.4062\nBatch 76/1683, Loss: 3.7656\nBatch 77/1683, Loss: 3.0938\nBatch 78/1683, Loss: 2.8906\nBatch 79/1683, Loss: 3.4844\nBatch 80/1683, Loss: 3.2812\n  Média parcial até aqui: 3.2047\nBatch 81/1683, Loss: 3.3281\nBatch 82/1683, Loss: 3.1250\nBatch 83/1683, Loss: 3.0938\nBatch 84/1683, Loss: 3.1406\nBatch 85/1683, Loss: 2.9844\nBatch 86/1683, Loss: 3.2031\nBatch 87/1683, Loss: 3.0938\nBatch 88/1683, Loss: 3.0938\nBatch 89/1683, Loss: 3.5625\nBatch 90/1683, Loss: 3.2031\n  Média parcial até aqui: 3.2023\nBatch 91/1683, Loss: 3.1250\nBatch 92/1683, Loss: 2.8750\nBatch 93/1683, Loss: 3.1719\nBatch 94/1683, Loss: 3.4062\nBatch 95/1683, Loss: 3.1094\nBatch 96/1683, Loss: 2.9062\nBatch 97/1683, Loss: 3.3281\nBatch 98/1683, Loss: 3.5781\nBatch 99/1683, Loss: 3.3281\nBatch 100/1683, Loss: 3.2812\n  Média parcial até aqui: 3.2031\nBatch 101/1683, Loss: 3.1562\nBatch 102/1683, Loss: 2.8594\nBatch 103/1683, Loss: 3.1562\nBatch 104/1683, Loss: 3.0156\nBatch 105/1683, Loss: 3.2812\nBatch 106/1683, Loss: 2.9688\nBatch 107/1683, Loss: 2.6719\nBatch 108/1683, Loss: 3.3906\nBatch 109/1683, Loss: 3.3906\nBatch 110/1683, Loss: 3.0469\n  Média parcial até aqui: 3.1932\nBatch 111/1683, Loss: 3.1250\nBatch 112/1683, Loss: 3.1719\nBatch 113/1683, Loss: 3.0312\nBatch 114/1683, Loss: 2.9531\nBatch 115/1683, Loss: 3.1094\nBatch 116/1683, Loss: 3.3438\nBatch 117/1683, Loss: 3.3438\nBatch 118/1683, Loss: 3.1875\nBatch 119/1683, Loss: 3.2031\nBatch 120/1683, Loss: 2.9531\n  Média parcial até aqui: 3.1889\nBatch 121/1683, Loss: 2.8750\nBatch 122/1683, Loss: 3.4375\nBatch 123/1683, Loss: 2.6562\nBatch 124/1683, Loss: 3.5469\nBatch 125/1683, Loss: 3.2812\nBatch 126/1683, Loss: 3.0781\nBatch 127/1683, Loss: 3.5625\nBatch 128/1683, Loss: 3.3125\nBatch 129/1683, Loss: 3.4844\nBatch 130/1683, Loss: 3.4062\n  Média parcial até aqui: 3.1947\nBatch 131/1683, Loss: 3.2031\nBatch 132/1683, Loss: 3.0469\nBatch 133/1683, Loss: 3.1250\nBatch 134/1683, Loss: 3.2656\nBatch 135/1683, Loss: 3.2344\nBatch 136/1683, Loss: 3.2188\nBatch 137/1683, Loss: 3.5625\nBatch 138/1683, Loss: 3.5781\nBatch 139/1683, Loss: 3.2188\nBatch 140/1683, Loss: 3.0938\n  Média parcial até aqui: 3.1990\nBatch 141/1683, Loss: 2.7969\nBatch 142/1683, Loss: 3.0625\nBatch 143/1683, Loss: 3.1250\nBatch 144/1683, Loss: 3.3594\nBatch 145/1683, Loss: 3.6094\nBatch 146/1683, Loss: 3.2969\nBatch 147/1683, Loss: 2.9688\nBatch 148/1683, Loss: 3.5156\nBatch 149/1683, Loss: 2.7969\nBatch 150/1683, Loss: 3.3906\n  Média parcial até aqui: 3.1985\nBatch 151/1683, Loss: 3.1406\nBatch 152/1683, Loss: 3.0312\nBatch 153/1683, Loss: 3.6719\nBatch 154/1683, Loss: 3.3906\nBatch 155/1683, Loss: 3.1250\nBatch 156/1683, Loss: 3.2969\nBatch 157/1683, Loss: 2.8438\nBatch 158/1683, Loss: 3.1094\nBatch 159/1683, Loss: 3.4844\nBatch 160/1683, Loss: 3.1719\n  Média parcial até aqui: 3.2003\nBatch 161/1683, Loss: 2.9219\nBatch 162/1683, Loss: 3.0000\nBatch 163/1683, Loss: 3.4062\nBatch 164/1683, Loss: 3.2031\nBatch 165/1683, Loss: 3.3281\nBatch 166/1683, Loss: 3.3750\nBatch 167/1683, Loss: 3.0938\nBatch 168/1683, Loss: 3.4531\nBatch 169/1683, Loss: 2.9062\nBatch 170/1683, Loss: 3.3438\n  Média parcial até aqui: 3.2005\nBatch 171/1683, Loss: 3.0938\nBatch 172/1683, Loss: 3.0781\nBatch 173/1683, Loss: 2.9844\nBatch 174/1683, Loss: 3.1094\nBatch 175/1683, Loss: 3.5000\nBatch 176/1683, Loss: 3.5156\nBatch 177/1683, Loss: 3.4844\nBatch 178/1683, Loss: 3.1094\nBatch 179/1683, Loss: 2.9219\nBatch 180/1683, Loss: 3.0938\n  Média parcial até aqui: 3.1998\nBatch 181/1683, Loss: 3.0938\nBatch 182/1683, Loss: 3.1094\nBatch 183/1683, Loss: 3.7656\nBatch 184/1683, Loss: 3.2344\nBatch 185/1683, Loss: 3.2344\nBatch 186/1683, Loss: 2.9219\nBatch 187/1683, Loss: 3.0781\nBatch 188/1683, Loss: 3.0469\nBatch 189/1683, Loss: 3.6875\nBatch 190/1683, Loss: 3.5156\n  Média parcial até aqui: 3.2035\nBatch 191/1683, Loss: 3.3281\nBatch 192/1683, Loss: 3.0000\nBatch 193/1683, Loss: 3.0469\nBatch 194/1683, Loss: 3.7656\nBatch 195/1683, Loss: 3.3281\nBatch 196/1683, Loss: 3.0469\nBatch 197/1683, Loss: 3.1875\nBatch 198/1683, Loss: 2.9531\nBatch 199/1683, Loss: 3.0625\nBatch 200/1683, Loss: 3.1719\n  Média parcial até aqui: 3.2027\nBatch 201/1683, Loss: 3.0312\nBatch 202/1683, Loss: 3.2031\nBatch 203/1683, Loss: 2.8906\nBatch 204/1683, Loss: 3.1875\nBatch 205/1683, Loss: 3.0156\nBatch 206/1683, Loss: 2.8750\nBatch 207/1683, Loss: 3.3906\nBatch 208/1683, Loss: 3.6719\nBatch 209/1683, Loss: 3.1094\nBatch 210/1683, Loss: 3.3906\n  Média parcial até aqui: 3.2015\nBatch 211/1683, Loss: 3.1250\nBatch 212/1683, Loss: 3.1406\nBatch 213/1683, Loss: 3.0000\nBatch 214/1683, Loss: 3.0312\nBatch 215/1683, Loss: 2.7969\nBatch 216/1683, Loss: 3.2344\nBatch 217/1683, Loss: 3.0938\nBatch 218/1683, Loss: 3.0781\nBatch 219/1683, Loss: 3.1406\nBatch 220/1683, Loss: 2.9062\n  Média parcial até aqui: 3.1948\nBatch 221/1683, Loss: 3.0938\nBatch 222/1683, Loss: 2.9531\nBatch 223/1683, Loss: 3.4688\nBatch 224/1683, Loss: 2.9219\nBatch 225/1683, Loss: 2.9688\nBatch 226/1683, Loss: 3.0781\nBatch 227/1683, Loss: 3.3906\nBatch 228/1683, Loss: 3.0938\nBatch 229/1683, Loss: 3.2031\nBatch 230/1683, Loss: 3.2031\n  Média parcial até aqui: 3.1923\nBatch 231/1683, Loss: 3.0938\nBatch 232/1683, Loss: 3.4844\nBatch 233/1683, Loss: 3.2344\nBatch 234/1683, Loss: 3.0000\nBatch 235/1683, Loss: 3.0312\nBatch 236/1683, Loss: 3.4531\nBatch 237/1683, Loss: 3.2656\nBatch 238/1683, Loss: 3.4375\nBatch 239/1683, Loss: 3.2812\nBatch 240/1683, Loss: 2.8906\n  Média parcial até aqui: 3.1934\nBatch 241/1683, Loss: 2.8438\nBatch 242/1683, Loss: 3.0312\nBatch 243/1683, Loss: 3.1250\nBatch 244/1683, Loss: 3.2188\nBatch 245/1683, Loss: 3.3438\nBatch 246/1683, Loss: 3.8906\nBatch 247/1683, Loss: 3.5312\nBatch 248/1683, Loss: 2.4531\nBatch 249/1683, Loss: 3.1250\nBatch 250/1683, Loss: 3.3438\n  Média parcial até aqui: 3.1932\nBatch 251/1683, Loss: 3.7812\nBatch 252/1683, Loss: 3.4688\nBatch 253/1683, Loss: 2.8906\nBatch 254/1683, Loss: 3.2656\nBatch 255/1683, Loss: 3.2188\nBatch 256/1683, Loss: 3.2188\nBatch 257/1683, Loss: 3.2031\nBatch 258/1683, Loss: 3.0469\nBatch 259/1683, Loss: 3.0469\nBatch 260/1683, Loss: 3.0938\n  Média parcial até aqui: 3.1944\nBatch 261/1683, Loss: 3.1562\nBatch 262/1683, Loss: 3.1562\nBatch 263/1683, Loss: 3.0625\nBatch 264/1683, Loss: 3.1406\nBatch 265/1683, Loss: 3.2969\nBatch 266/1683, Loss: 3.7031\nBatch 267/1683, Loss: 3.3125\nBatch 268/1683, Loss: 3.2656\nBatch 269/1683, Loss: 3.2969\nBatch 270/1683, Loss: 3.1875\n  Média parcial até aqui: 3.1968\nBatch 271/1683, Loss: 3.4062\nBatch 272/1683, Loss: 3.1562\nBatch 273/1683, Loss: 3.2500\nBatch 274/1683, Loss: 2.6250\nBatch 275/1683, Loss: 3.1562\nBatch 276/1683, Loss: 2.9219\nBatch 277/1683, Loss: 3.1250\nBatch 278/1683, Loss: 2.9688\nBatch 279/1683, Loss: 3.1875\nBatch 280/1683, Loss: 2.9062\n  Média parcial até aqui: 3.1922\nBatch 281/1683, Loss: 3.9062\nBatch 282/1683, Loss: 3.0469\nBatch 283/1683, Loss: 3.3438\nBatch 284/1683, Loss: 3.5156\nBatch 285/1683, Loss: 2.8125\nBatch 286/1683, Loss: 3.0000\nBatch 287/1683, Loss: 3.3750\nBatch 288/1683, Loss: 3.0312\nBatch 289/1683, Loss: 3.1406\nBatch 290/1683, Loss: 2.9688\n  Média parcial até aqui: 3.1930\nBatch 291/1683, Loss: 3.2188\nBatch 292/1683, Loss: 2.9688\nBatch 293/1683, Loss: 2.9219\nBatch 294/1683, Loss: 3.6406\nBatch 295/1683, Loss: 3.0625\nBatch 296/1683, Loss: 3.0469\nBatch 297/1683, Loss: 3.3438\nBatch 298/1683, Loss: 3.0000\nBatch 299/1683, Loss: 3.7500\nBatch 300/1683, Loss: 2.9375\n  Média parcial até aqui: 3.1929\nBatch 301/1683, Loss: 3.3438\nBatch 302/1683, Loss: 3.1719\nBatch 303/1683, Loss: 2.9219\nBatch 304/1683, Loss: 3.2500\nBatch 305/1683, Loss: 3.2188\nBatch 306/1683, Loss: 3.3750\nBatch 307/1683, Loss: 2.7656\nBatch 308/1683, Loss: 2.9531\nBatch 309/1683, Loss: 2.7500\nBatch 310/1683, Loss: 3.2344\n  Média parcial até aqui: 3.1898\nBatch 311/1683, Loss: 3.1250\nBatch 312/1683, Loss: 2.8594\nBatch 313/1683, Loss: 3.4062\nBatch 314/1683, Loss: 3.0469\nBatch 315/1683, Loss: 3.4062\nBatch 316/1683, Loss: 3.2656\nBatch 317/1683, Loss: 3.4375\nBatch 318/1683, Loss: 2.4531\nBatch 319/1683, Loss: 3.2969\nBatch 320/1683, Loss: 3.2656\n  Média parcial até aqui: 3.1888\nBatch 321/1683, Loss: 3.2344\nBatch 322/1683, Loss: 3.1562\nBatch 323/1683, Loss: 3.2656\nBatch 324/1683, Loss: 3.2344\nBatch 325/1683, Loss: 3.2656\nBatch 326/1683, Loss: 3.1250\nBatch 327/1683, Loss: 2.9531\nBatch 328/1683, Loss: 2.7031\nBatch 329/1683, Loss: 3.0938\nBatch 330/1683, Loss: 3.3125\n  Média parcial até aqui: 3.1871\nBatch 331/1683, Loss: 3.1250\nBatch 332/1683, Loss: 3.0625\nBatch 333/1683, Loss: 3.2188\nBatch 334/1683, Loss: 3.9062\nBatch 335/1683, Loss: 2.9219\nBatch 336/1683, Loss: 3.2812\nBatch 337/1683, Loss: 3.4062\nBatch 338/1683, Loss: 3.2656\nBatch 339/1683, Loss: 3.1875\nBatch 340/1683, Loss: 3.4531\n  Média parcial até aqui: 3.1899\nBatch 341/1683, Loss: 3.1094\nBatch 342/1683, Loss: 3.3281\nBatch 343/1683, Loss: 3.0312\nBatch 344/1683, Loss: 3.1406\nBatch 345/1683, Loss: 3.2969\nBatch 346/1683, Loss: 3.2812\nBatch 347/1683, Loss: 3.3281\nBatch 348/1683, Loss: 3.4688\nBatch 349/1683, Loss: 3.0469\nBatch 350/1683, Loss: 3.3750\n  Média parcial até aqui: 3.1914\nBatch 351/1683, Loss: 3.2812\nBatch 352/1683, Loss: 3.0781\nBatch 353/1683, Loss: 3.1875\nBatch 354/1683, Loss: 3.3125\nBatch 355/1683, Loss: 3.1875\nBatch 356/1683, Loss: 3.2656\nBatch 357/1683, Loss: 2.9375\nBatch 358/1683, Loss: 3.2344\nBatch 359/1683, Loss: 3.2188\nBatch 360/1683, Loss: 2.9531\n  Média parcial até aqui: 3.1907\nBatch 361/1683, Loss: 3.1562\nBatch 362/1683, Loss: 4.2188\nBatch 363/1683, Loss: 3.1250\nBatch 364/1683, Loss: 3.0781\nBatch 365/1683, Loss: 3.1719\nBatch 366/1683, Loss: 3.4688\nBatch 367/1683, Loss: 3.3594\nBatch 368/1683, Loss: 3.5156\nBatch 369/1683, Loss: 3.0938\nBatch 370/1683, Loss: 3.5781\n  Média parcial até aqui: 3.1957\nBatch 371/1683, Loss: 2.8906\nBatch 372/1683, Loss: 3.0625\nBatch 373/1683, Loss: 3.4531\nBatch 374/1683, Loss: 3.3125\nBatch 375/1683, Loss: 3.2969\nBatch 376/1683, Loss: 3.0781\nBatch 377/1683, Loss: 3.2031\nBatch 378/1683, Loss: 3.1406\nBatch 379/1683, Loss: 3.0312\nBatch 380/1683, Loss: 3.2188\n  Média parcial até aqui: 3.1950\nBatch 381/1683, Loss: 2.8281\nBatch 382/1683, Loss: 3.7656\nBatch 383/1683, Loss: 3.2344\nBatch 384/1683, Loss: 3.1719\nBatch 385/1683, Loss: 2.9688\nBatch 386/1683, Loss: 3.2500\nBatch 387/1683, Loss: 3.0781\nBatch 388/1683, Loss: 3.0938\nBatch 389/1683, Loss: 3.2656\nBatch 390/1683, Loss: 3.1406\n  Média parcial até aqui: 3.1946\nBatch 391/1683, Loss: 3.7188\nBatch 392/1683, Loss: 3.1562\nBatch 393/1683, Loss: 3.2812\nBatch 394/1683, Loss: 3.3594\nBatch 395/1683, Loss: 3.4219\nBatch 396/1683, Loss: 3.0000\nBatch 397/1683, Loss: 3.1875\nBatch 398/1683, Loss: 3.1250\nBatch 399/1683, Loss: 3.0312\nBatch 400/1683, Loss: 2.8750\n  Média parcial até aqui: 3.1951\nBatch 401/1683, Loss: 3.0000\nBatch 402/1683, Loss: 3.0938\nBatch 403/1683, Loss: 3.5781\nBatch 404/1683, Loss: 3.1562\nBatch 405/1683, Loss: 3.3125\nBatch 406/1683, Loss: 3.4688\nBatch 407/1683, Loss: 3.3594\nBatch 408/1683, Loss: 2.9062\nBatch 409/1683, Loss: 3.0000\nBatch 410/1683, Loss: 3.2031\n  Média parcial até aqui: 3.1954\nBatch 411/1683, Loss: 3.0938\nBatch 412/1683, Loss: 3.2188\nBatch 413/1683, Loss: 3.4531\nBatch 414/1683, Loss: 3.0156\nBatch 415/1683, Loss: 3.3281\nBatch 416/1683, Loss: 3.0938\nBatch 417/1683, Loss: 3.1406\nBatch 418/1683, Loss: 2.9219\nBatch 419/1683, Loss: 2.8594\nBatch 420/1683, Loss: 2.9062\n  Média parcial até aqui: 3.1932\nBatch 421/1683, Loss: 3.5469\nBatch 422/1683, Loss: 3.2344\nBatch 423/1683, Loss: 3.0469\nBatch 424/1683, Loss: 3.2344\nBatch 425/1683, Loss: 3.2031\nBatch 426/1683, Loss: 3.4219\nBatch 427/1683, Loss: 3.0938\nBatch 428/1683, Loss: 3.3594\nBatch 429/1683, Loss: 3.5938\nBatch 430/1683, Loss: 3.6562\n  Média parcial até aqui: 3.1966\nBatch 431/1683, Loss: 3.5000\nBatch 432/1683, Loss: 3.6875\nBatch 433/1683, Loss: 3.6719\nBatch 434/1683, Loss: 3.4219\nBatch 435/1683, Loss: 2.7656\nBatch 436/1683, Loss: 2.9219\nBatch 437/1683, Loss: 3.2031\nBatch 438/1683, Loss: 3.1719\nBatch 439/1683, Loss: 3.5781\nBatch 440/1683, Loss: 3.1406\n  Média parcial até aqui: 3.1991\nBatch 441/1683, Loss: 3.3125\nBatch 442/1683, Loss: 3.0156\nBatch 443/1683, Loss: 3.0312\nBatch 444/1683, Loss: 3.7969\nBatch 445/1683, Loss: 3.0781\nBatch 446/1683, Loss: 3.2344\nBatch 447/1683, Loss: 3.2500\nBatch 448/1683, Loss: 3.1250\nBatch 449/1683, Loss: 3.1875\nBatch 450/1683, Loss: 3.4531\n  Média parcial até aqui: 3.2002\nBatch 451/1683, Loss: 3.2188\nBatch 452/1683, Loss: 3.2812\nBatch 453/1683, Loss: 3.2812\nBatch 454/1683, Loss: 2.9688\nBatch 455/1683, Loss: 2.9844\nBatch 456/1683, Loss: 3.5938\nBatch 457/1683, Loss: 3.2344\nBatch 458/1683, Loss: 2.8594\nBatch 459/1683, Loss: 3.4375\nBatch 460/1683, Loss: 3.1719\n  Média parcial até aqui: 3.2003\nBatch 461/1683, Loss: 2.8438\nBatch 462/1683, Loss: 3.5781\nBatch 463/1683, Loss: 2.9531\nBatch 464/1683, Loss: 3.5625\nBatch 465/1683, Loss: 3.5156\nBatch 466/1683, Loss: 3.1875\nBatch 467/1683, Loss: 3.4531\nBatch 468/1683, Loss: 2.7969\nBatch 469/1683, Loss: 2.6562\nBatch 470/1683, Loss: 3.0312\n  Média parcial até aqui: 3.1994\nBatch 471/1683, Loss: 3.1875\nBatch 472/1683, Loss: 4.0000\nBatch 473/1683, Loss: 3.2500\nBatch 474/1683, Loss: 3.2188\nBatch 475/1683, Loss: 3.0781\nBatch 476/1683, Loss: 3.0781\nBatch 477/1683, Loss: 2.9375\nBatch 478/1683, Loss: 3.2812\nBatch 479/1683, Loss: 3.3750\nBatch 480/1683, Loss: 3.0625\n  Média parcial até aqui: 3.2004\nBatch 481/1683, Loss: 3.1719\nBatch 482/1683, Loss: 3.1719\nBatch 483/1683, Loss: 3.3125\nBatch 484/1683, Loss: 3.3281\nBatch 485/1683, Loss: 3.0156\nBatch 486/1683, Loss: 3.0312\nBatch 487/1683, Loss: 3.3906\nBatch 488/1683, Loss: 3.4531\nBatch 489/1683, Loss: 3.0000\nBatch 490/1683, Loss: 3.2656\n  Média parcial até aqui: 3.2006\nBatch 491/1683, Loss: 3.3594\nBatch 492/1683, Loss: 3.1719\nBatch 493/1683, Loss: 3.6719\nBatch 494/1683, Loss: 3.4062\nBatch 495/1683, Loss: 3.0156\nBatch 496/1683, Loss: 3.1719\nBatch 497/1683, Loss: 3.1562\nBatch 498/1683, Loss: 3.4531\nBatch 499/1683, Loss: 3.3438\nBatch 500/1683, Loss: 2.9844\n  Média parcial até aqui: 3.2021\nBatch 501/1683, Loss: 3.2812\nBatch 502/1683, Loss: 3.2188\nBatch 503/1683, Loss: 3.3125\nBatch 504/1683, Loss: 3.4375\nBatch 505/1683, Loss: 3.2500\nBatch 506/1683, Loss: 3.0000\nBatch 507/1683, Loss: 3.1406\nBatch 508/1683, Loss: 3.4844\nBatch 509/1683, Loss: 3.1406\nBatch 510/1683, Loss: 3.5000\n  Média parcial até aqui: 3.2036\nBatch 511/1683, Loss: 3.8594\nBatch 512/1683, Loss: 3.0938\nBatch 513/1683, Loss: 3.3438\nBatch 514/1683, Loss: 3.8906\nBatch 515/1683, Loss: 3.2969\nBatch 516/1683, Loss: 3.3594\nBatch 517/1683, Loss: 3.2500\nBatch 518/1683, Loss: 3.3906\nBatch 519/1683, Loss: 3.6094\nBatch 520/1683, Loss: 3.5938\n  Média parcial até aqui: 3.2087\nBatch 521/1683, Loss: 3.1094\nBatch 522/1683, Loss: 3.2344\nBatch 523/1683, Loss: 3.0156\nBatch 524/1683, Loss: 3.2031\nBatch 525/1683, Loss: 3.1250\nBatch 526/1683, Loss: 2.9375\nBatch 527/1683, Loss: 3.0938\nBatch 528/1683, Loss: 3.3438\nBatch 529/1683, Loss: 2.9375\nBatch 530/1683, Loss: 2.7500\n  Média parcial até aqui: 3.2061\nBatch 531/1683, Loss: 3.1719\nBatch 532/1683, Loss: 3.4219\nBatch 533/1683, Loss: 3.1094\nBatch 534/1683, Loss: 3.0156\nBatch 535/1683, Loss: 3.0938\nBatch 536/1683, Loss: 3.0781\nBatch 537/1683, Loss: 3.1094\nBatch 538/1683, Loss: 2.9219\nBatch 539/1683, Loss: 3.4375\nBatch 540/1683, Loss: 3.5938\n  Média parcial até aqui: 3.2059\nBatch 541/1683, Loss: 3.3594\nBatch 542/1683, Loss: 3.2500\nBatch 543/1683, Loss: 3.1562\nBatch 544/1683, Loss: 3.5000\nBatch 545/1683, Loss: 3.3594\nBatch 546/1683, Loss: 3.2188\nBatch 547/1683, Loss: 3.4844\nBatch 548/1683, Loss: 3.1562\nBatch 549/1683, Loss: 3.5312\nBatch 550/1683, Loss: 2.9688\n  Média parcial até aqui: 3.2076\nBatch 551/1683, Loss: 3.3125\nBatch 552/1683, Loss: 3.3438\nBatch 553/1683, Loss: 3.1250\nBatch 554/1683, Loss: 3.4688\nBatch 555/1683, Loss: 3.2969\nBatch 556/1683, Loss: 3.3438\nBatch 557/1683, Loss: 2.9219\nBatch 558/1683, Loss: 2.8750\nBatch 559/1683, Loss: 3.0781\nBatch 560/1683, Loss: 3.2812\n  Média parcial até aqui: 3.2076\nBatch 561/1683, Loss: 2.9531\nBatch 562/1683, Loss: 2.7031\nBatch 563/1683, Loss: 3.0156\nBatch 564/1683, Loss: 3.5625\nBatch 565/1683, Loss: 3.9375\nBatch 566/1683, Loss: 3.1250\nBatch 567/1683, Loss: 3.1406\nBatch 568/1683, Loss: 3.0000\nBatch 569/1683, Loss: 3.0312\nBatch 570/1683, Loss: 3.1562\n  Média parcial até aqui: 3.2068\nBatch 571/1683, Loss: 2.8906\nBatch 572/1683, Loss: 3.4844\nBatch 573/1683, Loss: 3.0938\nBatch 574/1683, Loss: 3.7031\nBatch 575/1683, Loss: 3.1562\nBatch 576/1683, Loss: 3.5781\nBatch 577/1683, Loss: 2.9688\nBatch 578/1683, Loss: 3.2344\nBatch 579/1683, Loss: 3.0469\nBatch 580/1683, Loss: 3.1719\n  Média parcial até aqui: 3.2072\nBatch 581/1683, Loss: 3.4688\nBatch 582/1683, Loss: 3.1562\nBatch 583/1683, Loss: 2.7031\nBatch 584/1683, Loss: 3.0781\nBatch 585/1683, Loss: 2.8594\nBatch 586/1683, Loss: 3.4531\nBatch 587/1683, Loss: 3.2031\nBatch 588/1683, Loss: 3.0625\nBatch 589/1683, Loss: 3.0469\nBatch 590/1683, Loss: 3.0625\n  Média parcial até aqui: 3.2056\nBatch 591/1683, Loss: 3.0000\nBatch 592/1683, Loss: 3.2344\nBatch 593/1683, Loss: 3.2656\nBatch 594/1683, Loss: 2.8438\nBatch 595/1683, Loss: 3.1875\nBatch 596/1683, Loss: 3.7031\nBatch 597/1683, Loss: 3.1094\nBatch 598/1683, Loss: 3.2812\nBatch 599/1683, Loss: 3.1250\nBatch 600/1683, Loss: 3.0938\n  Média parcial até aqui: 3.2052\nBatch 601/1683, Loss: 3.5000\nBatch 602/1683, Loss: 3.0938\nBatch 603/1683, Loss: 3.2344\nBatch 604/1683, Loss: 3.0000\nBatch 605/1683, Loss: 3.5156\nBatch 606/1683, Loss: 3.6875\nBatch 607/1683, Loss: 3.4688\nBatch 608/1683, Loss: 3.8906\nBatch 609/1683, Loss: 3.1094\nBatch 610/1683, Loss: 3.1875\n  Média parcial até aqui: 3.2079\nBatch 611/1683, Loss: 3.1719\nBatch 612/1683, Loss: 3.0312\nBatch 613/1683, Loss: 3.4062\nBatch 614/1683, Loss: 2.8438\nBatch 615/1683, Loss: 3.3750\nBatch 616/1683, Loss: 3.0000\nBatch 617/1683, Loss: 3.0938\nBatch 618/1683, Loss: 2.9375\nBatch 619/1683, Loss: 3.1250\nBatch 620/1683, Loss: 3.0469\n  Média parcial até aqui: 3.2062\nBatch 621/1683, Loss: 3.3125\nBatch 622/1683, Loss: 3.0938\nBatch 623/1683, Loss: 3.1562\nBatch 624/1683, Loss: 2.9531\nBatch 625/1683, Loss: 3.3125\nBatch 626/1683, Loss: 2.6094\nBatch 627/1683, Loss: 3.3906\nBatch 628/1683, Loss: 3.8125\nBatch 629/1683, Loss: 3.4375\nBatch 630/1683, Loss: 3.2031\n  Média parcial até aqui: 3.2065\nBatch 631/1683, Loss: 3.0781\nBatch 632/1683, Loss: 3.1406\nBatch 633/1683, Loss: 3.1875\nBatch 634/1683, Loss: 3.6094\nBatch 635/1683, Loss: 3.1875\nBatch 636/1683, Loss: 3.0000\nBatch 637/1683, Loss: 3.5781\nBatch 638/1683, Loss: 3.4375\nBatch 639/1683, Loss: 3.0625\nBatch 640/1683, Loss: 3.1719\n  Média parcial até aqui: 3.2072\nBatch 641/1683, Loss: 2.9531\nBatch 642/1683, Loss: 3.0469\nBatch 643/1683, Loss: 3.4844\nBatch 644/1683, Loss: 3.5156\nBatch 645/1683, Loss: 2.9844\nBatch 646/1683, Loss: 3.4062\nBatch 647/1683, Loss: 3.0938\nBatch 648/1683, Loss: 3.0156\nBatch 649/1683, Loss: 3.2344\nBatch 650/1683, Loss: 3.1406\n  Média parcial até aqui: 3.2069\nBatch 651/1683, Loss: 2.9219\nBatch 652/1683, Loss: 3.1719\nBatch 653/1683, Loss: 2.9375\nBatch 654/1683, Loss: 2.8906\nBatch 655/1683, Loss: 3.4062\nBatch 656/1683, Loss: 3.3906\nBatch 657/1683, Loss: 3.3906\nBatch 658/1683, Loss: 3.0625\nBatch 659/1683, Loss: 2.9688\nBatch 660/1683, Loss: 3.0938\n  Média parcial até aqui: 3.2056\nBatch 661/1683, Loss: 2.7188\nBatch 662/1683, Loss: 3.2812\nBatch 663/1683, Loss: 3.1406\nBatch 664/1683, Loss: 2.9844\nBatch 665/1683, Loss: 3.4219\nBatch 666/1683, Loss: 3.5469\nBatch 667/1683, Loss: 3.0781\nBatch 668/1683, Loss: 3.2812\nBatch 669/1683, Loss: 3.8906\nBatch 670/1683, Loss: 3.1562\n  Média parcial até aqui: 3.2062\nBatch 671/1683, Loss: 3.0625\nBatch 672/1683, Loss: 3.0938\nBatch 673/1683, Loss: 3.1562\nBatch 674/1683, Loss: 3.6562\nBatch 675/1683, Loss: 3.2812\nBatch 676/1683, Loss: 3.6094\nBatch 677/1683, Loss: 3.6562\nBatch 678/1683, Loss: 2.8906\nBatch 679/1683, Loss: 3.3125\nBatch 680/1683, Loss: 3.1562\n  Média parcial até aqui: 3.2074\nBatch 681/1683, Loss: 2.9688\nBatch 682/1683, Loss: 3.3125\nBatch 683/1683, Loss: 3.1875\nBatch 684/1683, Loss: 3.3594\nBatch 685/1683, Loss: 3.3750\nBatch 686/1683, Loss: 3.0156\nBatch 687/1683, Loss: 3.2188\nBatch 688/1683, Loss: 3.4531\nBatch 689/1683, Loss: 2.9688\nBatch 690/1683, Loss: 2.9844\n  Média parcial até aqui: 3.2071\nBatch 691/1683, Loss: 3.0938\nBatch 692/1683, Loss: 3.3906\nBatch 693/1683, Loss: 2.8906\nBatch 694/1683, Loss: 3.5312\nBatch 695/1683, Loss: 2.9219\nBatch 696/1683, Loss: 2.9375\nBatch 697/1683, Loss: 3.5938\nBatch 698/1683, Loss: 3.2656\nBatch 699/1683, Loss: 2.9688\nBatch 700/1683, Loss: 3.2969\n  Média parcial até aqui: 3.2069\nBatch 701/1683, Loss: 3.7812\nBatch 702/1683, Loss: 2.7656\nBatch 703/1683, Loss: 2.9688\nBatch 704/1683, Loss: 3.1094\nBatch 705/1683, Loss: 3.0312\nBatch 706/1683, Loss: 3.3438\nBatch 707/1683, Loss: 3.1094\nBatch 708/1683, Loss: 3.0781\nBatch 709/1683, Loss: 3.3906\nBatch 710/1683, Loss: 3.0625\n  Média parcial até aqui: 3.2062\nBatch 711/1683, Loss: 3.3281\nBatch 712/1683, Loss: 3.8594\nBatch 713/1683, Loss: 3.0938\nBatch 714/1683, Loss: 3.0156\nBatch 715/1683, Loss: 3.3438\nBatch 716/1683, Loss: 2.9844\nBatch 717/1683, Loss: 3.1406\nBatch 718/1683, Loss: 3.1562\nBatch 719/1683, Loss: 3.2500\nBatch 720/1683, Loss: 3.2969\n  Média parcial até aqui: 3.2068\nBatch 721/1683, Loss: 3.0781\nBatch 722/1683, Loss: 3.4062\nBatch 723/1683, Loss: 3.2031\nBatch 724/1683, Loss: 3.0781\nBatch 725/1683, Loss: 3.2344\nBatch 726/1683, Loss: 3.2188\nBatch 727/1683, Loss: 2.9844\nBatch 728/1683, Loss: 3.6094\nBatch 729/1683, Loss: 3.4062\nBatch 730/1683, Loss: 3.2656\n  Média parcial até aqui: 3.2074\nBatch 731/1683, Loss: 3.4688\nBatch 732/1683, Loss: 3.1719\nBatch 733/1683, Loss: 3.0469\nBatch 734/1683, Loss: 2.6562\nBatch 735/1683, Loss: 3.2188\nBatch 736/1683, Loss: 3.1094\nBatch 737/1683, Loss: 3.4375\nBatch 738/1683, Loss: 3.3281\nBatch 739/1683, Loss: 3.1719\nBatch 740/1683, Loss: 3.3125\n  Média parcial até aqui: 3.2072\nBatch 741/1683, Loss: 3.3438\nBatch 742/1683, Loss: 3.1562\nBatch 743/1683, Loss: 3.0156\nBatch 744/1683, Loss: 3.2969\nBatch 745/1683, Loss: 2.6406\nBatch 746/1683, Loss: 3.3906\nBatch 747/1683, Loss: 3.1094\nBatch 748/1683, Loss: 3.3594\nBatch 749/1683, Loss: 3.0000\nBatch 750/1683, Loss: 3.3281\n  Média parcial até aqui: 3.2066\nBatch 751/1683, Loss: 3.4219\nBatch 752/1683, Loss: 3.2656\nBatch 753/1683, Loss: 2.8125\nBatch 754/1683, Loss: 3.2656\nBatch 755/1683, Loss: 3.1562\nBatch 756/1683, Loss: 2.9375\nBatch 757/1683, Loss: 3.1250\nBatch 758/1683, Loss: 3.2188\nBatch 759/1683, Loss: 3.2812\nBatch 760/1683, Loss: 2.9688\n  Média parcial até aqui: 3.2058\nBatch 761/1683, Loss: 3.2500\nBatch 762/1683, Loss: 3.5469\nBatch 763/1683, Loss: 3.2812\nBatch 764/1683, Loss: 3.0000\nBatch 765/1683, Loss: 3.4844\nBatch 766/1683, Loss: 3.2344\nBatch 767/1683, Loss: 3.6250\nBatch 768/1683, Loss: 3.2969\nBatch 769/1683, Loss: 3.6250\nBatch 770/1683, Loss: 3.1562\n  Média parcial até aqui: 3.2077\nBatch 771/1683, Loss: 2.9219\nBatch 772/1683, Loss: 3.1250\nBatch 773/1683, Loss: 3.3125\nBatch 774/1683, Loss: 3.2812\nBatch 775/1683, Loss: 3.1875\nBatch 776/1683, Loss: 3.1719\nBatch 777/1683, Loss: 3.5625\nBatch 778/1683, Loss: 3.3594\nBatch 779/1683, Loss: 3.1250\nBatch 780/1683, Loss: 3.1406\n  Média parcial até aqui: 3.2078\nBatch 781/1683, Loss: 3.2969\nBatch 782/1683, Loss: 2.9688\nBatch 783/1683, Loss: 3.1406\nBatch 784/1683, Loss: 2.8594\nBatch 785/1683, Loss: 3.1094\nBatch 786/1683, Loss: 3.6250\nBatch 787/1683, Loss: 3.3594\nBatch 788/1683, Loss: 3.4062\nBatch 789/1683, Loss: 3.2969\nBatch 790/1683, Loss: 3.3594\n  Média parcial até aqui: 3.2082\nBatch 791/1683, Loss: 3.2344\nBatch 792/1683, Loss: 3.3281\nBatch 793/1683, Loss: 3.0781\nBatch 794/1683, Loss: 3.6406\nBatch 795/1683, Loss: 3.8438\nBatch 796/1683, Loss: 2.7969\nBatch 797/1683, Loss: 2.9375\nBatch 798/1683, Loss: 3.0156\nBatch 799/1683, Loss: 3.0469\nBatch 800/1683, Loss: 3.3438\n  Média parcial até aqui: 3.2085\nBatch 801/1683, Loss: 3.1250\nBatch 802/1683, Loss: 3.0625\nBatch 803/1683, Loss: 2.7812\nBatch 804/1683, Loss: 3.4375\nBatch 805/1683, Loss: 3.1094\nBatch 806/1683, Loss: 3.0312\nBatch 807/1683, Loss: 3.2656\nBatch 808/1683, Loss: 3.7188\nBatch 809/1683, Loss: 3.7500\nBatch 810/1683, Loss: 3.7031\n  Média parcial até aqui: 3.2096\nBatch 811/1683, Loss: 3.0781\nBatch 812/1683, Loss: 3.7344\nBatch 813/1683, Loss: 3.3281\nBatch 814/1683, Loss: 2.8594\nBatch 815/1683, Loss: 3.0625\nBatch 816/1683, Loss: 3.1719\nBatch 817/1683, Loss: 3.2500\nBatch 818/1683, Loss: 2.8906\nBatch 819/1683, Loss: 3.5000\nBatch 820/1683, Loss: 3.1719\n  Média parcial até aqui: 3.2095\nBatch 821/1683, Loss: 3.3125\nBatch 822/1683, Loss: 3.3906\nBatch 823/1683, Loss: 3.2656\nBatch 824/1683, Loss: 3.4375\nBatch 825/1683, Loss: 3.4062\nBatch 826/1683, Loss: 3.0938\nBatch 827/1683, Loss: 3.5469\nBatch 828/1683, Loss: 3.0625\nBatch 829/1683, Loss: 3.5938\nBatch 830/1683, Loss: 3.3438\n  Média parcial até aqui: 3.2112\nBatch 831/1683, Loss: 3.0781\nBatch 832/1683, Loss: 2.9531\nBatch 833/1683, Loss: 3.5781\nBatch 834/1683, Loss: 2.7188\nBatch 835/1683, Loss: 2.9062\nBatch 836/1683, Loss: 3.0000\nBatch 837/1683, Loss: 3.0938\nBatch 838/1683, Loss: 3.5781\nBatch 839/1683, Loss: 3.0000\nBatch 840/1683, Loss: 3.1094\n  Média parcial até aqui: 3.2099\nBatch 841/1683, Loss: 3.0469\nBatch 842/1683, Loss: 3.4844\nBatch 843/1683, Loss: 3.3125\nBatch 844/1683, Loss: 3.1406\nBatch 845/1683, Loss: 3.1406\nBatch 846/1683, Loss: 3.4531\nBatch 847/1683, Loss: 3.2812\nBatch 848/1683, Loss: 3.4375\nBatch 849/1683, Loss: 3.1562\nBatch 850/1683, Loss: 2.9688\n  Média parcial até aqui: 3.2102\nBatch 851/1683, Loss: 3.3125\nBatch 852/1683, Loss: 3.2969\nBatch 853/1683, Loss: 3.5469\nBatch 854/1683, Loss: 3.4375\nBatch 855/1683, Loss: 3.2500\nBatch 856/1683, Loss: 3.0938\nBatch 857/1683, Loss: 3.1250\nBatch 858/1683, Loss: 3.2656\nBatch 859/1683, Loss: 2.9844\nBatch 860/1683, Loss: 3.4844\n  Média parcial até aqui: 3.2110\nBatch 861/1683, Loss: 3.5781\nBatch 862/1683, Loss: 2.8438\nBatch 863/1683, Loss: 3.1094\nBatch 864/1683, Loss: 3.4062\nBatch 865/1683, Loss: 3.0312\nBatch 866/1683, Loss: 3.7500\nBatch 867/1683, Loss: 2.7656\nBatch 868/1683, Loss: 3.2031\nBatch 869/1683, Loss: 3.4219\nBatch 870/1683, Loss: 3.0312\n  Média parcial até aqui: 3.2111\nBatch 871/1683, Loss: 3.1562\nBatch 872/1683, Loss: 3.1719\nBatch 873/1683, Loss: 3.2031\nBatch 874/1683, Loss: 3.0000\nBatch 875/1683, Loss: 3.4062\nBatch 876/1683, Loss: 2.7656\nBatch 877/1683, Loss: 2.9844\nBatch 878/1683, Loss: 3.1719\nBatch 879/1683, Loss: 3.0938\nBatch 880/1683, Loss: 2.8125\n  Média parcial até aqui: 3.2096\nBatch 881/1683, Loss: 2.8594\nBatch 882/1683, Loss: 3.1562\nBatch 883/1683, Loss: 3.4375\nBatch 884/1683, Loss: 3.2812\nBatch 885/1683, Loss: 3.1250\nBatch 886/1683, Loss: 2.7656\nBatch 887/1683, Loss: 3.3750\nBatch 888/1683, Loss: 3.5312\nBatch 889/1683, Loss: 2.9375\nBatch 890/1683, Loss: 3.2812\n  Média parcial até aqui: 3.2092\nBatch 891/1683, Loss: 3.0000\nBatch 892/1683, Loss: 2.7656\nBatch 893/1683, Loss: 2.7188\nBatch 894/1683, Loss: 4.0312\nBatch 895/1683, Loss: 3.1250\nBatch 896/1683, Loss: 3.2656\nBatch 897/1683, Loss: 3.2656\nBatch 898/1683, Loss: 3.5625\nBatch 899/1683, Loss: 3.1562\nBatch 900/1683, Loss: 3.3594\n  Média parcial até aqui: 3.2093\nBatch 901/1683, Loss: 3.2344\nBatch 902/1683, Loss: 3.2656\nBatch 903/1683, Loss: 3.5469\nBatch 904/1683, Loss: 3.0625\nBatch 905/1683, Loss: 3.1719\nBatch 906/1683, Loss: 3.1406\nBatch 907/1683, Loss: 3.5156\nBatch 908/1683, Loss: 3.1250\nBatch 909/1683, Loss: 3.0625\nBatch 910/1683, Loss: 3.7656\n  Média parcial até aqui: 3.2102\nBatch 911/1683, Loss: 3.1562\nBatch 912/1683, Loss: 3.0938\nBatch 913/1683, Loss: 2.9219\nBatch 914/1683, Loss: 2.9219\nBatch 915/1683, Loss: 3.2344\nBatch 916/1683, Loss: 3.2656\nBatch 917/1683, Loss: 3.1250\nBatch 918/1683, Loss: 3.0938\nBatch 919/1683, Loss: 3.0781\nBatch 920/1683, Loss: 3.2500\n  Média parcial até aqui: 3.2092\nBatch 921/1683, Loss: 3.2031\nBatch 922/1683, Loss: 3.3125\nBatch 923/1683, Loss: 2.7656\nBatch 924/1683, Loss: 2.9688\nBatch 925/1683, Loss: 3.2188\nBatch 926/1683, Loss: 3.0781\nBatch 927/1683, Loss: 3.2812\nBatch 928/1683, Loss: 3.2188\nBatch 929/1683, Loss: 3.4844\nBatch 930/1683, Loss: 3.1562\n  Média parcial até aqui: 3.2087\nBatch 931/1683, Loss: 3.4062\nBatch 932/1683, Loss: 3.6094\nBatch 933/1683, Loss: 3.2656\nBatch 934/1683, Loss: 3.2500\nBatch 935/1683, Loss: 3.2500\nBatch 936/1683, Loss: 3.3125\nBatch 937/1683, Loss: 3.5156\nBatch 938/1683, Loss: 3.0781\nBatch 939/1683, Loss: 3.2344\nBatch 940/1683, Loss: 3.5625\n  Média parcial até aqui: 3.2102\nBatch 941/1683, Loss: 3.4531\nBatch 942/1683, Loss: 3.0781\nBatch 943/1683, Loss: 3.1719\nBatch 944/1683, Loss: 2.9844\nBatch 945/1683, Loss: 3.3594\nBatch 946/1683, Loss: 2.9531\nBatch 947/1683, Loss: 3.0469\nBatch 948/1683, Loss: 2.8281\nBatch 949/1683, Loss: 3.3125\nBatch 950/1683, Loss: 3.5156\n  Média parcial até aqui: 3.2098\nBatch 951/1683, Loss: 3.4688\nBatch 952/1683, Loss: 3.2344\nBatch 953/1683, Loss: 3.1250\nBatch 954/1683, Loss: 3.3125\nBatch 955/1683, Loss: 3.6562\nBatch 956/1683, Loss: 3.2188\nBatch 957/1683, Loss: 3.2344\nBatch 958/1683, Loss: 3.8594\nBatch 959/1683, Loss: 3.0625\nBatch 960/1683, Loss: 2.9062\n  Média parcial até aqui: 3.2108\nBatch 961/1683, Loss: 3.6406\nBatch 962/1683, Loss: 3.3125\nBatch 963/1683, Loss: 3.1875\nBatch 964/1683, Loss: 2.9375\nBatch 965/1683, Loss: 3.2500\nBatch 966/1683, Loss: 3.5781\nBatch 967/1683, Loss: 2.8750\nBatch 968/1683, Loss: 3.2344\nBatch 969/1683, Loss: 2.7812\nBatch 970/1683, Loss: 3.0469\n  Média parcial até aqui: 3.2106\nBatch 971/1683, Loss: 3.5781\nBatch 972/1683, Loss: 3.0000\nBatch 973/1683, Loss: 2.9844\nBatch 974/1683, Loss: 3.1719\nBatch 975/1683, Loss: 3.4375\nBatch 976/1683, Loss: 3.2656\nBatch 977/1683, Loss: 3.1562\nBatch 978/1683, Loss: 3.2656\nBatch 979/1683, Loss: 3.2031\nBatch 980/1683, Loss: 3.5000\n  Média parcial até aqui: 3.2110\nBatch 981/1683, Loss: 3.5938\nBatch 982/1683, Loss: 3.5000\nBatch 983/1683, Loss: 3.2969\nBatch 984/1683, Loss: 3.1094\nBatch 985/1683, Loss: 2.9844\nBatch 986/1683, Loss: 3.0469\nBatch 987/1683, Loss: 3.8594\nBatch 988/1683, Loss: 3.1562\nBatch 989/1683, Loss: 3.2656\nBatch 990/1683, Loss: 3.4375\n  Média parcial até aqui: 3.2122\nBatch 991/1683, Loss: 3.5000\nBatch 992/1683, Loss: 3.3125\nBatch 993/1683, Loss: 3.1406\nBatch 994/1683, Loss: 2.9531\nBatch 995/1683, Loss: 2.8281\nBatch 996/1683, Loss: 3.3906\nBatch 997/1683, Loss: 3.1094\nBatch 998/1683, Loss: 3.2344\nBatch 999/1683, Loss: 3.3125\nBatch 1000/1683, Loss: 3.5781\n  Média parcial até aqui: 3.2124\nBatch 1001/1683, Loss: 2.9844\nBatch 1002/1683, Loss: 3.2656\nBatch 1003/1683, Loss: 2.8750\nBatch 1004/1683, Loss: 3.2969\nBatch 1005/1683, Loss: 3.4219\nBatch 1006/1683, Loss: 3.1562\nBatch 1007/1683, Loss: 2.8750\nBatch 1008/1683, Loss: 2.9531\nBatch 1009/1683, Loss: 3.4062\nBatch 1010/1683, Loss: 3.2188\n  Média parcial até aqui: 3.2117\nBatch 1011/1683, Loss: 3.0781\nBatch 1012/1683, Loss: 2.9531\nBatch 1013/1683, Loss: 3.0312\nBatch 1014/1683, Loss: 2.8906\nBatch 1015/1683, Loss: 2.8125\nBatch 1016/1683, Loss: 2.9688\nBatch 1017/1683, Loss: 3.2031\nBatch 1018/1683, Loss: 3.0000\nBatch 1019/1683, Loss: 3.0156\nBatch 1020/1683, Loss: 2.9531\n  Média parcial até aqui: 3.2096\nBatch 1021/1683, Loss: 2.9844\nBatch 1022/1683, Loss: 3.2969\nBatch 1023/1683, Loss: 3.0156\nBatch 1024/1683, Loss: 3.0312\nBatch 1025/1683, Loss: 2.6406\nBatch 1026/1683, Loss: 2.9531\nBatch 1027/1683, Loss: 3.4062\nBatch 1028/1683, Loss: 3.0938\nBatch 1029/1683, Loss: 3.2500\nBatch 1030/1683, Loss: 3.1719\n  Média parcial até aqui: 3.2084\nBatch 1031/1683, Loss: 3.1719\nBatch 1032/1683, Loss: 3.0469\nBatch 1033/1683, Loss: 2.7969\nBatch 1034/1683, Loss: 2.7344\nBatch 1035/1683, Loss: 3.8281\nBatch 1036/1683, Loss: 3.2656\nBatch 1037/1683, Loss: 3.0781\nBatch 1038/1683, Loss: 3.4062\nBatch 1039/1683, Loss: 3.0781\nBatch 1040/1683, Loss: 3.1719\n  Média parcial até aqui: 3.2079\nBatch 1041/1683, Loss: 3.3125\nBatch 1042/1683, Loss: 2.8438\nBatch 1043/1683, Loss: 3.2031\nBatch 1044/1683, Loss: 3.2344\nBatch 1045/1683, Loss: 3.5469\nBatch 1046/1683, Loss: 2.9531\nBatch 1047/1683, Loss: 3.2812\nBatch 1048/1683, Loss: 3.1875\nBatch 1049/1683, Loss: 3.1094\nBatch 1050/1683, Loss: 3.5938\n  Média parcial até aqui: 3.2081\nBatch 1051/1683, Loss: 3.3594\nBatch 1052/1683, Loss: 3.1719\nBatch 1053/1683, Loss: 3.6250\nBatch 1054/1683, Loss: 3.1719\nBatch 1055/1683, Loss: 3.0469\nBatch 1056/1683, Loss: 3.2812\nBatch 1057/1683, Loss: 3.1875\nBatch 1058/1683, Loss: 3.4844\nBatch 1059/1683, Loss: 3.0781\nBatch 1060/1683, Loss: 3.2812\n  Média parcial até aqui: 3.2086\nBatch 1061/1683, Loss: 3.0000\nBatch 1062/1683, Loss: 3.1562\nBatch 1063/1683, Loss: 3.4688\nBatch 1064/1683, Loss: 3.3438\nBatch 1065/1683, Loss: 3.0625\nBatch 1066/1683, Loss: 3.2500\nBatch 1067/1683, Loss: 3.7031\nBatch 1068/1683, Loss: 3.2031\nBatch 1069/1683, Loss: 3.4531\nBatch 1070/1683, Loss: 3.1875\n  Média parcial até aqui: 3.2093\nBatch 1071/1683, Loss: 3.0938\nBatch 1072/1683, Loss: 2.9844\nBatch 1073/1683, Loss: 3.2812\nBatch 1074/1683, Loss: 2.8281\nBatch 1075/1683, Loss: 3.4531\nBatch 1076/1683, Loss: 3.1406\nBatch 1077/1683, Loss: 2.8594\nBatch 1078/1683, Loss: 3.2031\nBatch 1079/1683, Loss: 3.2812\nBatch 1080/1683, Loss: 3.7344\n  Média parcial até aqui: 3.2091\nBatch 1081/1683, Loss: 3.2344\nBatch 1082/1683, Loss: 3.6406\nBatch 1083/1683, Loss: 3.3438\nBatch 1084/1683, Loss: 3.0156\nBatch 1085/1683, Loss: 3.4531\nBatch 1086/1683, Loss: 3.2812\nBatch 1087/1683, Loss: 3.0625\nBatch 1088/1683, Loss: 2.7656\nBatch 1089/1683, Loss: 3.3906\nBatch 1090/1683, Loss: 3.4844\n  Média parcial até aqui: 3.2096\nBatch 1091/1683, Loss: 3.2969\nBatch 1092/1683, Loss: 3.1406\nBatch 1093/1683, Loss: 3.5156\nBatch 1094/1683, Loss: 3.2812\nBatch 1095/1683, Loss: 3.0938\nBatch 1096/1683, Loss: 2.9219\nBatch 1097/1683, Loss: 3.5938\nBatch 1098/1683, Loss: 3.3281\nBatch 1099/1683, Loss: 2.9219\nBatch 1100/1683, Loss: 2.9062\n  Média parcial até aqui: 3.2095\nBatch 1101/1683, Loss: 3.0156\nBatch 1102/1683, Loss: 3.3750\nBatch 1103/1683, Loss: 3.2812\nBatch 1104/1683, Loss: 3.1406\nBatch 1105/1683, Loss: 2.9844\nBatch 1106/1683, Loss: 2.8906\nBatch 1107/1683, Loss: 3.3281\nBatch 1108/1683, Loss: 3.0469\nBatch 1109/1683, Loss: 3.7500\nBatch 1110/1683, Loss: 3.5781\n  Média parcial até aqui: 3.2098\nBatch 1111/1683, Loss: 3.1406\nBatch 1112/1683, Loss: 2.7188\nBatch 1113/1683, Loss: 3.3125\nBatch 1114/1683, Loss: 3.5156\nBatch 1115/1683, Loss: 3.5469\nBatch 1116/1683, Loss: 3.2188\nBatch 1117/1683, Loss: 3.2656\nBatch 1118/1683, Loss: 3.4844\nBatch 1119/1683, Loss: 3.2031\nBatch 1120/1683, Loss: 3.0469\n  Média parcial até aqui: 3.2101\nBatch 1121/1683, Loss: 3.3594\nBatch 1122/1683, Loss: 3.2969\nBatch 1123/1683, Loss: 3.5625\nBatch 1124/1683, Loss: 3.0625\nBatch 1125/1683, Loss: 3.0625\nBatch 1126/1683, Loss: 3.1719\nBatch 1127/1683, Loss: 3.3438\nBatch 1128/1683, Loss: 3.4844\nBatch 1129/1683, Loss: 3.1562\nBatch 1130/1683, Loss: 3.2344\n  Média parcial até aqui: 3.2107\nBatch 1131/1683, Loss: 3.3906\nBatch 1132/1683, Loss: 2.9531\nBatch 1133/1683, Loss: 3.4688\nBatch 1134/1683, Loss: 2.8438\nBatch 1135/1683, Loss: 3.2031\nBatch 1136/1683, Loss: 3.2188\nBatch 1137/1683, Loss: 3.3125\nBatch 1138/1683, Loss: 3.4844\nBatch 1139/1683, Loss: 3.1562\nBatch 1140/1683, Loss: 2.7656\n  Média parcial até aqui: 3.2104\nBatch 1141/1683, Loss: 2.8594\nBatch 1142/1683, Loss: 3.3125\nBatch 1143/1683, Loss: 2.9219\nBatch 1144/1683, Loss: 3.3438\nBatch 1145/1683, Loss: 3.1562\nBatch 1146/1683, Loss: 3.4688\nBatch 1147/1683, Loss: 3.2031\nBatch 1148/1683, Loss: 3.3281\nBatch 1149/1683, Loss: 3.0938\nBatch 1150/1683, Loss: 3.2031\n  Média parcial até aqui: 3.2102\nBatch 1151/1683, Loss: 3.2500\nBatch 1152/1683, Loss: 3.2812\nBatch 1153/1683, Loss: 3.1562\nBatch 1154/1683, Loss: 3.1406\nBatch 1155/1683, Loss: 3.0156\nBatch 1156/1683, Loss: 3.5625\nBatch 1157/1683, Loss: 3.1094\nBatch 1158/1683, Loss: 3.1094\nBatch 1159/1683, Loss: 3.2344\nBatch 1160/1683, Loss: 3.3438\n  Média parcial até aqui: 3.2103\nBatch 1161/1683, Loss: 2.7812\nBatch 1162/1683, Loss: 2.8906\nBatch 1163/1683, Loss: 3.5469\nBatch 1164/1683, Loss: 3.0000\nBatch 1165/1683, Loss: 3.0000\nBatch 1166/1683, Loss: 3.3906\nBatch 1167/1683, Loss: 3.1250\nBatch 1168/1683, Loss: 3.0938\nBatch 1169/1683, Loss: 3.2344\nBatch 1170/1683, Loss: 3.0000\n  Média parcial até aqui: 3.2094\nBatch 1171/1683, Loss: 2.9688\nBatch 1172/1683, Loss: 3.4688\nBatch 1173/1683, Loss: 3.0938\nBatch 1174/1683, Loss: 3.1562\nBatch 1175/1683, Loss: 3.1094\nBatch 1176/1683, Loss: 2.9375\nBatch 1177/1683, Loss: 3.5938\nBatch 1178/1683, Loss: 3.6875\nBatch 1179/1683, Loss: 3.0000\nBatch 1180/1683, Loss: 2.9219\n  Média parcial até aqui: 3.2093\nBatch 1181/1683, Loss: 3.0000\nBatch 1182/1683, Loss: 3.3750\nBatch 1183/1683, Loss: 2.8438\nBatch 1184/1683, Loss: 2.9844\nBatch 1185/1683, Loss: 3.2500\nBatch 1186/1683, Loss: 3.0625\nBatch 1187/1683, Loss: 3.1719\nBatch 1188/1683, Loss: 3.4062\nBatch 1189/1683, Loss: 3.0469\nBatch 1190/1683, Loss: 3.0469\n  Média parcial até aqui: 3.2085\nBatch 1191/1683, Loss: 3.2344\nBatch 1192/1683, Loss: 3.3438\nBatch 1193/1683, Loss: 3.1250\nBatch 1194/1683, Loss: 3.4375\nBatch 1195/1683, Loss: 3.2344\nBatch 1196/1683, Loss: 3.1250\nBatch 1197/1683, Loss: 3.2344\nBatch 1198/1683, Loss: 3.3906\nBatch 1199/1683, Loss: 3.3594\nBatch 1200/1683, Loss: 3.2031\n  Média parcial até aqui: 3.2090\nBatch 1201/1683, Loss: 3.2500\nBatch 1202/1683, Loss: 3.0156\nBatch 1203/1683, Loss: 3.1562\nBatch 1204/1683, Loss: 3.1719\nBatch 1205/1683, Loss: 3.5469\nBatch 1206/1683, Loss: 3.3594\nBatch 1207/1683, Loss: 2.8594\nBatch 1208/1683, Loss: 3.1094\nBatch 1209/1683, Loss: 3.3594\nBatch 1210/1683, Loss: 2.9219\n  Média parcial até aqui: 3.2088\nBatch 1211/1683, Loss: 3.2031\nBatch 1212/1683, Loss: 3.2969\nBatch 1213/1683, Loss: 3.6875\nBatch 1214/1683, Loss: 3.0000\nBatch 1215/1683, Loss: 2.7969\nBatch 1216/1683, Loss: 3.4375\nBatch 1217/1683, Loss: 2.9531\nBatch 1218/1683, Loss: 3.8125\nBatch 1219/1683, Loss: 3.4688\nBatch 1220/1683, Loss: 3.0781\n  Média parcial até aqui: 3.2093\nBatch 1221/1683, Loss: 3.1250\nBatch 1222/1683, Loss: 3.3125\nBatch 1223/1683, Loss: 3.2344\nBatch 1224/1683, Loss: 3.2812\nBatch 1225/1683, Loss: 3.2812\nBatch 1226/1683, Loss: 3.3594\nBatch 1227/1683, Loss: 3.2969\nBatch 1228/1683, Loss: 3.0625\nBatch 1229/1683, Loss: 2.7188\nBatch 1230/1683, Loss: 2.9844\n  Média parcial até aqui: 3.2089\nBatch 1231/1683, Loss: 3.5781\nBatch 1232/1683, Loss: 2.9688\nBatch 1233/1683, Loss: 3.7656\nBatch 1234/1683, Loss: 2.6094\nBatch 1235/1683, Loss: 3.0469\nBatch 1236/1683, Loss: 3.1562\nBatch 1237/1683, Loss: 4.0625\nBatch 1238/1683, Loss: 3.2969\nBatch 1239/1683, Loss: 3.5312\nBatch 1240/1683, Loss: 3.0625\n  Média parcial até aqui: 3.2097\nBatch 1241/1683, Loss: 3.4531\nBatch 1242/1683, Loss: 2.9844\nBatch 1243/1683, Loss: 3.0781\nBatch 1244/1683, Loss: 2.9531\nBatch 1245/1683, Loss: 3.1719\nBatch 1246/1683, Loss: 3.0156\nBatch 1247/1683, Loss: 3.6250\nBatch 1248/1683, Loss: 3.8906\nBatch 1249/1683, Loss: 3.1562\nBatch 1250/1683, Loss: 3.3594\n  Média parcial até aqui: 3.2102\nBatch 1251/1683, Loss: 3.1250\nBatch 1252/1683, Loss: 2.9219\nBatch 1253/1683, Loss: 3.9062\nBatch 1254/1683, Loss: 3.1406\nBatch 1255/1683, Loss: 3.2500\nBatch 1256/1683, Loss: 3.0938\nBatch 1257/1683, Loss: 3.1406\nBatch 1258/1683, Loss: 2.8906\nBatch 1259/1683, Loss: 3.2031\nBatch 1260/1683, Loss: 3.2344\n  Média parcial até aqui: 3.2100\nBatch 1261/1683, Loss: 3.0469\nBatch 1262/1683, Loss: 3.2500\nBatch 1263/1683, Loss: 2.9844\nBatch 1264/1683, Loss: 2.9688\nBatch 1265/1683, Loss: 3.2344\nBatch 1266/1683, Loss: 3.0312\nBatch 1267/1683, Loss: 3.5469\nBatch 1268/1683, Loss: 3.1875\nBatch 1269/1683, Loss: 3.0469\nBatch 1270/1683, Loss: 3.9375\n  Média parcial até aqui: 3.2102\nBatch 1271/1683, Loss: 2.9062\nBatch 1272/1683, Loss: 3.6719\nBatch 1273/1683, Loss: 3.1719\nBatch 1274/1683, Loss: 3.0781\nBatch 1275/1683, Loss: 2.3750\nBatch 1276/1683, Loss: 2.6875\nBatch 1277/1683, Loss: 2.9531\nBatch 1278/1683, Loss: 3.3125\nBatch 1279/1683, Loss: 3.1875\nBatch 1280/1683, Loss: 3.3281\n  Média parcial até aqui: 3.2090\nBatch 1281/1683, Loss: 3.7188\nBatch 1282/1683, Loss: 3.4375\nBatch 1283/1683, Loss: 3.1094\nBatch 1284/1683, Loss: 3.1562\nBatch 1285/1683, Loss: 3.0469\nBatch 1286/1683, Loss: 3.1250\nBatch 1287/1683, Loss: 2.9531\nBatch 1288/1683, Loss: 3.2969\nBatch 1289/1683, Loss: 3.1719\nBatch 1290/1683, Loss: 3.3281\n  Média parcial até aqui: 3.2092\nBatch 1291/1683, Loss: 2.7344\nBatch 1292/1683, Loss: 3.3438\nBatch 1293/1683, Loss: 3.6250\nBatch 1294/1683, Loss: 2.9688\nBatch 1295/1683, Loss: 2.7969\nBatch 1296/1683, Loss: 3.1094\nBatch 1297/1683, Loss: 3.1875\nBatch 1298/1683, Loss: 3.4219\nBatch 1299/1683, Loss: 3.3438\nBatch 1300/1683, Loss: 3.5000\n  Média parcial até aqui: 3.2092\nBatch 1301/1683, Loss: 3.0625\nBatch 1302/1683, Loss: 3.2188\nBatch 1303/1683, Loss: 3.1562\nBatch 1304/1683, Loss: 3.2969\nBatch 1305/1683, Loss: 3.2812\nBatch 1306/1683, Loss: 3.8438\nBatch 1307/1683, Loss: 3.0000\nBatch 1308/1683, Loss: 3.1875\nBatch 1309/1683, Loss: 3.3906\nBatch 1310/1683, Loss: 3.2188\n  Média parcial até aqui: 3.2096\nBatch 1311/1683, Loss: 3.4688\nBatch 1312/1683, Loss: 3.5625\nBatch 1313/1683, Loss: 2.9375\nBatch 1314/1683, Loss: 3.1875\nBatch 1315/1683, Loss: 2.7812\nBatch 1316/1683, Loss: 3.0000\nBatch 1317/1683, Loss: 3.1719\nBatch 1318/1683, Loss: 2.5312\nBatch 1319/1683, Loss: 2.7188\nBatch 1320/1683, Loss: 3.2031\n  Média parcial até aqui: 3.2085\nBatch 1321/1683, Loss: 3.1875\nBatch 1322/1683, Loss: 3.2812\nBatch 1323/1683, Loss: 3.3750\nBatch 1324/1683, Loss: 3.2969\nBatch 1325/1683, Loss: 3.4219\nBatch 1326/1683, Loss: 3.2344\nBatch 1327/1683, Loss: 3.1094\nBatch 1328/1683, Loss: 2.8906\nBatch 1329/1683, Loss: 3.1406\nBatch 1330/1683, Loss: 3.8438\n  Média parcial até aqui: 3.2090\nBatch 1331/1683, Loss: 3.2188\nBatch 1332/1683, Loss: 3.3438\nBatch 1333/1683, Loss: 3.2188\nBatch 1334/1683, Loss: 2.8125\nBatch 1335/1683, Loss: 3.1719\nBatch 1336/1683, Loss: 3.0781\nBatch 1337/1683, Loss: 2.9688\nBatch 1338/1683, Loss: 3.2656\nBatch 1339/1683, Loss: 3.1250\nBatch 1340/1683, Loss: 3.0781\n  Média parcial até aqui: 3.2084\nBatch 1341/1683, Loss: 3.0469\nBatch 1342/1683, Loss: 2.8906\nBatch 1343/1683, Loss: 3.1875\nBatch 1344/1683, Loss: 2.9844\nBatch 1345/1683, Loss: 3.0469\nBatch 1346/1683, Loss: 3.2656\nBatch 1347/1683, Loss: 3.4531\nBatch 1348/1683, Loss: 2.9219\nBatch 1349/1683, Loss: 4.0000\nBatch 1350/1683, Loss: 3.3438\n  Média parcial até aqui: 3.2084\nBatch 1351/1683, Loss: 3.0312\nBatch 1352/1683, Loss: 3.1250\nBatch 1353/1683, Loss: 3.2812\nBatch 1354/1683, Loss: 3.3125\nBatch 1355/1683, Loss: 3.1250\nBatch 1356/1683, Loss: 3.0156\nBatch 1357/1683, Loss: 3.1875\nBatch 1358/1683, Loss: 3.7031\nBatch 1359/1683, Loss: 3.2500\nBatch 1360/1683, Loss: 3.1406\n  Média parcial até aqui: 3.2085\nBatch 1361/1683, Loss: 3.6562\nBatch 1362/1683, Loss: 3.5312\nBatch 1363/1683, Loss: 3.0938\nBatch 1364/1683, Loss: 2.8906\nBatch 1365/1683, Loss: 2.9688\nBatch 1366/1683, Loss: 3.5156\nBatch 1367/1683, Loss: 3.3594\nBatch 1368/1683, Loss: 3.1094\nBatch 1369/1683, Loss: 3.0156\nBatch 1370/1683, Loss: 3.0625\n  Média parcial até aqui: 3.2086\nBatch 1371/1683, Loss: 3.1562\nBatch 1372/1683, Loss: 3.6406\nBatch 1373/1683, Loss: 3.3594\nBatch 1374/1683, Loss: 3.0625\nBatch 1375/1683, Loss: 3.1406\nBatch 1376/1683, Loss: 3.3750\nBatch 1377/1683, Loss: 3.2812\nBatch 1378/1683, Loss: 3.0312\nBatch 1379/1683, Loss: 3.1094\nBatch 1380/1683, Loss: 3.6406\n  Média parcial até aqui: 3.2091\nBatch 1381/1683, Loss: 3.2656\nBatch 1382/1683, Loss: 3.2969\nBatch 1383/1683, Loss: 3.0781\nBatch 1384/1683, Loss: 3.0469\nBatch 1385/1683, Loss: 3.2031\nBatch 1386/1683, Loss: 3.6094\nBatch 1387/1683, Loss: 3.3281\nBatch 1388/1683, Loss: 3.0625\nBatch 1389/1683, Loss: 2.9844\nBatch 1390/1683, Loss: 2.9531\n  Média parcial até aqui: 3.2089\nBatch 1391/1683, Loss: 3.0312\nBatch 1392/1683, Loss: 3.2031\nBatch 1393/1683, Loss: 3.3750\nBatch 1394/1683, Loss: 2.9688\nBatch 1395/1683, Loss: 3.3594\nBatch 1396/1683, Loss: 3.1875\nBatch 1397/1683, Loss: 3.0469\nBatch 1398/1683, Loss: 3.0156\nBatch 1399/1683, Loss: 3.1875\nBatch 1400/1683, Loss: 3.0625\n  Média parcial até aqui: 3.2084\nBatch 1401/1683, Loss: 3.2969\nBatch 1402/1683, Loss: 3.0938\nBatch 1403/1683, Loss: 3.1094\nBatch 1404/1683, Loss: 3.4531\nBatch 1405/1683, Loss: 3.2031\nBatch 1406/1683, Loss: 2.9531\nBatch 1407/1683, Loss: 3.6406\nBatch 1408/1683, Loss: 3.3594\nBatch 1409/1683, Loss: 3.9531\nBatch 1410/1683, Loss: 3.2812\n  Média parcial até aqui: 3.2093\nBatch 1411/1683, Loss: 3.1562\nBatch 1412/1683, Loss: 2.5469\nBatch 1413/1683, Loss: 3.2969\nBatch 1414/1683, Loss: 3.3125\nBatch 1415/1683, Loss: 2.9062\nBatch 1416/1683, Loss: 2.8125\nBatch 1417/1683, Loss: 3.4688\nBatch 1418/1683, Loss: 3.4062\nBatch 1419/1683, Loss: 2.7500\nBatch 1420/1683, Loss: 3.1094\n  Média parcial até aqui: 3.2084\nBatch 1421/1683, Loss: 3.2656\nBatch 1422/1683, Loss: 3.6094\nBatch 1423/1683, Loss: 3.2812\nBatch 1424/1683, Loss: 3.2031\nBatch 1425/1683, Loss: 3.3750\nBatch 1426/1683, Loss: 3.3438\nBatch 1427/1683, Loss: 3.1094\nBatch 1428/1683, Loss: 3.0469\nBatch 1429/1683, Loss: 3.0156\nBatch 1430/1683, Loss: 3.5312\n  Média parcial até aqui: 3.2089\nBatch 1431/1683, Loss: 3.1562\nBatch 1432/1683, Loss: 3.0781\nBatch 1433/1683, Loss: 3.1094\nBatch 1434/1683, Loss: 3.0312\nBatch 1435/1683, Loss: 3.2500\nBatch 1436/1683, Loss: 3.2344\nBatch 1437/1683, Loss: 3.3125\nBatch 1438/1683, Loss: 3.0156\nBatch 1439/1683, Loss: 3.1719\nBatch 1440/1683, Loss: 3.2031\n  Média parcial até aqui: 3.2085\nBatch 1441/1683, Loss: 2.9375\nBatch 1442/1683, Loss: 3.3906\nBatch 1443/1683, Loss: 3.3281\nBatch 1444/1683, Loss: 3.2031\nBatch 1445/1683, Loss: 3.1875\nBatch 1446/1683, Loss: 3.2188\nBatch 1447/1683, Loss: 2.9375\nBatch 1448/1683, Loss: 3.3750\nBatch 1449/1683, Loss: 3.0625\nBatch 1450/1683, Loss: 3.2812\n  Média parcial até aqui: 3.2084\nBatch 1451/1683, Loss: 3.1094\nBatch 1452/1683, Loss: 3.0938\nBatch 1453/1683, Loss: 3.7969\nBatch 1454/1683, Loss: 3.1875\nBatch 1455/1683, Loss: 2.9531\nBatch 1456/1683, Loss: 3.2656\nBatch 1457/1683, Loss: 2.9531\nBatch 1458/1683, Loss: 3.5312\nBatch 1459/1683, Loss: 3.3281\nBatch 1460/1683, Loss: 3.1250\n  Média parcial até aqui: 3.2086\nBatch 1461/1683, Loss: 3.2188\nBatch 1462/1683, Loss: 3.4375\nBatch 1463/1683, Loss: 3.0781\nBatch 1464/1683, Loss: 3.3594\nBatch 1465/1683, Loss: 3.4531\nBatch 1466/1683, Loss: 3.3438\nBatch 1467/1683, Loss: 3.1094\nBatch 1468/1683, Loss: 2.8438\nBatch 1469/1683, Loss: 3.0625\nBatch 1470/1683, Loss: 3.1250\n  Média parcial até aqui: 3.2085\nBatch 1471/1683, Loss: 2.6719\nBatch 1472/1683, Loss: 2.8750\nBatch 1473/1683, Loss: 3.3281\nBatch 1474/1683, Loss: 3.1406\nBatch 1475/1683, Loss: 3.6406\nBatch 1476/1683, Loss: 3.1562\nBatch 1477/1683, Loss: 3.1094\nBatch 1478/1683, Loss: 3.0156\nBatch 1479/1683, Loss: 3.1094\nBatch 1480/1683, Loss: 4.0625\n  Média parcial até aqui: 3.2086\nBatch 1481/1683, Loss: 3.5156\nBatch 1482/1683, Loss: 3.0000\nBatch 1483/1683, Loss: 3.0000\nBatch 1484/1683, Loss: 3.3750\nBatch 1485/1683, Loss: 3.2969\nBatch 1486/1683, Loss: 3.1406\nBatch 1487/1683, Loss: 3.1875\nBatch 1488/1683, Loss: 3.0000\nBatch 1489/1683, Loss: 3.2031\nBatch 1490/1683, Loss: 3.4844\n  Média parcial até aqui: 3.2086\nBatch 1491/1683, Loss: 2.9531\nBatch 1492/1683, Loss: 3.6875\nBatch 1493/1683, Loss: 3.2656\nBatch 1494/1683, Loss: 4.0938\nBatch 1495/1683, Loss: 3.1562\nBatch 1496/1683, Loss: 3.4844\nBatch 1497/1683, Loss: 3.4219\nBatch 1498/1683, Loss: 3.4531\nBatch 1499/1683, Loss: 2.9375\nBatch 1500/1683, Loss: 3.1719\n  Média parcial até aqui: 3.2097\nBatch 1501/1683, Loss: 3.0469\nBatch 1502/1683, Loss: 3.1562\nBatch 1503/1683, Loss: 3.3438\nBatch 1504/1683, Loss: 3.5469\nBatch 1505/1683, Loss: 3.1406\nBatch 1506/1683, Loss: 3.2031\nBatch 1507/1683, Loss: 3.1719\nBatch 1508/1683, Loss: 3.2500\nBatch 1509/1683, Loss: 3.2812\nBatch 1510/1683, Loss: 3.3125\n  Média parcial até aqui: 3.2099\nBatch 1511/1683, Loss: 3.1406\nBatch 1512/1683, Loss: 3.1562\nBatch 1513/1683, Loss: 3.2500\nBatch 1514/1683, Loss: 3.3281\nBatch 1515/1683, Loss: 3.0938\nBatch 1516/1683, Loss: 3.5625\nBatch 1517/1683, Loss: 3.5000\nBatch 1518/1683, Loss: 3.0625\nBatch 1519/1683, Loss: 2.6562\nBatch 1520/1683, Loss: 2.6094\n  Média parcial até aqui: 3.2094\nBatch 1521/1683, Loss: 3.1406\nBatch 1522/1683, Loss: 3.2188\nBatch 1523/1683, Loss: 2.9531\nBatch 1524/1683, Loss: 3.2031\nBatch 1525/1683, Loss: 3.6094\nBatch 1526/1683, Loss: 3.1406\nBatch 1527/1683, Loss: 2.9375\nBatch 1528/1683, Loss: 3.0469\nBatch 1529/1683, Loss: 2.8906\nBatch 1530/1683, Loss: 3.4531\n  Média parcial até aqui: 3.2091\nBatch 1531/1683, Loss: 3.2344\nBatch 1532/1683, Loss: 3.4844\nBatch 1533/1683, Loss: 3.0781\nBatch 1534/1683, Loss: 3.1250\nBatch 1535/1683, Loss: 3.4688\nBatch 1536/1683, Loss: 3.3750\nBatch 1537/1683, Loss: 2.9844\nBatch 1538/1683, Loss: 3.0938\nBatch 1539/1683, Loss: 3.1719\nBatch 1540/1683, Loss: 3.2188\n  Média parcial até aqui: 3.2092\nBatch 1541/1683, Loss: 3.0156\nBatch 1542/1683, Loss: 3.0312\nBatch 1543/1683, Loss: 3.3281\nBatch 1544/1683, Loss: 3.0312\nBatch 1545/1683, Loss: 3.8594\nBatch 1546/1683, Loss: 3.2656\nBatch 1547/1683, Loss: 2.8594\nBatch 1548/1683, Loss: 3.0312\nBatch 1549/1683, Loss: 3.2969\nBatch 1550/1683, Loss: 3.0469\n  Média parcial até aqui: 3.2090\nBatch 1551/1683, Loss: 3.8125\nBatch 1552/1683, Loss: 3.5156\nBatch 1553/1683, Loss: 3.8438\nBatch 1554/1683, Loss: 2.8125\nBatch 1555/1683, Loss: 2.7344\nBatch 1556/1683, Loss: 3.1406\nBatch 1557/1683, Loss: 2.9219\nBatch 1558/1683, Loss: 3.1094\nBatch 1559/1683, Loss: 3.4844\nBatch 1560/1683, Loss: 3.1562\n  Média parcial até aqui: 3.2092\nBatch 1561/1683, Loss: 3.4219\nBatch 1562/1683, Loss: 3.1250\nBatch 1563/1683, Loss: 3.5000\nBatch 1564/1683, Loss: 2.9219\nBatch 1565/1683, Loss: 3.8594\nBatch 1566/1683, Loss: 3.1562\nBatch 1567/1683, Loss: 3.3281\nBatch 1568/1683, Loss: 3.1875\nBatch 1569/1683, Loss: 3.0938\nBatch 1570/1683, Loss: 3.1250\n  Média parcial até aqui: 3.2096\nBatch 1571/1683, Loss: 3.5156\nBatch 1572/1683, Loss: 3.1406\nBatch 1573/1683, Loss: 3.3125\nBatch 1574/1683, Loss: 3.2500\nBatch 1575/1683, Loss: 3.2969\nBatch 1576/1683, Loss: 3.5156\nBatch 1577/1683, Loss: 3.8438\nBatch 1578/1683, Loss: 3.1250\nBatch 1579/1683, Loss: 3.0312\nBatch 1580/1683, Loss: 3.2500\n  Média parcial até aqui: 3.2104\nBatch 1581/1683, Loss: 3.0156\nBatch 1582/1683, Loss: 3.2031\nBatch 1583/1683, Loss: 2.9688\nBatch 1584/1683, Loss: 3.2031\nBatch 1585/1683, Loss: 2.9062\nBatch 1586/1683, Loss: 3.2969\nBatch 1587/1683, Loss: 3.5469\nBatch 1588/1683, Loss: 3.2812\nBatch 1589/1683, Loss: 3.0469\nBatch 1590/1683, Loss: 3.0469\n  Média parcial até aqui: 3.2100\nBatch 1591/1683, Loss: 3.1094\nBatch 1592/1683, Loss: 3.1250\nBatch 1593/1683, Loss: 3.6875\nBatch 1594/1683, Loss: 3.3125\nBatch 1595/1683, Loss: 3.4531\nBatch 1596/1683, Loss: 3.4844\nBatch 1597/1683, Loss: 3.1250\nBatch 1598/1683, Loss: 3.2656\nBatch 1599/1683, Loss: 3.5000\nBatch 1600/1683, Loss: 3.2969\n  Média parcial até aqui: 3.2108\nBatch 1601/1683, Loss: 3.3281\nBatch 1602/1683, Loss: 3.6094\nBatch 1603/1683, Loss: 2.8750\nBatch 1604/1683, Loss: 2.9844\nBatch 1605/1683, Loss: 3.4062\nBatch 1606/1683, Loss: 2.8906\nBatch 1607/1683, Loss: 3.3281\nBatch 1608/1683, Loss: 3.2031\nBatch 1609/1683, Loss: 3.2656\nBatch 1610/1683, Loss: 3.4062\n  Média parcial até aqui: 3.2109\nBatch 1611/1683, Loss: 3.4219\nBatch 1612/1683, Loss: 3.1562\nBatch 1613/1683, Loss: 3.1719\nBatch 1614/1683, Loss: 3.0781\nBatch 1615/1683, Loss: 3.2812\nBatch 1616/1683, Loss: 3.5625\nBatch 1617/1683, Loss: 3.0469\nBatch 1618/1683, Loss: 3.2031\nBatch 1619/1683, Loss: 3.4531\nBatch 1620/1683, Loss: 3.0781\n  Média parcial até aqui: 3.2111\nBatch 1621/1683, Loss: 2.8594\nBatch 1622/1683, Loss: 2.9688\nBatch 1623/1683, Loss: 3.4062\nBatch 1624/1683, Loss: 3.1562\nBatch 1625/1683, Loss: 3.1719\nBatch 1626/1683, Loss: 3.3125\nBatch 1627/1683, Loss: 3.3438\nBatch 1628/1683, Loss: 3.3281\nBatch 1629/1683, Loss: 2.9375\nBatch 1630/1683, Loss: 4.4688\n  Média parcial até aqui: 3.2117\nBatch 1631/1683, Loss: 3.2969\nBatch 1632/1683, Loss: 3.2344\nBatch 1633/1683, Loss: 3.5000\nBatch 1634/1683, Loss: 3.1406\nBatch 1635/1683, Loss: 3.5312\nBatch 1636/1683, Loss: 3.1250\nBatch 1637/1683, Loss: 3.0000\nBatch 1638/1683, Loss: 3.5625\nBatch 1639/1683, Loss: 3.2344\nBatch 1640/1683, Loss: 3.0781\n  Média parcial até aqui: 3.2120\nBatch 1641/1683, Loss: 3.2500\nBatch 1642/1683, Loss: 3.0156\nBatch 1643/1683, Loss: 3.1406\nBatch 1644/1683, Loss: 3.5625\nBatch 1645/1683, Loss: 3.3125\nBatch 1646/1683, Loss: 3.2031\nBatch 1647/1683, Loss: 3.2656\nBatch 1648/1683, Loss: 3.0000\nBatch 1649/1683, Loss: 3.4688\nBatch 1650/1683, Loss: 3.0938\n  Média parcial até aqui: 3.2121\nBatch 1651/1683, Loss: 3.4375\nBatch 1652/1683, Loss: 3.3906\nBatch 1653/1683, Loss: 3.2188\nBatch 1654/1683, Loss: 3.5156\nBatch 1655/1683, Loss: 3.2812\nBatch 1656/1683, Loss: 3.8594\nBatch 1657/1683, Loss: 2.8594\nBatch 1658/1683, Loss: 3.2031\nBatch 1659/1683, Loss: 3.0469\nBatch 1660/1683, Loss: 3.1562\n  Média parcial até aqui: 3.2126\nBatch 1661/1683, Loss: 3.3281\nBatch 1662/1683, Loss: 2.8438\nBatch 1663/1683, Loss: 3.5312\nBatch 1664/1683, Loss: 3.1406\nBatch 1665/1683, Loss: 3.0781\nBatch 1666/1683, Loss: 3.3906\nBatch 1667/1683, Loss: 3.2812\nBatch 1668/1683, Loss: 3.1406\nBatch 1669/1683, Loss: 3.0781\nBatch 1670/1683, Loss: 3.2969\n  Média parcial até aqui: 3.2126\nBatch 1671/1683, Loss: 3.4062\nBatch 1672/1683, Loss: 3.7188\nBatch 1673/1683, Loss: 3.2812\nBatch 1674/1683, Loss: 2.9062\nBatch 1675/1683, Loss: 2.9688\nBatch 1676/1683, Loss: 3.4688\nBatch 1677/1683, Loss: 3.6250\nBatch 1678/1683, Loss: 3.1719\nBatch 1679/1683, Loss: 3.0000\nBatch 1680/1683, Loss: 3.1250\n  Média parcial até aqui: 3.2130\nBatch 1681/1683, Loss: 3.2031\nBatch 1682/1683, Loss: 3.1719\nBatch 1683/1683, Loss: 2.7812\nLoss média final no dataset de teste: 3.2127\nPerplexidade calculada: 24.85\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"generate_text_temp(model_qwen3, \"lisboa\", tokenizer=TOKENIZER, device=device, max_new_tokens=60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T02:29:02.303168Z","iopub.execute_input":"2025-10-18T02:29:02.303911Z","iopub.status.idle":"2025-10-18T02:29:04.547179Z","shell.execute_reply.started":"2025-10-18T02:29:02.303887Z","shell.execute_reply":"2025-10-18T02:29:04.546347Z"}},"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"'lisboa, se faziam já nos prazos, e nada havia que a ajudasse, e tornasse o fim ás tres horas da manhã.\\n\\nPouco depois, chegou o tempo e viuva, e'"},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}